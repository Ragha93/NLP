{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdOnVL6l1eo-"
      },
      "source": [
        "# Part1\n",
        "\n",
        "* Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkzssc-yjicI",
        "outputId": "88661ebe-0ff6-4ea4-abc7-63d52fdf4101"
      },
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import string,nltk,re,pickle\n",
        "from string import punctuation\n",
        "nltk.download(['punkt','stopwords','wordnet'])\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MultiLabelBinarizer,LabelEncoder\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "drive.mount('/content/drive/')\n",
        "project_path = '/content/drive/My Drive/Data Science/NLP/Project1/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcyUZL44k20u"
      },
      "source": [
        "# **Part 1: PROJECT OBJECTIVE**: The need is to build a NLP classifier which can use input text parameters to determine the label/s of the blog.\n",
        "\n",
        "\n",
        "### 1. Import and analyse the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O72WM4tgj580"
      },
      "source": [
        "df = pd.read_csv(project_path + \"blogtext.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "903azVMLlw6K",
        "outputId": "02533456-8429-4cc6-ab48-e64b7f348b32"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>topic</th>\n",
              "      <th>sign</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>14,May,2004</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>13,May,2004</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>11,June,2004</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...                                               text\n",
              "0  2059027  ...             Info has been found (+/- 100 pages,...\n",
              "1  2059027  ...             These are the team members:   Drewe...\n",
              "2  2059027  ...             In het kader van kernfusie op aarde...\n",
              "3  2059027  ...                   testing!!!  testing!!!          \n",
              "4  3581210  ...               Thanks to Yahoo!'s Toolbar I can ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vi5w9xWly7N",
        "outputId": "71c9f752-adb7-4e4b-fbdd-456b18ceb065"
      },
      "source": [
        "df.info()\n",
        "# There are 681284 rows of data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 681284 entries, 0 to 681283\n",
            "Data columns (total 7 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   id      681284 non-null  int64 \n",
            " 1   gender  681284 non-null  object\n",
            " 2   age     681284 non-null  int64 \n",
            " 3   topic   681284 non-null  object\n",
            " 4   sign    681284 non-null  object\n",
            " 5   date    681284 non-null  object\n",
            " 6   text    681284 non-null  object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 36.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0muhnVuMl_kd",
        "outputId": "47eb748e-244a-4c63-d504-ada90639c099"
      },
      "source": [
        "df[df['id']==3581210]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>topic</th>\n",
              "      <th>sign</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>11,June,2004</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>10,June,2004</td>\n",
              "      <td>I had an interesting conversation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>10,June,2004</td>\n",
              "      <td>Somehow Coca-Cola has a way of su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>10,June,2004</td>\n",
              "      <td>If anything, Korea is a country o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>10,June,2004</td>\n",
              "      <td>Take a read of this news article ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>10,August,2004</td>\n",
              "      <td>Korea's pretty funny sometimes.  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>09,August,2004</td>\n",
              "      <td>Ya, I'm off to Canada/Vancouver a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>09,August,2004</td>\n",
              "      <td>Ah, finally...someone else I know...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>08,August,2004</td>\n",
              "      <td>I think if I'm going to claim 여의도...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>08,August,2004</td>\n",
              "      <td>Being gay in Korea is like being ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ...                                               text\n",
              "4   3581210  ...               Thanks to Yahoo!'s Toolbar I can ...\n",
              "5   3581210  ...               I had an interesting conversation...\n",
              "6   3581210  ...               Somehow Coca-Cola has a way of su...\n",
              "7   3581210  ...               If anything, Korea is a country o...\n",
              "8   3581210  ...               Take a read of this news article ...\n",
              "..      ...  ...                                                ...\n",
              "69  3581210  ...               Korea's pretty funny sometimes.  ...\n",
              "70  3581210  ...               Ya, I'm off to Canada/Vancouver a...\n",
              "71  3581210  ...               Ah, finally...someone else I know...\n",
              "72  3581210  ...               I think if I'm going to claim 여의도...\n",
              "73  3581210  ...               Being gay in Korea is like being ...\n",
              "\n",
              "[70 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "6FkxEfb_mL_T",
        "outputId": "ffa2e142-0bb9-4f10-8f35-10bb575d26cd"
      },
      "source": [
        "df[df['id']==3581210]['text'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"             Thanks to Yahoo!'s Toolbar I can now 'capture' the URLs of popups...which means now I can show you some cool links to Korean Pop (K-Pop) audio and video without the need to relate instructions like: 'go to the site, click on the pop-audio button then choose...'.  So, without further ado here is the link to 24-hour K-Pop  urlLink audio  and the  urlLink video  streaming.  Enjoy.         \""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "yrfSINmoRnqk",
        "outputId": "d73c6c8b-dc82-4b46-c502-6a91345e6e9f"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.grid()\n",
        "sns.distplot(df['age'])\n",
        "\n",
        "#We notice a multimodal distribution, it has 3 peaks as mentioned in the project PDF.\n",
        "# 8240 \"10s\" blogs (ages 13-17) - Peak1\n",
        "# 8086 \"20s\" blogs(ages 23-27) - Peak2\n",
        "# 2994 \"30s\" blogs (ages 33-47)- Peak3 - less number of people"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb686225f10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "rM0-cldNS9_M",
        "outputId": "41b9fa8c-0119-4c25-a67b-e147ccfd79fe"
      },
      "source": [
        "#We notice 9 outliers \n",
        "plt.grid()\n",
        "sns.boxplot(df['age'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6856c8390>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krgFzrUHVdqB"
      },
      "source": [
        "**Although it looks clear that the data is multimodal, lets prove it with the help of statistical evidence**\n",
        "\n",
        "* Ha = The data is not normally distributed\n",
        "* Ho = The data is normally distributed\n",
        "\n",
        "**The p-value is less than alpha (P<0.5) considering we take the 95% confidence, so we reject the null hypothesis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uizDDAZjVc7M",
        "outputId": "7929300c-119a-497c-a8e6-847c9d515bb8"
      },
      "source": [
        "from scipy.stats import shapiro\n",
        "Stat,P_value = shapiro(df['age'])\n",
        "print(P_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "ZsVp6kdfRtQM",
        "outputId": "5dd585cc-003d-4bef-d31a-e3cdd065a51e"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.grid()\n",
        "sns.countplot(x=df['gender'])\n",
        "#Almost equal number of Male and Female"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f57e53d8510>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "akCkPBEsTS7w",
        "outputId": "a6a7123b-48b6-4e94-8709-758aec8ac40e"
      },
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.grid()\n",
        "sns.countplot(y = df['topic'],orient=\"H\")\n",
        "#Here we notice a lot of mismatch (ie) imbalanced data, the student and indUnk contribute the most. This will lead to\n",
        "#an imblance in dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f57db07ba10>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "E2iGPpP0Uiiq",
        "outputId": "75036c9d-7589-4b01-9ee8-73da11f39c8c"
      },
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.grid(axis=\"both\")\n",
        "sns.countplot(y = df['sign'])\n",
        "#These classes are still imbalanced but compared to topic they look little better"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f57dadcaa50>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNAJX-H9nKkb"
      },
      "source": [
        "# 2. Perform data pre-processing on the data:\n",
        "\n",
        "*   **Data cleansing by removing unwanted characters, spaces, stop words etc. Convert text to lowercase.<-**\n",
        "*   Target/label merger and transformation\n",
        "*   Train and test split\n",
        "*   Vectorisation, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvMKVQWw-KrH"
      },
      "source": [
        "#Operations performed:\n",
        "### 1. Lowering the words\n",
        "### 2. Remove html, 2 or more spaces, numbers, non-english words & punctuation symbols out\n",
        "### 3. Remove stopwords and give a list of words\n",
        "\n",
        "# **Raw form of coding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmxCgtIf-KAX"
      },
      "source": [
        "for t in df['text']:\n",
        "  t = t.lower()\n",
        "  t = re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",t)\n",
        "  t = [i for i in t if i not in string.punctuation]\n",
        "  t = \"\".join(t)\n",
        "  b = []\n",
        "  for i in nltk.word_tokenize(t):\n",
        "    if i not in stopwords.words('english'):\n",
        "      b.append(i)\n",
        "\n",
        "# Simplified coding\n",
        "# cleaned_text = [[i for i in nltk.word_tokenize(\"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",t.lower()) if i not in string.punctuation])) if i not in stopwords.words('english')] for t in df['text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zkcUP-n-aiJ"
      },
      "source": [
        "# **Simplified coding** - Using List comprehension and eliminating multiple list, variable declaration, appending methods etc etc which might save some memory and time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIz5TMI_RND"
      },
      "source": [
        "cleaned_text = [[i for i in nltk.word_tokenize(\"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",t.lower()) if i not in string.punctuation])) if i not in stopwords.words('english')] for t in df['text']]\n",
        "\n",
        "\n",
        "#Saving it in the form of array so that the above function is not run multiple times.\n",
        "# np.save('/content/drive/My Drive/Data Science/NLP/Project1/cleaned_text.npy',np.array(cleaned_text))\n",
        "# cleaned_text = np.load('/content/drive/My Drive/Data Science/NLP/Project1/cleaned_text.npy',allow_pickle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fA1gvmN-KF5",
        "outputId": "5b5830ab-1575-4d7a-dbc0-c5499e2f9d14"
      },
      "source": [
        "len(cleaned_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "681284"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCadesrc-KIj",
        "outputId": "702f1ed2-9308-4acf-f85c-15387bc45670"
      },
      "source": [
        "len(df['text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "681284"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8w1CtG7vKv8"
      },
      "source": [
        "*  **Target/label merger and transformation**  <--\n",
        "\n",
        "  * In this case we see that there is Age, Gender, Topic and Zodiac sign which needs to be merged to create a target.\n",
        "  * Once merged in the form of list, we will try to convert it into binary form of list that we usually use in neural networks with softmax\n",
        "  * Now, this becomes 1 input vs multiple targets\n",
        "  * In order to convert the target in the form of text to one hot bits we try to understand all the contents of target in the form of dictionary\n",
        "  * Post this use that to convert target to binary encoded form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZv7Ps6F1neK"
      },
      "source": [
        "def target(x):\n",
        "  return x['gender'],x['age'],x['topic'],x['sign']\n",
        "\n",
        "df['age1'] = df['age'].apply(str)\n",
        "df['Target_labels'] = df.apply(lambda col : [col['gender'],col['age1'],col['topic'],col['sign']], axis=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWlZKakb-KOc"
      },
      "source": [
        "target_count=dict()\n",
        "\n",
        "for tl in df['Target_labels'].values:\n",
        "  for t in tl:\n",
        "    if t in target_count:\n",
        "      target_count[t]+=1\n",
        "    else:\n",
        "      target_count[t]=1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnmEsvhOTR6A"
      },
      "source": [
        "* **Train test split** <--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDuzxxly-KXz"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(cleaned_text, df['Target_labels'], test_size=0.3, random_state=11)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR1DTAkXYzbX"
      },
      "source": [
        "## **We use Multilabel binarizer as we have have Age, Gender, Topic and Sign to predict per each row of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml9YU5xn-KU9"
      },
      "source": [
        "binarizer=MultiLabelBinarizer(classes=sorted(target_count.keys()))\n",
        "y_train = binarizer.fit_transform(y_train)\n",
        "y_test = binarizer.transform(y_test)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23XCbjkWtTyL"
      },
      "source": [
        "* **Vectorisation** <--\n",
        "  * Before vectorization we will try to stem or lemmatize the data so that we will get to know the models performance on both the data.(We dont try both Stemming and Lemmatization as the data is too huge to be run on any platform)\n",
        "  * Post this we convert the text to vectors using both TF-IDF method and also Count vectorization(BOW).We stop only with stemming as Lemmatization takes more time.\n",
        "\n",
        "  * I am personally not trying Word2Vec as it is crashing the Colab's memory and continues to run for more than 5 hrs in jupyter notebook and sometimes crashes with memory error message"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwZYrfJKTn9A"
      },
      "source": [
        "  #Using Stemming & Lemmatization techinique\n",
        "PS = PorterStemmer()\n",
        "LM = WordNetLemmatizer()\n",
        "\n",
        "#Vectorization done using both TF-IDF and BOW\n",
        "tfidf = TfidfVectorizer()\n",
        "cv = CountVectorizer()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpELT7xWoLsB"
      },
      "source": [
        " X_train_s = []\n",
        "for i in X_train:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(PS.stem(j))\n",
        "  X_train_s.append(m)\n",
        "\n",
        "\n",
        "X_test_s = []\n",
        "for i in X_test:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(PS.stem(j))\n",
        "  X_test_s.append(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cycbsB4w8INF"
      },
      "source": [
        "X_train_1 = [\" \".join(i) for i in X_train_s]\n",
        "X_test_1 = [\" \".join(i) for i in X_test_s]\n",
        "X_train_1 = tfidf.fit_transform(X_train_1)\n",
        "X_test_1 = tfidf.transform(X_test_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN38IXR_I02T"
      },
      "source": [
        "# 3. Design, train, tune and test the best text classifier. \n",
        "\n",
        "## **Naive Bayes Theorem**\n",
        "  * Currently stopping with Naive bayes theorem as the data is too big to be processed (6,81,284 rows of data)\n",
        "  * Difficulty faced while Lowering the words, Removing html, more spaces, numbers, non-english words, stop words & punctuation symbols out. It took around 3+hours to complete this.\n",
        "  * Post this the idea was to apply stemming or Lemmatization but even Stemming couldnt be done with ease due to RAM restrictions and high data hence i have stopped only with stem. It took around 2+ hours to complete this.\n",
        "  * Instead of using Stemming or Lemmatization, i can explain what it does. For eg: if we have a total of 10000 words in vocabulary then applying a count vectorizer or TF-IDF might bring in a matrix with too many features, a matrix with 10000 rows and columns but if we apply stemming or lemmatization some of the words can be made into single word, for eg: Running, Runs, Ran and Run can be displayed as Run and similary for all the words which will reduce the complexity.\n",
        "  * Running Naive Bayes theorem took around 40+ minutes and Similarly i can try out Logistic Regression, Support Vector Classifier,  Decision Tree Classifier etc but due to enormous data, i am stopping with Naive Bayes theorem. \n",
        "  * During this time my laptop crashed multiple times and Google colab also crashed multiple times so i had to store all the data in the form of numpy array to reload and process it for applying in a single ML model. The other way to solve this is by taking only 50,000 rows of data which can be pre-processed quickly and we can try applying multiple algorithms as well.\n",
        "\n",
        "**Hint: The aim here Is to import the text, process it such a way that it can be taken as an inout to the ML/NN classifiers. Be analytical and experimental here in trying new\n",
        "approaches to design the best model.**\n",
        "\n",
        "* As the hint states the aim here is to import text, preprocess it so that it can fed to ML, post noticing this i focused only on applying a singl ML model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flDDGhUj9QO9",
        "outputId": "dd4e3f9f-1fa3-417d-f5c0-e26a64c6a90b"
      },
      "source": [
        "model= MultinomialNB(alpha=1.5)\n",
        "model=OneVsRestClassifier(model)\n",
        "model.fit(X_train_1,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=MultinomialNB(alpha=1.5, class_prior=None,\n",
              "                                            fit_prior=True),\n",
              "                    n_jobs=None)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE2i95BR9R9Z"
      },
      "source": [
        "pred = model.predict(X_test_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5TCVSHmSdD4",
        "outputId": "b9eb11e4-ae83-403c-fc20-c6f936a16937"
      },
      "source": [
        "model.score(X_train_1,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.001157480215895223"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm84nchPSjQQ",
        "outputId": "5683a8c4-8e92-45ed-aa4d-24eeb2444f01"
      },
      "source": [
        "model.score(X_test_1,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0016145919974949361"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucOPpQagPPHP",
        "outputId": "f7fcb1d3-3a51-48c1-f44d-80af42660f65"
      },
      "source": [
        "print(classification_report(y_test,pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      3983\n",
            "           1       0.60      0.01      0.01      8324\n",
            "           2       0.15      0.00      0.00     12504\n",
            "           3       0.51      0.00      0.01     21776\n",
            "           4       0.40      0.00      0.01     24282\n",
            "           5       0.46      0.00      0.01     21811\n",
            "           6       0.72      0.00      0.01     24115\n",
            "           7       0.33      0.00      0.00     20038\n",
            "           8       0.28      0.00      0.00     16665\n",
            "           9       0.35      0.00      0.00     13817\n",
            "          10       0.00      0.00      0.00      5220\n",
            "          11       0.14      0.00      0.00      6216\n",
            "          12       0.00      0.00      0.00      5305\n",
            "          13       0.00      0.00      0.00      4269\n",
            "          14       0.00      0.00      0.00      2750\n",
            "          15       0.00      0.00      0.00      2285\n",
            "          16       0.00      0.00      0.00      1701\n",
            "          17       0.00      0.00      0.00      1552\n",
            "          18       0.00      0.00      0.00      1126\n",
            "          19       0.00      0.00      0.00       883\n",
            "          20       0.00      0.00      0.00      1271\n",
            "          21       0.00      0.00      0.00       621\n",
            "          22       0.00      0.00      0.00      1358\n",
            "          23       0.00      0.00      0.00       812\n",
            "          24       0.00      0.00      0.00       650\n",
            "          25       0.00      0.00      0.00      1052\n",
            "          26       0.00      0.00      0.00      1141\n",
            "          27       0.09      0.00      0.00      1394\n",
            "          28       0.00      0.00      0.00       390\n",
            "          29       0.23      0.00      0.00     15073\n",
            "          30       0.00      0.00      0.00       506\n",
            "          31       0.22      0.00      0.00     19387\n",
            "          32       0.23      0.00      0.00      9708\n",
            "          33       0.00      0.00      0.00       381\n",
            "          34       0.00      0.00      0.00      1263\n",
            "          35       0.17      0.00      0.00       665\n",
            "          36       0.00      0.00      0.00      1385\n",
            "          37       0.72      0.01      0.02     19290\n",
            "          38       0.26      0.00      0.00     14604\n",
            "          39       0.00      0.00      0.00      1205\n",
            "          40       0.26      0.00      0.00      5957\n",
            "          41       0.00      0.00      0.00       327\n",
            "          42       0.00      0.00      0.00      1728\n",
            "          43       0.15      0.00      0.00      8917\n",
            "          44       0.00      0.00      0.00      3520\n",
            "          45       0.00      0.00      0.00       170\n",
            "          46       0.00      0.00      0.00      1456\n",
            "          47       0.33      0.00      0.00     15657\n",
            "          48       0.27      0.00      0.00      2071\n",
            "          49       0.00      0.00      0.00       846\n",
            "          50       0.00      0.00      0.00      4873\n",
            "          51       0.00      0.00      0.00       396\n",
            "          52       0.00      0.00      0.00      2664\n",
            "          53       0.00      0.00      0.00       539\n",
            "          54       0.04      0.00      0.00     16305\n",
            "          55       0.30      0.00      0.00     18576\n",
            "          56       0.00      0.00      0.00       686\n",
            "          57       0.00      0.00      0.00        93\n",
            "          58       0.00      0.00      0.00      1415\n",
            "          59       0.00      0.00      0.00       909\n",
            "          60       0.00      0.00      0.00       948\n",
            "          61       0.11      0.00      0.00      4393\n",
            "          62       0.54      0.00      0.01     16108\n",
            "          63       0.09      0.00      0.00      2283\n",
            "          64       0.00      0.00      0.00       877\n",
            "          65       0.00      0.00      0.00      1614\n",
            "          66       0.31      0.00      0.00     14937\n",
            "          67       0.00      0.00      0.00      2228\n",
            "          68       0.18      0.00      0.00     17290\n",
            "          69       0.00      0.00      0.00       901\n",
            "          70       0.69      0.03      0.05     46116\n",
            "          71       0.72      0.01      0.01     18900\n",
            "          72       0.29      0.00      0.00     12483\n",
            "          73       0.00      0.00      0.00      1146\n",
            "          74       0.00      0.00      0.00       583\n",
            "          75       0.00      0.00      0.00       750\n",
            "          76       0.28      0.00      0.00     18259\n",
            "          77       0.66      0.75      0.70    100842\n",
            "          78       0.90      0.03      0.06     75459\n",
            "          79       0.72      0.62      0.67    103544\n",
            "\n",
            "   micro avg       0.68      0.18      0.28    817544\n",
            "   macro avg       0.16      0.02      0.02    817544\n",
            "weighted avg       0.48      0.18      0.18    817544\n",
            " samples avg       0.69      0.18      0.28    817544\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0dwtbQJMPy"
      },
      "source": [
        "### **4. Display and explain detail the classification report**\n",
        "\n",
        "* As we can see above the model behaves very badly and gives a poor Accuracy metric. We dont touch even 10% of Accuracy probably because we have a wide variety of classes (80 classes) which is completely imbalanced, the data is too big to be processed and we have tried only one ML model without any hyperparameter tuning considering the hardware restrictions.\n",
        "\n",
        "* The classes 77,78 & 79 constitute almost 30% of total data due to which we notice a better Precision, Recall and F1 score on those classes. The rest of the classes have a bad score and in this case we try to focus only on F1 Score since we want both the Precision and Recall to be high.\n",
        "\n",
        "* Since we prefer overall data and not just any one class we look into micro average of F1 score which is 29%, i.e: aggregation of all classes to compute the 1 average metric.\n",
        "\n",
        "* The data is completely imbalanced as we have around 80 classes, we need atleast equal classes but if we were to use the Upsampling, Downsampling, SMOTE or Class weights in RFC or DTC we need computational power and hence i am dropping it to single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGpRWeTdYQvt"
      },
      "source": [
        "# 5. Print the true vs predicted labels for any 5 entries from the dataset. <--\n",
        "* Since we have used the MultiLabelBinarizer to convert multiple fields in 1 prediction, we can inverse transform the predicted values using same Multilabelbinarizer\n",
        "\n",
        "* We try to pick the best 5 predicitons and display them for reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjWgYU1-H954"
      },
      "source": [
        "prediction = binarizer.inverse_transform(pred)\n",
        "Actual = binarizer.inverse_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isp4n538cadE",
        "outputId": "f71869a6-bfde-4b3a-d220-0ec763ed32fa"
      },
      "source": [
        "for i,j in enumerate(prediction):\n",
        "  if len(j) == 3 and i!= 1358 and i!= 1684 and i!= 2395:\n",
        "    print(i)\n",
        "    break\n",
        "  else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx5TOZBjgLc3"
      },
      "source": [
        "* Prediction 1 - Perfecly aligned with the actual test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O9tjHVQcnZy",
        "outputId": "5e735f35-dba4-410a-cf12-680016eac65e"
      },
      "source": [
        "prediction[1581]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('16', 'Libra', 'female', 'indUnk')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0udFd1aHe4YB",
        "outputId": "2d7cd24e-a6a2-42cc-dc0d-902e45834efc"
      },
      "source": [
        "Actual[1581]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('16', 'Libra', 'female', 'indUnk')"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSbb4UQRgQDY"
      },
      "source": [
        "* Prediction 2 - Missing the age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DzweKFwfoyu",
        "outputId": "7f6a5b0d-8954-403a-a472-ec05a2eee7c2"
      },
      "source": [
        "prediction[1358]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Cancer', 'female', 'indUnk')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyLGCe48gHoy",
        "outputId": "46f12319-833e-422f-b8b0-37f40264b9b4"
      },
      "source": [
        "Actual[1358]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('40', 'Cancer', 'female', 'indUnk')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_6vDDGgdCg"
      },
      "source": [
        "* Prediction 3 - Missing the age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bRA-eDDgJtH",
        "outputId": "602d6748-384a-4ba1-d0aa-d573acbfc2bb"
      },
      "source": [
        "prediction[1684]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Cancer', 'female', 'indUnk')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSyCXKxcgYfI",
        "outputId": "b3773b3e-6edd-4893-991c-a56a3e5cfcd1"
      },
      "source": [
        "Actual[1684]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('40', 'Cancer', 'female', 'indUnk')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExGre-t-ge2r"
      },
      "source": [
        "* Prediction 4 - Missing the age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOtbisE-gZjl",
        "outputId": "30fc411b-809c-4688-9b47-fb7993326f40"
      },
      "source": [
        "prediction[2395]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Taurus', 'female', 'indUnk')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHhdAMyAggp6",
        "outputId": "85cfbb85-50f8-46b1-849c-5f07dc7d5eac"
      },
      "source": [
        "Actual[2395]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('14', 'Taurus', 'female', 'indUnk')"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlLVLRQ2ghOR"
      },
      "source": [
        "* Prediction 5 - Missing the age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbKOkAiJgiD7",
        "outputId": "f0d41310-17ab-4347-e04e-73ebbc857f1a"
      },
      "source": [
        "prediction[2983]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Cancer', 'female', 'indUnk')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpcoAkMgyY_",
        "outputId": "55bbc090-5405-4fc8-af4f-0242d89c096f"
      },
      "source": [
        "Actual[2983]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('40', 'Cancer', 'female', 'indUnk')"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roU9rzhj_7rg"
      },
      "source": [
        "## **For a change, lets take only 100000rows of data and try to build a model using that**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLPuAie7hqbQ"
      },
      "source": [
        "df1 = df[:100000]\n",
        "#Picking only 100000 rows of data\n",
        "\n",
        "cleaned_text1 = [[i for i in nltk.word_tokenize(\"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",t.lower()) if i not in string.punctuation])) if i not in stopwords.words('english')] for t in df1['text']]\n",
        "#Cleaning the text from Stopwords, punctuations, nonenglish words, HTML tags, unnecessary spaces etc.\n",
        "\n",
        "df1['age1'] = df1['age'].apply(str)\n",
        "df1['Target_labels'] = df1.apply(lambda col : [col['gender'],col['age1'],col['topic'],col['sign']], axis=1)\n",
        "#Creating Target labels\n",
        "\n",
        "\n",
        "target_count1=dict()\n",
        "\n",
        "for tl in df1['Target_labels'].values:\n",
        "  for t in tl:\n",
        "    if t in target_count1:\n",
        "      target_count1[t]+=1\n",
        "    else:\n",
        "      target_count1[t]=1\n",
        "#Collecting the target counts      \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(cleaned_text1, df1['Target_labels'], test_size=0.3, random_state=11)\n",
        "#performing Train test split\n",
        "\n",
        "binarizer=MultiLabelBinarizer(classes=sorted(target_count1.keys()))\n",
        "y_train = binarizer.fit_transform(y_train)\n",
        "y_test = binarizer.transform(y_test)\n",
        "#Converting the target to multilabel onehot encoded values\n",
        "\n",
        "#Using Stemming & Lemmatization techinique\n",
        "PS = PorterStemmer()\n",
        "LM = WordNetLemmatizer()\n",
        "\n",
        "#Vectorization done using both TF-IDF and BOW\n",
        "tfidf = TfidfVectorizer()\n",
        "cv = CountVectorizer()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gllcjJH5aXNo"
      },
      "source": [
        "X_train_h = []\n",
        "for i in X_train:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(PS.stem(j))\n",
        "  X_train_h.append(m)\n",
        "\n",
        "\n",
        "X_test_h = []\n",
        "for i in X_test:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(PS.stem(j))\n",
        "  X_test_h.append(m)\n",
        "\n",
        "#Using Stem  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXbryVFhae1L"
      },
      "source": [
        "X_train_L = []\n",
        "for i in X_train:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(LM.lemmatize(j))\n",
        "  X_train_L.append(m)\n",
        "\n",
        "\n",
        "X_test_L = []\n",
        "for i in X_test:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(LM.lemmatize(j))\n",
        "  X_test_L.append(m)\n",
        "\n",
        "#Using Lemmatization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRaXWvdAa7R2"
      },
      "source": [
        "X_train_1 = [\" \".join(i) for i in X_train_h]\n",
        "X_test_1 = [\" \".join(i) for i in X_test_h]\n",
        "\n",
        "X_train_1 = tfidf.fit_transform(X_train_1)\n",
        "X_test_1 = tfidf.transform(X_test_1)\n",
        "\n",
        "#X_train & X_test prepared for Stemmed data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkmIXiT9mHHR"
      },
      "source": [
        "X_train_2 = [\" \".join(i) for i in X_train_L]\n",
        "X_test_2 = [\" \".join(i) for i in X_test_L]\n",
        "\n",
        "X_train_2L = tfidf.fit_transform(X_train_2)\n",
        "X_test_2L = tfidf.transform(X_test_2)\n",
        "\n",
        "#X_train & X_test prepared for Lemmatized data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDBEwUwCvy5"
      },
      "source": [
        "## 1. Using Decision Tree classifier using Stemmed data\n",
        " * This model gives a 21% of F1 score with the stemmed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPp8xyjTbC9P",
        "outputId": "db6bdf0f-e831-4e68-c1d3-030016f48489"
      },
      "source": [
        "model= DecisionTreeClassifier(max_depth=5)\n",
        "model= OneVsRestClassifier(model)\n",
        "model.fit(X_train_1,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     criterion='gini',\n",
              "                                                     max_depth=5,\n",
              "                                                     max_features=None,\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_impurity_split=None,\n",
              "                                                     min_samples_leaf=1,\n",
              "                                                     min_samples_split=2,\n",
              "                                                     min_weight_fraction_leaf=0.0,\n",
              "                                                     presort='deprecated',\n",
              "                                                     random_state=None,\n",
              "                                                     splitter='best'),\n",
              "                    n_jobs=None)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNG1PE9lbQPY"
      },
      "source": [
        "pred = model.predict(X_test_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qm9nb1F7cJ9",
        "outputId": "5cc485a2-6283-4cfc-ecb1-68e0338dea0f"
      },
      "source": [
        "print(classification_report(y_test,pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.15      0.26       432\n",
            "           1       0.83      0.04      0.08      1017\n",
            "           2       0.95      0.05      0.10      1942\n",
            "           3       0.96      0.11      0.20      2532\n",
            "           4       0.70      0.07      0.12      3766\n",
            "           5       0.86      0.05      0.09      3214\n",
            "           6       0.97      0.06      0.12      3596\n",
            "           7       0.94      0.05      0.09      2577\n",
            "           8       0.90      0.11      0.19      2440\n",
            "           9       0.98      0.05      0.10      2404\n",
            "          10       0.93      0.06      0.11       834\n",
            "          11       0.92      0.42      0.57       742\n",
            "          12       0.96      0.15      0.26      1427\n",
            "          13       0.86      0.26      0.40       943\n",
            "          14       0.65      0.07      0.12       253\n",
            "          15       0.97      0.28      0.43       259\n",
            "          16       0.69      0.10      0.18       177\n",
            "          17       0.64      0.09      0.15       164\n",
            "          18       0.77      0.10      0.17       236\n",
            "          19       0.70      0.13      0.22        55\n",
            "          20       0.94      0.09      0.17       160\n",
            "          21       1.00      0.20      0.33        25\n",
            "          22       0.76      0.17      0.28       250\n",
            "          23       0.74      0.09      0.16       288\n",
            "          24       0.88      0.30      0.45       119\n",
            "          25       0.76      0.21      0.33       148\n",
            "          26       0.86      0.08      0.14       159\n",
            "          27       0.86      0.17      0.28       223\n",
            "          28       0.80      0.09      0.16        44\n",
            "          29       0.94      0.09      0.16      2691\n",
            "          30       1.00      0.04      0.07        28\n",
            "          31       0.96      0.11      0.19      3236\n",
            "          32       0.73      0.04      0.08      1530\n",
            "          33       0.00      0.00      0.00        36\n",
            "          34       0.90      0.18      0.30       102\n",
            "          35       0.73      0.17      0.28       110\n",
            "          36       0.78      0.12      0.22       168\n",
            "          37       0.97      0.05      0.09      2781\n",
            "          38       0.84      0.05      0.09      2608\n",
            "          39       0.93      0.15      0.26        85\n",
            "          40       0.80      0.04      0.08       882\n",
            "          41       0.44      0.05      0.09        79\n",
            "          42       0.88      0.13      0.23       286\n",
            "          43       0.99      0.12      0.22      1669\n",
            "          44       0.75      0.11      0.19       657\n",
            "          45       0.00      0.00      0.00         3\n",
            "          46       0.87      0.40      0.55       558\n",
            "          47       0.87      0.03      0.06      2854\n",
            "          48       0.94      0.25      0.40       654\n",
            "          49       1.00      0.17      0.29        72\n",
            "          50       0.87      0.06      0.12       633\n",
            "          51       0.93      0.36      0.52        74\n",
            "          52       0.43      0.03      0.06        97\n",
            "          53       0.83      0.18      0.29       114\n",
            "          54       0.94      0.04      0.08      2456\n",
            "          55       0.97      0.09      0.16      2152\n",
            "          56       0.74      0.14      0.24       159\n",
            "          57       0.00      0.00      0.00        25\n",
            "          58       0.77      0.15      0.26       221\n",
            "          59       0.79      0.11      0.20       228\n",
            "          60       0.93      0.38      0.54        97\n",
            "          61       0.77      0.07      0.12       408\n",
            "          62       0.97      0.12      0.22      2268\n",
            "          63       0.62      0.12      0.20       317\n",
            "          64       0.33      0.02      0.04        48\n",
            "          65       0.61      0.06      0.11       329\n",
            "          66       0.75      0.10      0.18      2248\n",
            "          67       0.88      0.17      0.28       308\n",
            "          68       0.82      0.03      0.06      2070\n",
            "          69       0.71      0.10      0.18       120\n",
            "          70       0.79      0.06      0.11      6597\n",
            "          71       0.90      0.10      0.18      2566\n",
            "          72       0.81      0.05      0.09      2531\n",
            "          73       0.17      0.02      0.04        47\n",
            "          74       0.57      0.16      0.25        79\n",
            "          75       0.99      0.56      0.72       246\n",
            "          76       0.79      0.04      0.07      2070\n",
            "          77       0.61      0.38      0.46     14027\n",
            "          78       0.84      0.05      0.09      9977\n",
            "          79       0.59      0.79      0.68     15973\n",
            "\n",
            "   micro avg       0.66      0.21      0.32    120000\n",
            "   macro avg       0.78      0.13      0.21    120000\n",
            "weighted avg       0.80      0.21      0.25    120000\n",
            " samples avg       0.62      0.21      0.30    120000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsDCY1H7C2F5"
      },
      "source": [
        "## 2. Using Lemmatized data for Decision Tree Classifier\n",
        "* This model gives a 21% of F1 score with the Lemmatized data\n",
        "\n",
        "Both the Stemmed and Lemmatized data give the same F1score since we took very few data maybe if we take entire data Stemming might give us better prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86df98zN7fua",
        "outputId": "ef5a57bf-4bfd-49c2-85a4-38eae328c4ab"
      },
      "source": [
        "model1= DecisionTreeClassifier(max_depth=5)\n",
        "model2= OneVsRestClassifier(model1)\n",
        "model2.fit(X_train_2L,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     criterion='gini',\n",
              "                                                     max_depth=5,\n",
              "                                                     max_features=None,\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_impurity_split=None,\n",
              "                                                     min_samples_leaf=1,\n",
              "                                                     min_samples_split=2,\n",
              "                                                     min_weight_fraction_leaf=0.0,\n",
              "                                                     presort='deprecated',\n",
              "                                                     random_state=None,\n",
              "                                                     splitter='best'),\n",
              "                    n_jobs=None)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8FU7x3l7o9z"
      },
      "source": [
        "pred1 = model2.predict(X_test_2L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtO9E3pU7sc8",
        "outputId": "7ecf0fb1-b21a-4623-b417-6e99a3529926"
      },
      "source": [
        "print(classification_report(y_test,pred1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.16      0.28       432\n",
            "           1       0.77      0.04      0.08      1017\n",
            "           2       0.96      0.05      0.10      1942\n",
            "           3       0.97      0.11      0.20      2532\n",
            "           4       0.72      0.07      0.13      3766\n",
            "           5       0.96      0.05      0.09      3214\n",
            "           6       0.90      0.07      0.13      3596\n",
            "           7       0.96      0.05      0.09      2577\n",
            "           8       0.96      0.09      0.16      2440\n",
            "           9       0.78      0.06      0.11      2404\n",
            "          10       0.94      0.06      0.10       834\n",
            "          11       0.92      0.42      0.58       742\n",
            "          12       0.91      0.16      0.27      1427\n",
            "          13       0.87      0.27      0.41       943\n",
            "          14       0.67      0.06      0.10       253\n",
            "          15       0.89      0.29      0.44       259\n",
            "          16       0.80      0.11      0.20       177\n",
            "          17       0.64      0.09      0.15       164\n",
            "          18       0.61      0.08      0.15       236\n",
            "          19       0.70      0.13      0.22        55\n",
            "          20       0.94      0.09      0.17       160\n",
            "          21       1.00      0.24      0.39        25\n",
            "          22       0.80      0.21      0.34       250\n",
            "          23       0.73      0.08      0.15       288\n",
            "          24       0.93      0.31      0.47       119\n",
            "          25       0.94      0.20      0.33       148\n",
            "          26       0.80      0.08      0.14       159\n",
            "          27       0.86      0.19      0.31       223\n",
            "          28       0.86      0.14      0.24        44\n",
            "          29       0.97      0.09      0.16      2691\n",
            "          30       1.00      0.04      0.07        28\n",
            "          31       0.96      0.11      0.19      3236\n",
            "          32       0.80      0.04      0.08      1530\n",
            "          33       0.00      0.00      0.00        36\n",
            "          34       0.95      0.19      0.31       102\n",
            "          35       0.78      0.16      0.27       110\n",
            "          36       0.81      0.12      0.22       168\n",
            "          37       0.94      0.05      0.09      2781\n",
            "          38       0.89      0.05      0.10      2608\n",
            "          39       0.89      0.19      0.31        85\n",
            "          40       0.82      0.04      0.08       882\n",
            "          41       0.29      0.03      0.05        79\n",
            "          42       0.89      0.12      0.20       286\n",
            "          43       0.99      0.12      0.22      1669\n",
            "          44       0.74      0.11      0.19       657\n",
            "          45       0.00      0.00      0.00         3\n",
            "          46       0.87      0.41      0.56       558\n",
            "          47       0.89      0.03      0.06      2854\n",
            "          48       0.95      0.27      0.43       654\n",
            "          49       1.00      0.18      0.31        72\n",
            "          50       0.78      0.06      0.11       633\n",
            "          51       0.96      0.34      0.50        74\n",
            "          52       0.67      0.04      0.08        97\n",
            "          53       0.77      0.15      0.25       114\n",
            "          54       0.95      0.04      0.08      2456\n",
            "          55       0.98      0.09      0.16      2152\n",
            "          56       0.85      0.18      0.29       159\n",
            "          57       0.00      0.00      0.00        25\n",
            "          58       0.95      0.18      0.30       221\n",
            "          59       0.79      0.11      0.20       228\n",
            "          60       0.90      0.38      0.54        97\n",
            "          61       0.75      0.05      0.10       408\n",
            "          62       0.97      0.12      0.22      2268\n",
            "          63       0.70      0.13      0.21       317\n",
            "          64       0.50      0.02      0.04        48\n",
            "          65       0.73      0.09      0.16       329\n",
            "          66       0.75      0.10      0.18      2248\n",
            "          67       0.90      0.19      0.31       308\n",
            "          68       0.90      0.03      0.06      2070\n",
            "          69       0.65      0.12      0.21       120\n",
            "          70       0.80      0.06      0.10      6597\n",
            "          71       0.92      0.10      0.18      2566\n",
            "          72       0.73      0.06      0.10      2531\n",
            "          73       0.44      0.09      0.14        47\n",
            "          74       0.57      0.16      0.25        79\n",
            "          75       0.99      0.56      0.71       246\n",
            "          76       0.89      0.04      0.07      2070\n",
            "          77       0.61      0.33      0.43     14027\n",
            "          78       0.94      0.04      0.07      9977\n",
            "          79       0.58      0.82      0.68     15973\n",
            "\n",
            "   micro avg       0.65      0.21      0.32    120000\n",
            "   macro avg       0.80      0.14      0.21    120000\n",
            "weighted avg       0.81      0.21      0.25    120000\n",
            " samples avg       0.61      0.21      0.29    120000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cT10qUYExPe"
      },
      "source": [
        "pred1 = binarizer.inverse_transform(pred1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWVW66W5FPhj"
      },
      "source": [
        "y_test = binarizer.inverse_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVNL974iGQV_",
        "outputId": "0b302173-641d-42ec-9572-836b6e87c77d"
      },
      "source": [
        "pred1[38]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('36', 'Aries', 'Fashion', 'male')"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlIeTZzeGX9D",
        "outputId": "048c6494-286a-4a01-d05c-e6114dd8cf57"
      },
      "source": [
        "y_test[38]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('36', 'Aries', 'Fashion', 'male')"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9yhT0RyGZaA",
        "outputId": "edba8b26-f8d4-4b2d-f246-a440928289dc"
      },
      "source": [
        "pred1[78]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('34', 'Sagittarius', 'female', 'indUnk')"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e38V3Fh1H8aO",
        "outputId": "d59cf5be-2931-4004-be79-eb4ba201f1cf"
      },
      "source": [
        "y_test[78]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('34', 'Sagittarius', 'female', 'indUnk')"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqsfoLwiPnq4"
      },
      "source": [
        "# Part 2 - Great Learning Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvFNqJ72Po5-"
      },
      "source": [
        "mm = pd.read_json('/content/drive/My Drive/Data Science/NLP/Project1/GL Bot.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVhxap5jmi56"
      },
      "source": [
        "* We first need to create a Dataframe using the Json input:\n",
        "1. Normalize the Json input to generate a Dataframe (df1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl0CMQa9Xnde"
      },
      "source": [
        "df1 = pd.json_normalize(mm['intents'])\n",
        "#We normalize the json to convert it into a Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "czU1YhwYnVGf",
        "outputId": "5cbf34ff-4cea-4ccb-e4e7-61719a91b4c5"
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>patterns</th>\n",
              "      <th>responses</th>\n",
              "      <th>context_set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Intro</td>\n",
              "      <td>[hi, how are you, is anyone there, hello, what...</td>\n",
              "      <td>[Hello! how can i help you ?]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Exit</td>\n",
              "      <td>[thank you, thanks, cya, see you, later, see y...</td>\n",
              "      <td>[I hope I was able to assist you, Good Bye]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Olympus</td>\n",
              "      <td>[olympus, explain me how olympus works, I am n...</td>\n",
              "      <td>[Link: Olympus wiki]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SL</td>\n",
              "      <td>[i am not able to understand svm, explain me h...</td>\n",
              "      <td>[Link: Machine Learning wiki ]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NN</td>\n",
              "      <td>[what is deep learning, unable to understand d...</td>\n",
              "      <td>[Link: Neural Nets wiki]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       tag  ... context_set\n",
              "0    Intro  ...            \n",
              "1     Exit  ...            \n",
              "2  Olympus  ...            \n",
              "3       SL  ...            \n",
              "4       NN  ...            \n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soskmhEenLh5"
      },
      "source": [
        "2. Iterate over the Json to generate a Dataframe (df2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Fi-2iadNNV"
      },
      "source": [
        "df2 = pd.DataFrame()\n",
        "for i in mm['intents']:\n",
        "  for j in i['patterns']:\n",
        "    df2 = df2.append({'Tag':i['tag'],'Responses':i['responses'][0],'Pattern':j},ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pZDdgTeHjeFb",
        "outputId": "527963de-4c7f-4004-cec1-b7d62ad4890a"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pattern</th>\n",
              "      <th>Responses</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hi</td>\n",
              "      <td>Hello! how can i help you ?</td>\n",
              "      <td>Intro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>how are you</td>\n",
              "      <td>Hello! how can i help you ?</td>\n",
              "      <td>Intro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>is anyone there</td>\n",
              "      <td>Hello! how can i help you ?</td>\n",
              "      <td>Intro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hello</td>\n",
              "      <td>Hello! how can i help you ?</td>\n",
              "      <td>Intro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>whats up</td>\n",
              "      <td>Hello! how can i help you ?</td>\n",
              "      <td>Intro</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Pattern                    Responses    Tag\n",
              "0               hi  Hello! how can i help you ?  Intro\n",
              "1      how are you  Hello! how can i help you ?  Intro\n",
              "2  is anyone there  Hello! how can i help you ?  Intro\n",
              "3            hello  Hello! how can i help you ?  Intro\n",
              "4         whats up  Hello! how can i help you ?  Intro"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlW_NWrHnm1K"
      },
      "source": [
        "## We have 8 unique responses that the chat bot is limited to give as a response for any input. **The idea is to create a proper ML model that can take any input and share one of these responses as output and we have to make sure the response is relevant and not Irrelevant.**\n",
        "\n",
        "* In this we notice a lot of stopwords used as an input and hence we will not work on eliminating the stopwords and also we have very few input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX4GjxqtOw68",
        "outputId": "94d52843-34f3-45e9-b267-c8f967799670"
      },
      "source": [
        "le = LabelEncoder()\n",
        "df2['Target'] = le.fit_transform(df2['Responses'])\n",
        "df2['Target'].value_counts()\n",
        "\n",
        "#We see there is a complete imbalance in the dataset, we need to find a way to balance them out.\n",
        "#We will manually try to fillin the values to balance the dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    29\n",
              "4    24\n",
              "0    20\n",
              "2    16\n",
              "5    13\n",
              "7     9\n",
              "6     9\n",
              "1     8\n",
              "Name: Target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC5DYqiHX9_Y"
      },
      "source": [
        "#Manually pushing some inputs to balance the classes\n",
        "df2 = df2.append([{'Pattern':\"Ok, i am good\",\"Responses\":\"I hope I was able to assist you, Good Bye\",\"Tag\":\"Exit\",\"Target\":2 },\n",
        "                  {'Pattern':\"Alright, i'm good\",\"Responses\":\"I hope I was able to assist you, Good Bye\",\"Tag\":\"Exit\",\"Target\":2},\n",
        "                  {'Pattern':\"Thanks for help\",\"Responses\":\"I hope I was able to assist you, Good Bye\",\"Tag\":\"Exit\",\"Target\":2 },\n",
        "                  {'Pattern':\"Doubt cleared\",\"Responses\":\"I hope I was able to assist you, Good Bye\",\"Tag\":\"Exit\",\"Target\":2},\n",
        "                  {'Pattern':\"Exit\",\"Responses\":\"I hope I was able to assist you, Good Bye\",\"Tag\":\"Exit\",\"Target\":2},\n",
        "                  {'Pattern':\"quit cleared\",\"Responses\":\"I hope I was able to assist you, Good Bye\",\"Tag\":\"Exit\",\"Target\":2},\n",
        "                  {'Pattern':\"Portal not working\",\"Responses\":\"Link: Olympus wiki\",\"Tag\":\"Olympus\",\"Target\":5 },\n",
        "                  {'Pattern':\"Give me portal link\",\"Responses\":\"Link: Olympus wiki\",\"Tag\":\"Olympus\",\"Target\":5},\n",
        "                  {'Pattern':\"Olympus Portal\",\"Responses\":\"Link: Olympus wiki\",\"Tag\":\"Olympus\",\"Target\":5 },\n",
        "                  {'Pattern':\"PG portal\",\"Responses\":\"Link: Olympus wiki\",\"Tag\":\"Olympus\",\"Target\":5},\n",
        "                  {'Pattern':\"Course portal\",\"Responses\":\"Link: Olympus wiki\",\"Tag\":\"Olympus\",\"Target\":5},\n",
        "                  {'Pattern':\"Poc for olympus portal\",\"Responses\":\"Link: Olympus wiki\",\"Tag\":\"Olympus\",\"Target\":5},\n",
        "                  {'Pattern':\"Help me on olympus portal\",\"Responses\":\"Link: Olympus wiki\",\"Tag\":\"Olympus\",\"Target\":5},\n",
        "                  {'Pattern':\"I dont see a solution\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"Where is the solution\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"connect me to a human\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"Want to talk to a person\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"This is not an answer\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"Its not a solution\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"This is not a solution\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"Please push it to a human\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"I do not want to talk to a bot\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"Do not connect to a bot\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"Someone please answer my question\",\"Responses\":\"Tarnsferring the request to your PM\",\"Tag\":\"Ticket\",\"Target\":7 },\n",
        "                  {'Pattern':\"Get lost\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"You stupid\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Useless bot\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Get lost\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Dont talk like an idiot\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Do you have sense\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Senseless creature\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Shut your mouth\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Dont speak like a dummy\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"You stupid dummy\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Shut up\",\"Responses\":\"Please use respectful words\",\"Tag\":\"Profane\",\"Target\":6 },\n",
        "                  {'Pattern':\"Do i know you\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Is anyone there\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Who is there\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Who is you\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Your name please\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"name please\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Speak up\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Are you a human\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Do you answer questions\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Who is learning assistant\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"who is supporting assistant\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 },\n",
        "                  {'Pattern':\"Any assistant\",\"Responses\":\"I am your virtual learning assistant\",\"Tag\":\"Bot\",\"Target\":1 }],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7yMwqM2Y4jQ",
        "outputId": "87b0c15d-c85c-40ca-a1b5-a3560921b191"
      },
      "source": [
        "df2['Target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    29\n",
              "4    24\n",
              "2    22\n",
              "7    20\n",
              "6    20\n",
              "5    20\n",
              "1    20\n",
              "0    20\n",
              "Name: Target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w34oKSUSmB8V",
        "outputId": "ebd9b1f1-145f-4cdf-dda0-dc688bc5258a"
      },
      "source": [
        "df2['Responses'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVSUH41qkopi"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbzCIRyVoKUT"
      },
      "source": [
        "cleaned_text1 = [[i for i in nltk.word_tokenize(\"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",t.lower()) if i not in string.punctuation]))] for t in df2['Pattern']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UpGDClpoOdL"
      },
      "source": [
        "PS = PorterStemmer()\n",
        "WNL = WordNetLemmatizer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cleaned_text1, df2['Target'], test_size=0.3, random_state=11)\n",
        "\n",
        "X_train_L = []\n",
        "for i in X_train:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(WNL.lemmatize(j,pos='v'))\n",
        "  X_train_L.append(m)\n",
        "\n",
        "\n",
        "X_test_L = []\n",
        "for i in X_test:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(WNL.lemmatize(j,pos='v'))\n",
        "  X_test_L.append(m)\n",
        "\n",
        "#Using Lemmatization\n",
        "\n",
        "X_train_S = []\n",
        "for i in X_train:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(PS.stem(j))\n",
        "  X_train_S.append(m)\n",
        "\n",
        "\n",
        "X_test_S = []\n",
        "for i in X_test:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(PS.stem(j))\n",
        "  X_test_S.append(m)\n",
        "\n",
        "#Using Stemmer\n",
        "\n",
        "X_train_S = [\" \".join(i) for i in X_train_S]\n",
        "X_test_S = [\" \".join(i) for i in X_test_S]\n",
        "tfs = TfidfVectorizer()\n",
        "X_train_S_tf = tfs.fit_transform(X_train_S)\n",
        "X_test_S_tf = tfs.transform(X_test_S)\n",
        "\n",
        "X_train_L = [\" \".join(i) for i in X_train_L]\n",
        "X_test_L = [\" \".join(i) for i in X_test_L]\n",
        "tfL = TfidfVectorizer()\n",
        "X_train_L_tf = tfL.fit_transform(X_train_L)\n",
        "X_test_L_tf = tfL.transform(X_test_L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2vMTSBYNexa"
      },
      "source": [
        "1. Naive Bayes theorem using Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZquSKvUuUiC"
      },
      "source": [
        "model_nb = MultinomialNB()\n",
        "model_nb.fit(X_train_L_tf,y_train)\n",
        "pred_L_tf = model_nb.predict(X_test_L_tf)\n",
        "\n",
        "#using LEmma data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNXB5d1jv_FO",
        "outputId": "c0fd238f-0f39-416b-a66b-b0b81b40bce0"
      },
      "source": [
        "model_nb.predict(X_test_L_tf[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXd0RFXCwhbF",
        "outputId": "7d6f4625-39b1-49a3-8602-a0368df9387c"
      },
      "source": [
        "X_test_L_tf[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 157)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQY3k_wAMWml",
        "outputId": "507d845c-8087-45aa-b95d-de8e4d2645d7"
      },
      "source": [
        "print(classification_report(y_test,pred_L_tf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.29      0.44         7\n",
            "           1       0.83      0.71      0.77         7\n",
            "           2       0.60      0.60      0.60         5\n",
            "           3       0.41      1.00      0.58         9\n",
            "           4       1.00      0.38      0.55         8\n",
            "           5       1.00      1.00      1.00         8\n",
            "           6       0.67      0.50      0.57         4\n",
            "           7       0.75      0.60      0.67         5\n",
            "\n",
            "    accuracy                           0.66        53\n",
            "   macro avg       0.78      0.63      0.65        53\n",
            "weighted avg       0.79      0.66      0.65        53\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4vBfswkNizA"
      },
      "source": [
        "2. Naive Bayes theorem using Stemmed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EDYQwVwNRYn"
      },
      "source": [
        "model_nb_S = MultinomialNB(alpha=0.5)\n",
        "model_nb_S.fit(X_train_S_tf,y_train)\n",
        "pred_S_tf = model_nb_S.predict(X_test_S_tf)\n",
        "\n",
        "#using Stem data - Better accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK3vU0KANYLk",
        "outputId": "156005f2-515c-440b-e93b-f013df532e12"
      },
      "source": [
        "print(classification_report(y_test,pred_S_tf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.29      0.44         7\n",
            "           1       0.75      0.86      0.80         7\n",
            "           2       0.60      0.60      0.60         5\n",
            "           3       0.47      1.00      0.64         9\n",
            "           4       1.00      0.50      0.67         8\n",
            "           5       1.00      1.00      1.00         8\n",
            "           6       0.67      0.50      0.57         4\n",
            "           7       0.75      0.60      0.67         5\n",
            "\n",
            "    accuracy                           0.70        53\n",
            "   macro avg       0.78      0.67      0.67        53\n",
            "weighted avg       0.79      0.70      0.69        53\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5suMr391q-XK"
      },
      "source": [
        "pk_du = pickle.dump(model_nb_S,open('/content/drive/My Drive/Data Science/NLP/Project1/model_nb_S.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNMJMUfEOFi3"
      },
      "source": [
        "3. Decision Tree Classifier using Lemmatized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boqpI3UhMa4a"
      },
      "source": [
        "DTC = DecisionTreeClassifier(max_depth=5)\n",
        "DTC.fit(X_train_L_tf,y_train)\n",
        "pred_L_tf_DTC = model_nb.predict(X_test_L_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivrwQBdWMepO",
        "outputId": "5467f44a-89f2-431d-be6e-30bc642c58fa"
      },
      "source": [
        "print(classification_report(y_test,pred_L_tf_DTC))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.29      0.44         7\n",
            "           1       0.83      0.71      0.77         7\n",
            "           2       0.60      0.60      0.60         5\n",
            "           3       0.41      1.00      0.58         9\n",
            "           4       1.00      0.38      0.55         8\n",
            "           5       1.00      1.00      1.00         8\n",
            "           6       0.67      0.50      0.57         4\n",
            "           7       0.75      0.60      0.67         5\n",
            "\n",
            "    accuracy                           0.66        53\n",
            "   macro avg       0.78      0.63      0.65        53\n",
            "weighted avg       0.79      0.66      0.65        53\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDqQTl1yOLZQ"
      },
      "source": [
        "4. Decision Tree Classifier using stemmed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-JfcfDIN4iK"
      },
      "source": [
        "DTC_s = DecisionTreeClassifier(max_depth=5)\n",
        "DTC_s.fit(X_train_S_tf,y_train)\n",
        "pred_s_tf_DTC = DTC_s.predict(X_test_S_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5uVrKnIOV1_",
        "outputId": "64c45c46-5e68-4c67-bebc-e4a931eeb1e1"
      },
      "source": [
        "print(classification_report(y_test,pred_s_tf_DTC))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         7\n",
            "           1       1.00      0.14      0.25         7\n",
            "           2       1.00      0.20      0.33         5\n",
            "           3       1.00      0.44      0.62         9\n",
            "           4       0.20      1.00      0.33         8\n",
            "           5       1.00      0.62      0.77         8\n",
            "           6       0.00      0.00      0.00         4\n",
            "           7       0.50      0.20      0.29         5\n",
            "\n",
            "    accuracy                           0.38        53\n",
            "   macro avg       0.59      0.33      0.32        53\n",
            "weighted avg       0.62      0.38      0.36        53\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufLez6-RkhYh"
      },
      "source": [
        "5. Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YwXChlUkY1k"
      },
      "source": [
        "RFC = RandomForestClassifier()\n",
        "param_grid = {\"n_estimators\":[50,100,150,200,250,300,350,400,450,500],\"criterion\":['gini','entropy']}\n",
        "\n",
        "# , , , min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
        "# max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
        "# n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None\n",
        "\n",
        "\n",
        "GSCV = GridSearchCV(estimator=RFC,param_grid=param_grid)\n",
        "GSCV.fit(X_train_L_tf,y_train)\n",
        "pred_L_tf_RFC = GSCV.predict(X_test_L_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-tPzlTdkcL4",
        "outputId": "c56f1403-3020-40cf-b706-7678a2e13a3f"
      },
      "source": [
        "print(classification_report(y_test,pred_L_tf_RFC))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.57      0.53         7\n",
            "           1       0.80      0.57      0.67         7\n",
            "           2       0.75      0.60      0.67         5\n",
            "           3       0.45      1.00      0.62         9\n",
            "           4       1.00      0.38      0.55         8\n",
            "           5       1.00      1.00      1.00         8\n",
            "           6       1.00      0.50      0.67         4\n",
            "           7       1.00      0.60      0.75         5\n",
            "\n",
            "    accuracy                           0.68        53\n",
            "   macro avg       0.81      0.65      0.68        53\n",
            "weighted avg       0.79      0.68      0.68        53\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ch-0pDPlGvC",
        "outputId": "85d92816-10eb-4551-9e28-4ff07c6f1b9e"
      },
      "source": [
        "X_train_L_tf1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(122, 157)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5IOzd1zy4xn",
        "outputId": "9cb9365c-98c9-4a95-f58c-aba28d7dde65"
      },
      "source": [
        "X_train_S_tf1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(122, 158)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0METnpQc8TWs"
      },
      "source": [
        "6. Lets use Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-58qNOe6V0o"
      },
      "source": [
        "X_train_L_tf1 = X_train_L_tf.toarray()\n",
        "X_test_L_tf1 = X_test_L_tf.toarray()\n",
        "X_train_S_tf1 = X_train_S_tf.toarray()\n",
        "X_test_S_tf1 = X_test_S_tf.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twRzNjg963CC"
      },
      "source": [
        "y_train_tc = to_categorical(y_train,num_classes=8)\n",
        "y_test_tc = to_categorical(y_test,num_classes=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjCb053ly6XE"
      },
      "source": [
        "NN_s = Sequential()\n",
        "NN_s.add(Dense(200,input_shape=(158,),activation='relu'))\n",
        "NN_s.add(Dense(150,activation='relu'))\n",
        "NN_s.add(Dense(100,activation='relu'))\n",
        "NN_s.add(Dense(50,activation='relu'))\n",
        "NN_s.add(Dense(25,activation='relu'))\n",
        "NN_s.add(Dense(8,activation='softmax'))\n",
        "\n",
        "NN_L = Sequential()\n",
        "NN_L.add(Dense(200,input_shape=(157,),activation='relu'))\n",
        "NN_L.add(Dense(150,activation='relu'))\n",
        "NN_L.add(Dense(100,activation='relu'))\n",
        "NN_L.add(Dense(50,activation='relu'))\n",
        "NN_L.add(Dense(25,activation='relu'))\n",
        "NN_L.add(Dense(8,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ7RReAs835T"
      },
      "source": [
        "* Neural Network with Lemmatized data - Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Q35BKLzpsN",
        "outputId": "62052996-8e8c-4ce3-b795-eba812a21f32"
      },
      "source": [
        "NN_L1 = Sequential()\n",
        "NN_L1.add(Dense(200,input_shape=(157,),activation='relu'))\n",
        "NN_L1.add(Dense(150,activation='relu'))\n",
        "NN_L1.add(Dense(100,activation='relu'))\n",
        "NN_L1.add(Dense(50,activation='relu'))\n",
        "NN_L1.add(Dense(25,activation='relu'))\n",
        "NN_L1.add(Dense(8,activation='softmax'))\n",
        "NN_L1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "NN_L1.fit(X_train_L_tf1,y_train_tc,epochs=100,validation_data=(X_test_L_tf1,y_test_tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 72ms/step - loss: 2.0765 - accuracy: 0.1311 - val_loss: 2.0749 - val_accuracy: 0.1509\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0525 - accuracy: 0.1803 - val_loss: 2.0654 - val_accuracy: 0.1698\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 2.0262 - accuracy: 0.2049 - val_loss: 2.0511 - val_accuracy: 0.1698\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 1.9916 - accuracy: 0.2951 - val_loss: 2.0307 - val_accuracy: 0.1698\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.9435 - accuracy: 0.3361 - val_loss: 2.0037 - val_accuracy: 0.1887\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 1.8791 - accuracy: 0.3689 - val_loss: 1.9663 - val_accuracy: 0.2075\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.7969 - accuracy: 0.4016 - val_loss: 1.9180 - val_accuracy: 0.2642\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 1.6843 - accuracy: 0.4754 - val_loss: 1.8532 - val_accuracy: 0.2830\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.5499 - accuracy: 0.5492 - val_loss: 1.7794 - val_accuracy: 0.3019\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.3886 - accuracy: 0.5902 - val_loss: 1.7007 - val_accuracy: 0.3019\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.1988 - accuracy: 0.6311 - val_loss: 1.6035 - val_accuracy: 0.3208\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.0000 - accuracy: 0.6721 - val_loss: 1.5312 - val_accuracy: 0.3774\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.8238 - accuracy: 0.7541 - val_loss: 1.4382 - val_accuracy: 0.3962\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.6515 - accuracy: 0.7951 - val_loss: 1.3701 - val_accuracy: 0.4717\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5099 - accuracy: 0.8607 - val_loss: 1.3042 - val_accuracy: 0.4717\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.3889 - accuracy: 0.8770 - val_loss: 1.2485 - val_accuracy: 0.4906\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2947 - accuracy: 0.8934 - val_loss: 1.2418 - val_accuracy: 0.5094\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2465 - accuracy: 0.9180 - val_loss: 1.1756 - val_accuracy: 0.5094\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1717 - accuracy: 0.9672 - val_loss: 1.2245 - val_accuracy: 0.5849\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1324 - accuracy: 0.9918 - val_loss: 1.1388 - val_accuracy: 0.6226\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 0.9918 - val_loss: 1.1673 - val_accuracy: 0.5849\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0701 - accuracy: 0.9918 - val_loss: 1.1577 - val_accuracy: 0.6038\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0697 - accuracy: 0.9918 - val_loss: 1.1117 - val_accuracy: 0.6415\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0429 - accuracy: 0.9918 - val_loss: 1.1394 - val_accuracy: 0.6038\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0352 - accuracy: 0.9918 - val_loss: 1.1544 - val_accuracy: 0.6226\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 1.1105 - val_accuracy: 0.6792\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0370 - accuracy: 0.9918 - val_loss: 1.1376 - val_accuracy: 0.6792\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0279 - accuracy: 0.9918 - val_loss: 1.2281 - val_accuracy: 0.6038\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0347 - accuracy: 0.9918 - val_loss: 1.1903 - val_accuracy: 0.6226\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0243 - accuracy: 0.9918 - val_loss: 1.1503 - val_accuracy: 0.6604\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0235 - accuracy: 0.9918 - val_loss: 1.1649 - val_accuracy: 0.6604\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0209 - accuracy: 0.9836 - val_loss: 1.1609 - val_accuracy: 0.6604\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0221 - accuracy: 0.9836 - val_loss: 1.1607 - val_accuracy: 0.6604\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0148 - accuracy: 0.9918 - val_loss: 1.1852 - val_accuracy: 0.6415\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0175 - accuracy: 0.9918 - val_loss: 1.1766 - val_accuracy: 0.6792\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0220 - accuracy: 0.9918 - val_loss: 1.1899 - val_accuracy: 0.6981\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0140 - accuracy: 0.9918 - val_loss: 1.1978 - val_accuracy: 0.6604\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0191 - accuracy: 0.9918 - val_loss: 1.2021 - val_accuracy: 0.6604\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9918 - val_loss: 1.1988 - val_accuracy: 0.6604\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0152 - accuracy: 0.9918 - val_loss: 1.2028 - val_accuracy: 0.6604\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 0.9918 - val_loss: 1.2112 - val_accuracy: 0.6604\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0156 - accuracy: 0.9836 - val_loss: 1.2220 - val_accuracy: 0.6604\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0172 - accuracy: 0.9836 - val_loss: 1.2234 - val_accuracy: 0.6604\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0140 - accuracy: 0.9836 - val_loss: 1.2366 - val_accuracy: 0.6604\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9918 - val_loss: 1.2891 - val_accuracy: 0.6226\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0253 - accuracy: 0.9836 - val_loss: 1.2360 - val_accuracy: 0.6792\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 1.2465 - val_accuracy: 0.6792\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0110 - accuracy: 0.9918 - val_loss: 1.2742 - val_accuracy: 0.6226\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0324 - accuracy: 0.9918 - val_loss: 1.3639 - val_accuracy: 0.6038\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 1.2390 - val_accuracy: 0.6604\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 1.2778 - val_accuracy: 0.6604\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0219 - accuracy: 0.9918 - val_loss: 1.2548 - val_accuracy: 0.6792\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0140 - accuracy: 0.9918 - val_loss: 1.2477 - val_accuracy: 0.6604\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0149 - accuracy: 0.9918 - val_loss: 1.2539 - val_accuracy: 0.6604\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0136 - accuracy: 0.9918 - val_loss: 1.2550 - val_accuracy: 0.6604\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0164 - accuracy: 0.9836 - val_loss: 1.2643 - val_accuracy: 0.6792\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0193 - accuracy: 0.9918 - val_loss: 1.2677 - val_accuracy: 0.6792\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0190 - accuracy: 0.9836 - val_loss: 1.3325 - val_accuracy: 0.6038\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0284 - accuracy: 0.9918 - val_loss: 1.3662 - val_accuracy: 0.6038\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0128 - accuracy: 0.9918 - val_loss: 1.2637 - val_accuracy: 0.6604\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0147 - accuracy: 0.9918 - val_loss: 1.3255 - val_accuracy: 0.6604\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0257 - accuracy: 0.9918 - val_loss: 1.3080 - val_accuracy: 0.6604\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0291 - accuracy: 0.9836 - val_loss: 1.2856 - val_accuracy: 0.6415\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0165 - accuracy: 0.9918 - val_loss: 1.2651 - val_accuracy: 0.6415\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0148 - accuracy: 0.9918 - val_loss: 1.2647 - val_accuracy: 0.6415\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0136 - accuracy: 0.9918 - val_loss: 1.2676 - val_accuracy: 0.6415\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0129 - accuracy: 0.9836 - val_loss: 1.2808 - val_accuracy: 0.6415\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0135 - accuracy: 0.9918 - val_loss: 1.2827 - val_accuracy: 0.6415\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0158 - accuracy: 0.9918 - val_loss: 1.2868 - val_accuracy: 0.6415\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 1.2879 - val_accuracy: 0.6792\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 1.3231 - val_accuracy: 0.6792\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0141 - accuracy: 0.9918 - val_loss: 1.2863 - val_accuracy: 0.6604\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0173 - accuracy: 0.9918 - val_loss: 1.3404 - val_accuracy: 0.6038\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0171 - accuracy: 0.9918 - val_loss: 1.3079 - val_accuracy: 0.6415\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 1.2935 - val_accuracy: 0.6604\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0149 - accuracy: 0.9918 - val_loss: 1.3042 - val_accuracy: 0.6604\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0176 - accuracy: 0.9918 - val_loss: 1.2925 - val_accuracy: 0.6415\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 1.2985 - val_accuracy: 0.6415\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0136 - accuracy: 0.9836 - val_loss: 1.2992 - val_accuracy: 0.6415\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0135 - accuracy: 0.9836 - val_loss: 1.3017 - val_accuracy: 0.6415\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0146 - accuracy: 0.9918 - val_loss: 1.3049 - val_accuracy: 0.6604\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0179 - accuracy: 0.9918 - val_loss: 1.3315 - val_accuracy: 0.6226\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0139 - accuracy: 0.9918 - val_loss: 1.3116 - val_accuracy: 0.6604\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0189 - accuracy: 0.9918 - val_loss: 1.3304 - val_accuracy: 0.6792\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0157 - accuracy: 0.9918 - val_loss: 1.3177 - val_accuracy: 0.6604\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 1.3379 - val_accuracy: 0.6604\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0174 - accuracy: 0.9918 - val_loss: 1.3666 - val_accuracy: 0.6226\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9918 - val_loss: 1.3177 - val_accuracy: 0.6604\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0129 - accuracy: 0.9918 - val_loss: 1.3199 - val_accuracy: 0.6415\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0129 - accuracy: 0.9918 - val_loss: 1.3186 - val_accuracy: 0.6415\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0139 - accuracy: 0.9918 - val_loss: 1.3191 - val_accuracy: 0.6415\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0133 - accuracy: 0.9918 - val_loss: 1.3202 - val_accuracy: 0.6415\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0137 - accuracy: 0.9836 - val_loss: 1.3267 - val_accuracy: 0.6415\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0149 - accuracy: 0.9836 - val_loss: 1.3237 - val_accuracy: 0.6415\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0140 - accuracy: 0.9918 - val_loss: 1.3256 - val_accuracy: 0.6415\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 1.3363 - val_accuracy: 0.6415\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0126 - accuracy: 0.9918 - val_loss: 1.3553 - val_accuracy: 0.6415\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0159 - accuracy: 0.9918 - val_loss: 1.3403 - val_accuracy: 0.6604\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0183 - accuracy: 0.9918 - val_loss: 1.3700 - val_accuracy: 0.6604\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0186 - accuracy: 0.9836 - val_loss: 1.3439 - val_accuracy: 0.6792\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa46df3a90>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HDh5drg9Euk"
      },
      "source": [
        "* Neural Network with Lemmatized data - SGD Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqWVQIVr9BhF",
        "outputId": "1d6d2168-cd9a-4f27-f7c9-6f3db6432e32"
      },
      "source": [
        "NN_L2 = Sequential()\n",
        "NN_L2.add(Dense(200,input_shape=(157,),activation='relu'))\n",
        "NN_L2.add(Dense(150,activation='relu'))\n",
        "NN_L2.add(Dense(100,activation='relu'))\n",
        "NN_L2.add(Dense(50,activation='relu'))\n",
        "NN_L2.add(Dense(25,activation='relu'))\n",
        "NN_L2.add(Dense(8,activation='softmax'))\n",
        "NN_L2.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "NN_L2.fit(X_train_L_tf1,y_train_tc,epochs=110,validation_data=(X_test_L_tf1,y_test_tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/110\n",
            "4/4 [==============================] - 1s 85ms/step - loss: 2.0795 - accuracy: 0.1230 - val_loss: 2.0756 - val_accuracy: 0.1698\n",
            "Epoch 2/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0787 - accuracy: 0.1475 - val_loss: 2.0755 - val_accuracy: 0.1698\n",
            "Epoch 3/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0780 - accuracy: 0.1557 - val_loss: 2.0755 - val_accuracy: 0.2264\n",
            "Epoch 4/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0773 - accuracy: 0.1967 - val_loss: 2.0754 - val_accuracy: 0.2264\n",
            "Epoch 5/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0767 - accuracy: 0.2049 - val_loss: 2.0753 - val_accuracy: 0.2453\n",
            "Epoch 6/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0762 - accuracy: 0.2295 - val_loss: 2.0752 - val_accuracy: 0.2642\n",
            "Epoch 7/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0756 - accuracy: 0.2459 - val_loss: 2.0750 - val_accuracy: 0.2453\n",
            "Epoch 8/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0753 - accuracy: 0.2459 - val_loss: 2.0748 - val_accuracy: 0.2453\n",
            "Epoch 9/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0746 - accuracy: 0.2131 - val_loss: 2.0746 - val_accuracy: 0.2453\n",
            "Epoch 10/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0741 - accuracy: 0.2213 - val_loss: 2.0744 - val_accuracy: 0.2453\n",
            "Epoch 11/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0735 - accuracy: 0.2295 - val_loss: 2.0741 - val_accuracy: 0.2453\n",
            "Epoch 12/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0729 - accuracy: 0.2459 - val_loss: 2.0740 - val_accuracy: 0.2453\n",
            "Epoch 13/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0724 - accuracy: 0.2541 - val_loss: 2.0738 - val_accuracy: 0.2075\n",
            "Epoch 14/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0720 - accuracy: 0.2377 - val_loss: 2.0736 - val_accuracy: 0.2075\n",
            "Epoch 15/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0713 - accuracy: 0.2295 - val_loss: 2.0734 - val_accuracy: 0.2075\n",
            "Epoch 16/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0711 - accuracy: 0.2213 - val_loss: 2.0733 - val_accuracy: 0.2075\n",
            "Epoch 17/110\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 2.0706 - accuracy: 0.2131 - val_loss: 2.0732 - val_accuracy: 0.1887\n",
            "Epoch 18/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0702 - accuracy: 0.2049 - val_loss: 2.0730 - val_accuracy: 0.1887\n",
            "Epoch 19/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0697 - accuracy: 0.1967 - val_loss: 2.0727 - val_accuracy: 0.1887\n",
            "Epoch 20/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0693 - accuracy: 0.2131 - val_loss: 2.0725 - val_accuracy: 0.1887\n",
            "Epoch 21/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0687 - accuracy: 0.1967 - val_loss: 2.0723 - val_accuracy: 0.1887\n",
            "Epoch 22/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0684 - accuracy: 0.1967 - val_loss: 2.0721 - val_accuracy: 0.1887\n",
            "Epoch 23/110\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 2.0679 - accuracy: 0.1885 - val_loss: 2.0719 - val_accuracy: 0.1887\n",
            "Epoch 24/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0675 - accuracy: 0.1639 - val_loss: 2.0717 - val_accuracy: 0.1698\n",
            "Epoch 25/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0671 - accuracy: 0.1639 - val_loss: 2.0715 - val_accuracy: 0.1698\n",
            "Epoch 26/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0665 - accuracy: 0.1639 - val_loss: 2.0712 - val_accuracy: 0.1698\n",
            "Epoch 27/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0660 - accuracy: 0.1639 - val_loss: 2.0710 - val_accuracy: 0.1698\n",
            "Epoch 28/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0654 - accuracy: 0.1639 - val_loss: 2.0708 - val_accuracy: 0.1698\n",
            "Epoch 29/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0650 - accuracy: 0.1639 - val_loss: 2.0705 - val_accuracy: 0.1698\n",
            "Epoch 30/110\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0648 - accuracy: 0.1639 - val_loss: 2.0704 - val_accuracy: 0.1698\n",
            "Epoch 31/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0641 - accuracy: 0.1639 - val_loss: 2.0704 - val_accuracy: 0.1698\n",
            "Epoch 32/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0637 - accuracy: 0.1639 - val_loss: 2.0703 - val_accuracy: 0.1698\n",
            "Epoch 33/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0635 - accuracy: 0.1639 - val_loss: 2.0702 - val_accuracy: 0.1698\n",
            "Epoch 34/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0630 - accuracy: 0.1639 - val_loss: 2.0702 - val_accuracy: 0.1698\n",
            "Epoch 35/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0626 - accuracy: 0.1639 - val_loss: 2.0701 - val_accuracy: 0.1698\n",
            "Epoch 36/110\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.0621 - accuracy: 0.1639 - val_loss: 2.0699 - val_accuracy: 0.1698\n",
            "Epoch 37/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0618 - accuracy: 0.1639 - val_loss: 2.0698 - val_accuracy: 0.1698\n",
            "Epoch 38/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0612 - accuracy: 0.1639 - val_loss: 2.0697 - val_accuracy: 0.1698\n",
            "Epoch 39/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0608 - accuracy: 0.1639 - val_loss: 2.0696 - val_accuracy: 0.1698\n",
            "Epoch 40/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0605 - accuracy: 0.1639 - val_loss: 2.0695 - val_accuracy: 0.1698\n",
            "Epoch 41/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0601 - accuracy: 0.1639 - val_loss: 2.0695 - val_accuracy: 0.1698\n",
            "Epoch 42/110\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0596 - accuracy: 0.1639 - val_loss: 2.0693 - val_accuracy: 0.1698\n",
            "Epoch 43/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0593 - accuracy: 0.1639 - val_loss: 2.0692 - val_accuracy: 0.1698\n",
            "Epoch 44/110\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 2.0589 - accuracy: 0.1639 - val_loss: 2.0691 - val_accuracy: 0.1698\n",
            "Epoch 45/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0585 - accuracy: 0.1639 - val_loss: 2.0690 - val_accuracy: 0.1698\n",
            "Epoch 46/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0580 - accuracy: 0.1639 - val_loss: 2.0690 - val_accuracy: 0.1698\n",
            "Epoch 47/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0577 - accuracy: 0.1639 - val_loss: 2.0689 - val_accuracy: 0.1698\n",
            "Epoch 48/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0573 - accuracy: 0.1639 - val_loss: 2.0687 - val_accuracy: 0.1698\n",
            "Epoch 49/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0568 - accuracy: 0.1639 - val_loss: 2.0686 - val_accuracy: 0.1698\n",
            "Epoch 50/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0565 - accuracy: 0.1639 - val_loss: 2.0684 - val_accuracy: 0.1698\n",
            "Epoch 51/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0561 - accuracy: 0.1639 - val_loss: 2.0684 - val_accuracy: 0.1698\n",
            "Epoch 52/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0558 - accuracy: 0.1639 - val_loss: 2.0683 - val_accuracy: 0.1698\n",
            "Epoch 53/110\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 2.0555 - accuracy: 0.1639 - val_loss: 2.0683 - val_accuracy: 0.1698\n",
            "Epoch 54/110\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 2.0550 - accuracy: 0.1639 - val_loss: 2.0681 - val_accuracy: 0.1698\n",
            "Epoch 55/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0546 - accuracy: 0.1639 - val_loss: 2.0681 - val_accuracy: 0.1698\n",
            "Epoch 56/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0543 - accuracy: 0.1639 - val_loss: 2.0680 - val_accuracy: 0.1698\n",
            "Epoch 57/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0540 - accuracy: 0.1639 - val_loss: 2.0679 - val_accuracy: 0.1698\n",
            "Epoch 58/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0536 - accuracy: 0.1639 - val_loss: 2.0678 - val_accuracy: 0.1698\n",
            "Epoch 59/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0533 - accuracy: 0.1639 - val_loss: 2.0678 - val_accuracy: 0.1698\n",
            "Epoch 60/110\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 2.0530 - accuracy: 0.1639 - val_loss: 2.0677 - val_accuracy: 0.1698\n",
            "Epoch 61/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0527 - accuracy: 0.1639 - val_loss: 2.0677 - val_accuracy: 0.1698\n",
            "Epoch 62/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0522 - accuracy: 0.1639 - val_loss: 2.0675 - val_accuracy: 0.1698\n",
            "Epoch 63/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0519 - accuracy: 0.1639 - val_loss: 2.0674 - val_accuracy: 0.1698\n",
            "Epoch 64/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0514 - accuracy: 0.1639 - val_loss: 2.0672 - val_accuracy: 0.1698\n",
            "Epoch 65/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0510 - accuracy: 0.1639 - val_loss: 2.0671 - val_accuracy: 0.1698\n",
            "Epoch 66/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0507 - accuracy: 0.1639 - val_loss: 2.0670 - val_accuracy: 0.1698\n",
            "Epoch 67/110\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 2.0503 - accuracy: 0.1639 - val_loss: 2.0669 - val_accuracy: 0.1698\n",
            "Epoch 68/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0500 - accuracy: 0.1639 - val_loss: 2.0668 - val_accuracy: 0.1698\n",
            "Epoch 69/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0497 - accuracy: 0.1639 - val_loss: 2.0667 - val_accuracy: 0.1698\n",
            "Epoch 70/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0493 - accuracy: 0.1639 - val_loss: 2.0666 - val_accuracy: 0.1698\n",
            "Epoch 71/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0490 - accuracy: 0.1639 - val_loss: 2.0666 - val_accuracy: 0.1698\n",
            "Epoch 72/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0487 - accuracy: 0.1639 - val_loss: 2.0665 - val_accuracy: 0.1698\n",
            "Epoch 73/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0483 - accuracy: 0.1639 - val_loss: 2.0663 - val_accuracy: 0.1698\n",
            "Epoch 74/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0479 - accuracy: 0.1639 - val_loss: 2.0663 - val_accuracy: 0.1698\n",
            "Epoch 75/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0476 - accuracy: 0.1639 - val_loss: 2.0663 - val_accuracy: 0.1698\n",
            "Epoch 76/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0471 - accuracy: 0.1639 - val_loss: 2.0662 - val_accuracy: 0.1698\n",
            "Epoch 77/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0469 - accuracy: 0.1639 - val_loss: 2.0660 - val_accuracy: 0.1698\n",
            "Epoch 78/110\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 2.0464 - accuracy: 0.1639 - val_loss: 2.0660 - val_accuracy: 0.1698\n",
            "Epoch 79/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0461 - accuracy: 0.1639 - val_loss: 2.0659 - val_accuracy: 0.1698\n",
            "Epoch 80/110\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0458 - accuracy: 0.1639 - val_loss: 2.0657 - val_accuracy: 0.1698\n",
            "Epoch 81/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0453 - accuracy: 0.1639 - val_loss: 2.0655 - val_accuracy: 0.1698\n",
            "Epoch 82/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0450 - accuracy: 0.1639 - val_loss: 2.0654 - val_accuracy: 0.1698\n",
            "Epoch 83/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0447 - accuracy: 0.1639 - val_loss: 2.0654 - val_accuracy: 0.1698\n",
            "Epoch 84/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0442 - accuracy: 0.1639 - val_loss: 2.0652 - val_accuracy: 0.1698\n",
            "Epoch 85/110\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0439 - accuracy: 0.1639 - val_loss: 2.0652 - val_accuracy: 0.1698\n",
            "Epoch 86/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0436 - accuracy: 0.1639 - val_loss: 2.0652 - val_accuracy: 0.1698\n",
            "Epoch 87/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0433 - accuracy: 0.1639 - val_loss: 2.0649 - val_accuracy: 0.1698\n",
            "Epoch 88/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0429 - accuracy: 0.1639 - val_loss: 2.0648 - val_accuracy: 0.1698\n",
            "Epoch 89/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0427 - accuracy: 0.1639 - val_loss: 2.0647 - val_accuracy: 0.1698\n",
            "Epoch 90/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0422 - accuracy: 0.1639 - val_loss: 2.0646 - val_accuracy: 0.1698\n",
            "Epoch 91/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0420 - accuracy: 0.1639 - val_loss: 2.0646 - val_accuracy: 0.1698\n",
            "Epoch 92/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0414 - accuracy: 0.1639 - val_loss: 2.0645 - val_accuracy: 0.1698\n",
            "Epoch 93/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0412 - accuracy: 0.1639 - val_loss: 2.0643 - val_accuracy: 0.1698\n",
            "Epoch 94/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0408 - accuracy: 0.1639 - val_loss: 2.0643 - val_accuracy: 0.1698\n",
            "Epoch 95/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0406 - accuracy: 0.1639 - val_loss: 2.0641 - val_accuracy: 0.1698\n",
            "Epoch 96/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0401 - accuracy: 0.1639 - val_loss: 2.0640 - val_accuracy: 0.1698\n",
            "Epoch 97/110\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 2.0399 - accuracy: 0.1639 - val_loss: 2.0638 - val_accuracy: 0.1698\n",
            "Epoch 98/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0393 - accuracy: 0.1639 - val_loss: 2.0638 - val_accuracy: 0.1698\n",
            "Epoch 99/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0390 - accuracy: 0.1639 - val_loss: 2.0636 - val_accuracy: 0.1698\n",
            "Epoch 100/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0386 - accuracy: 0.1639 - val_loss: 2.0634 - val_accuracy: 0.1698\n",
            "Epoch 101/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0381 - accuracy: 0.1639 - val_loss: 2.0634 - val_accuracy: 0.1698\n",
            "Epoch 102/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0378 - accuracy: 0.1639 - val_loss: 2.0633 - val_accuracy: 0.1698\n",
            "Epoch 103/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0377 - accuracy: 0.1639 - val_loss: 2.0631 - val_accuracy: 0.1698\n",
            "Epoch 104/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0372 - accuracy: 0.1639 - val_loss: 2.0629 - val_accuracy: 0.1698\n",
            "Epoch 105/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0370 - accuracy: 0.1639 - val_loss: 2.0627 - val_accuracy: 0.1698\n",
            "Epoch 106/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0364 - accuracy: 0.1639 - val_loss: 2.0625 - val_accuracy: 0.1698\n",
            "Epoch 107/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0362 - accuracy: 0.1639 - val_loss: 2.0624 - val_accuracy: 0.1698\n",
            "Epoch 108/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0357 - accuracy: 0.1639 - val_loss: 2.0622 - val_accuracy: 0.1698\n",
            "Epoch 109/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0352 - accuracy: 0.1639 - val_loss: 2.0621 - val_accuracy: 0.1698\n",
            "Epoch 110/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0349 - accuracy: 0.1639 - val_loss: 2.0620 - val_accuracy: 0.1698\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa46e31890>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gnGZ6ab9X-G"
      },
      "source": [
        "* Neural Network with Lemmatized data - RMS prop optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buDqWUcz9VDL",
        "outputId": "2dbaecce-3972-4b1f-b352-8ff20eeeefc0"
      },
      "source": [
        "NN_L3 = Sequential()\n",
        "NN_L3.add(Dense(200,input_shape=(157,),activation='relu'))\n",
        "NN_L3.add(Dense(150,activation='relu'))\n",
        "NN_L3.add(Dense(100,activation='relu'))\n",
        "NN_L3.add(Dense(50,activation='relu'))\n",
        "NN_L3.add(Dense(25,activation='relu'))\n",
        "NN_L3.add(Dense(8,activation='softmax'))\n",
        "NN_L3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "NN_L3.fit(X_train_L_tf1,y_train_tc,epochs=110,validation_data=(X_test_L_tf1,y_test_tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/110\n",
            "4/4 [==============================] - 1s 76ms/step - loss: 2.0785 - accuracy: 0.1803 - val_loss: 2.0679 - val_accuracy: 0.1887\n",
            "Epoch 2/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0432 - accuracy: 0.3033 - val_loss: 2.0513 - val_accuracy: 0.1509\n",
            "Epoch 3/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.9838 - accuracy: 0.3770 - val_loss: 2.0003 - val_accuracy: 0.1887\n",
            "Epoch 4/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.8845 - accuracy: 0.4590 - val_loss: 1.9303 - val_accuracy: 0.2830\n",
            "Epoch 5/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.7520 - accuracy: 0.5656 - val_loss: 1.8335 - val_accuracy: 0.4528\n",
            "Epoch 6/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 1.6287 - accuracy: 0.5410 - val_loss: 1.7647 - val_accuracy: 0.3962\n",
            "Epoch 7/110\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 1.4289 - accuracy: 0.6967 - val_loss: 1.6714 - val_accuracy: 0.4340\n",
            "Epoch 8/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 1.2094 - accuracy: 0.7787 - val_loss: 1.5171 - val_accuracy: 0.5472\n",
            "Epoch 9/110\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.9731 - accuracy: 0.8934 - val_loss: 1.4362 - val_accuracy: 0.5283\n",
            "Epoch 10/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.7635 - accuracy: 0.8934 - val_loss: 1.3624 - val_accuracy: 0.5849\n",
            "Epoch 11/110\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5946 - accuracy: 0.9344 - val_loss: 1.2246 - val_accuracy: 0.6604\n",
            "Epoch 12/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4156 - accuracy: 0.9836 - val_loss: 1.2210 - val_accuracy: 0.5472\n",
            "Epoch 13/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3101 - accuracy: 0.9754 - val_loss: 1.1664 - val_accuracy: 0.6415\n",
            "Epoch 14/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2077 - accuracy: 0.9918 - val_loss: 1.2590 - val_accuracy: 0.5472\n",
            "Epoch 15/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1780 - accuracy: 0.9918 - val_loss: 1.1636 - val_accuracy: 0.6226\n",
            "Epoch 16/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1116 - accuracy: 0.9918 - val_loss: 1.1344 - val_accuracy: 0.6415\n",
            "Epoch 17/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0764 - accuracy: 0.9918 - val_loss: 1.1100 - val_accuracy: 0.6415\n",
            "Epoch 18/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0853 - accuracy: 0.9918 - val_loss: 1.2228 - val_accuracy: 0.5849\n",
            "Epoch 19/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0605 - accuracy: 0.9918 - val_loss: 1.3625 - val_accuracy: 0.5660\n",
            "Epoch 20/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0751 - accuracy: 0.9918 - val_loss: 1.1233 - val_accuracy: 0.6604\n",
            "Epoch 21/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0418 - accuracy: 0.9918 - val_loss: 1.1552 - val_accuracy: 0.6226\n",
            "Epoch 22/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0330 - accuracy: 0.9918 - val_loss: 1.3414 - val_accuracy: 0.6226\n",
            "Epoch 23/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0403 - accuracy: 0.9918 - val_loss: 1.1502 - val_accuracy: 0.6792\n",
            "Epoch 24/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0327 - accuracy: 0.9918 - val_loss: 1.3562 - val_accuracy: 0.6038\n",
            "Epoch 25/110\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0491 - accuracy: 0.9836 - val_loss: 1.2668 - val_accuracy: 0.6792\n",
            "Epoch 26/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0463 - accuracy: 0.9836 - val_loss: 1.1887 - val_accuracy: 0.6604\n",
            "Epoch 27/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0384 - accuracy: 0.9836 - val_loss: 1.2882 - val_accuracy: 0.6604\n",
            "Epoch 28/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0389 - accuracy: 0.9836 - val_loss: 1.2589 - val_accuracy: 0.6604\n",
            "Epoch 29/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0258 - accuracy: 0.9918 - val_loss: 1.2481 - val_accuracy: 0.6981\n",
            "Epoch 30/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0227 - accuracy: 0.9918 - val_loss: 1.3707 - val_accuracy: 0.6792\n",
            "Epoch 31/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 1.2620 - val_accuracy: 0.6981\n",
            "Epoch 32/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0163 - accuracy: 0.9918 - val_loss: 1.2439 - val_accuracy: 0.7170\n",
            "Epoch 33/110\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0369 - accuracy: 0.9836 - val_loss: 1.3972 - val_accuracy: 0.6792\n",
            "Epoch 34/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0373 - accuracy: 0.9836 - val_loss: 1.3141 - val_accuracy: 0.6792\n",
            "Epoch 35/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0328 - accuracy: 0.9836 - val_loss: 1.2546 - val_accuracy: 0.7358\n",
            "Epoch 36/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0167 - accuracy: 0.9918 - val_loss: 1.2684 - val_accuracy: 0.7170\n",
            "Epoch 37/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0297 - accuracy: 0.9836 - val_loss: 1.2834 - val_accuracy: 0.7170\n",
            "Epoch 38/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0318 - accuracy: 0.9836 - val_loss: 1.3320 - val_accuracy: 0.6792\n",
            "Epoch 39/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0287 - accuracy: 0.9836 - val_loss: 1.3608 - val_accuracy: 0.7170\n",
            "Epoch 40/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0224 - accuracy: 0.9918 - val_loss: 1.3250 - val_accuracy: 0.7170\n",
            "Epoch 41/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0202 - accuracy: 0.9918 - val_loss: 1.3397 - val_accuracy: 0.7170\n",
            "Epoch 42/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0213 - accuracy: 0.9918 - val_loss: 1.4791 - val_accuracy: 0.6604\n",
            "Epoch 43/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0212 - accuracy: 0.9918 - val_loss: 1.3649 - val_accuracy: 0.7170\n",
            "Epoch 44/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0251 - accuracy: 0.9836 - val_loss: 1.3483 - val_accuracy: 0.7170\n",
            "Epoch 45/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0248 - accuracy: 0.9836 - val_loss: 1.3460 - val_accuracy: 0.7170\n",
            "Epoch 46/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0139 - accuracy: 0.9918 - val_loss: 1.3595 - val_accuracy: 0.7358\n",
            "Epoch 47/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 1.3662 - val_accuracy: 0.7170\n",
            "Epoch 48/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0274 - accuracy: 0.9836 - val_loss: 1.5813 - val_accuracy: 0.6226\n",
            "Epoch 49/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0277 - accuracy: 0.9836 - val_loss: 1.3714 - val_accuracy: 0.7170\n",
            "Epoch 50/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0231 - accuracy: 0.9836 - val_loss: 1.3767 - val_accuracy: 0.7170\n",
            "Epoch 51/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 1.4382 - val_accuracy: 0.7170\n",
            "Epoch 52/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0209 - accuracy: 0.9836 - val_loss: 1.4663 - val_accuracy: 0.6981\n",
            "Epoch 53/110\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0189 - accuracy: 0.9918 - val_loss: 1.3995 - val_accuracy: 0.7358\n",
            "Epoch 54/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0123 - accuracy: 0.9918 - val_loss: 1.4432 - val_accuracy: 0.7358\n",
            "Epoch 55/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0223 - accuracy: 0.9836 - val_loss: 1.5106 - val_accuracy: 0.6981\n",
            "Epoch 56/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0213 - accuracy: 0.9836 - val_loss: 1.6031 - val_accuracy: 0.6981\n",
            "Epoch 57/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0148 - accuracy: 0.9918 - val_loss: 1.4717 - val_accuracy: 0.7358\n",
            "Epoch 58/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0203 - accuracy: 0.9836 - val_loss: 1.5291 - val_accuracy: 0.7170\n",
            "Epoch 59/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0136 - accuracy: 0.9918 - val_loss: 1.5267 - val_accuracy: 0.7170\n",
            "Epoch 60/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 1.5384 - val_accuracy: 0.7358\n",
            "Epoch 61/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 1.5626 - val_accuracy: 0.7358\n",
            "Epoch 62/110\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 1.7440 - val_accuracy: 0.6604\n",
            "Epoch 63/110\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0207 - accuracy: 0.9836 - val_loss: 1.5343 - val_accuracy: 0.7358\n",
            "Epoch 64/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0174 - accuracy: 0.9918 - val_loss: 1.7092 - val_accuracy: 0.6792\n",
            "Epoch 65/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0174 - accuracy: 0.9918 - val_loss: 1.5904 - val_accuracy: 0.6981\n",
            "Epoch 66/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 1.5820 - val_accuracy: 0.7358\n",
            "Epoch 67/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0175 - accuracy: 0.9836 - val_loss: 1.5636 - val_accuracy: 0.7358\n",
            "Epoch 68/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0173 - accuracy: 0.9836 - val_loss: 1.5815 - val_accuracy: 0.7358\n",
            "Epoch 69/110\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0171 - accuracy: 0.9918 - val_loss: 1.6584 - val_accuracy: 0.6981\n",
            "Epoch 70/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0164 - accuracy: 0.9918 - val_loss: 1.6631 - val_accuracy: 0.6981\n",
            "Epoch 71/110\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 1.6331 - val_accuracy: 0.7358\n",
            "Epoch 72/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0177 - accuracy: 0.9918 - val_loss: 1.7965 - val_accuracy: 0.6981\n",
            "Epoch 73/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0170 - accuracy: 0.9836 - val_loss: 1.7056 - val_accuracy: 0.6981\n",
            "Epoch 74/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 1.6558 - val_accuracy: 0.7358\n",
            "Epoch 75/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0170 - accuracy: 0.9918 - val_loss: 1.7173 - val_accuracy: 0.6792\n",
            "Epoch 76/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0172 - accuracy: 0.9836 - val_loss: 1.7134 - val_accuracy: 0.7358\n",
            "Epoch 77/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0164 - accuracy: 0.9918 - val_loss: 1.8969 - val_accuracy: 0.6792\n",
            "Epoch 78/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0164 - accuracy: 0.9918 - val_loss: 1.8011 - val_accuracy: 0.6604\n",
            "Epoch 79/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9918 - val_loss: 1.7553 - val_accuracy: 0.7170\n",
            "Epoch 80/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 1.7652 - val_accuracy: 0.7170\n",
            "Epoch 81/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0156 - accuracy: 0.9918 - val_loss: 1.8466 - val_accuracy: 0.6981\n",
            "Epoch 82/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0166 - accuracy: 0.9918 - val_loss: 1.8219 - val_accuracy: 0.6981\n",
            "Epoch 83/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0151 - accuracy: 0.9836 - val_loss: 1.8170 - val_accuracy: 0.6604\n",
            "Epoch 84/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0154 - accuracy: 0.9918 - val_loss: 1.8727 - val_accuracy: 0.6981\n",
            "Epoch 85/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0152 - accuracy: 0.9836 - val_loss: 1.8850 - val_accuracy: 0.6604\n",
            "Epoch 86/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0120 - accuracy: 0.9918 - val_loss: 1.8242 - val_accuracy: 0.7170\n",
            "Epoch 87/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 0.9836 - val_loss: 1.8990 - val_accuracy: 0.6792\n",
            "Epoch 88/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0151 - accuracy: 0.9918 - val_loss: 2.0162 - val_accuracy: 0.6981\n",
            "Epoch 89/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.9836 - val_loss: 1.9831 - val_accuracy: 0.6981\n",
            "Epoch 90/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9918 - val_loss: 1.9631 - val_accuracy: 0.6981\n",
            "Epoch 91/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0146 - accuracy: 0.9836 - val_loss: 1.9747 - val_accuracy: 0.6981\n",
            "Epoch 92/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0145 - accuracy: 0.9918 - val_loss: 1.9622 - val_accuracy: 0.6981\n",
            "Epoch 93/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0143 - accuracy: 0.9836 - val_loss: 2.0134 - val_accuracy: 0.6981\n",
            "Epoch 94/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0147 - accuracy: 0.9836 - val_loss: 2.0173 - val_accuracy: 0.6981\n",
            "Epoch 95/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9836 - val_loss: 2.0191 - val_accuracy: 0.6981\n",
            "Epoch 96/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0118 - accuracy: 0.9918 - val_loss: 2.0123 - val_accuracy: 0.6981\n",
            "Epoch 97/110\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0148 - accuracy: 0.9918 - val_loss: 2.0315 - val_accuracy: 0.6415\n",
            "Epoch 98/110\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 1.9691 - val_accuracy: 0.7170\n",
            "Epoch 99/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0153 - accuracy: 0.9836 - val_loss: 2.0332 - val_accuracy: 0.7170\n",
            "Epoch 100/110\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0148 - accuracy: 0.9918 - val_loss: 2.1341 - val_accuracy: 0.6981\n",
            "Epoch 101/110\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 2.0376 - val_accuracy: 0.7170\n",
            "Epoch 102/110\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 2.0688 - val_accuracy: 0.7170\n",
            "Epoch 103/110\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0159 - accuracy: 0.9918 - val_loss: 2.1465 - val_accuracy: 0.6981\n",
            "Epoch 104/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0146 - accuracy: 0.9836 - val_loss: 2.1833 - val_accuracy: 0.6792\n",
            "Epoch 105/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0142 - accuracy: 0.9836 - val_loss: 2.1310 - val_accuracy: 0.6981\n",
            "Epoch 106/110\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0146 - accuracy: 0.9918 - val_loss: 2.1763 - val_accuracy: 0.6038\n",
            "Epoch 107/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0135 - accuracy: 0.9836 - val_loss: 2.1101 - val_accuracy: 0.6415\n",
            "Epoch 108/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 2.1240 - val_accuracy: 0.6792\n",
            "Epoch 109/110\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0144 - accuracy: 0.9918 - val_loss: 2.2470 - val_accuracy: 0.6792\n",
            "Epoch 110/110\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0137 - accuracy: 0.9836 - val_loss: 2.1927 - val_accuracy: 0.6981\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa57614390>"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBsWnoX69liR"
      },
      "source": [
        "* We ran Neural Network for the Lemmatized data with Adam, RMSprop and SGD optimizers without any hyperparameter tuning for 100 epochs, we notice that **RMSprop** with better accuracy on validation, we will focus on tweaking this optimizer and tune hyperparameters to get better model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_PSX86t_qcj",
        "outputId": "94c33d50-144f-4114-9d95-0943211f8ea0"
      },
      "source": [
        "NN_L3 = Sequential()\n",
        "NN_L3.add(Dense(200,input_shape=(157,),activation='relu'))\n",
        "NN_L3.add(Dropout(0.2))\n",
        "NN_L3.add(Dense(150,activation='relu'))\n",
        "NN_L3.add(Dropout(0.2))\n",
        "NN_L3.add(Dense(100,activation='relu'))\n",
        "NN_L3.add(Dropout(0.2))\n",
        "NN_L3.add(Dense(50,activation='relu'))\n",
        "NN_L3.add(Dropout(0.2))\n",
        "NN_L3.add(Dense(25,activation='relu'))\n",
        "NN_L3.add(Dense(8,activation='softmax'))\n",
        "rms = RMSprop(learning_rate=0.001, rho=0.2, momentum=0.8, epsilon=1e-04)\n",
        "ES=EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=10)\n",
        "NN_L3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "NN_L3.fit(X_train_L_tf1,y_train_tc,epochs=500,validation_data=(X_test_L_tf1,y_test_tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "4/4 [==============================] - 1s 76ms/step - loss: 2.0768 - accuracy: 0.1557 - val_loss: 2.0718 - val_accuracy: 0.2075\n",
            "Epoch 2/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0636 - accuracy: 0.2213 - val_loss: 2.0569 - val_accuracy: 0.3585\n",
            "Epoch 3/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0518 - accuracy: 0.2705 - val_loss: 2.0468 - val_accuracy: 0.3396\n",
            "Epoch 4/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0354 - accuracy: 0.2787 - val_loss: 2.0343 - val_accuracy: 0.3019\n",
            "Epoch 5/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.9995 - accuracy: 0.3934 - val_loss: 2.0107 - val_accuracy: 0.3396\n",
            "Epoch 6/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.9649 - accuracy: 0.3689 - val_loss: 1.9789 - val_accuracy: 0.3774\n",
            "Epoch 7/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.9195 - accuracy: 0.4426 - val_loss: 1.9405 - val_accuracy: 0.3208\n",
            "Epoch 8/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.8779 - accuracy: 0.4344 - val_loss: 1.8906 - val_accuracy: 0.3585\n",
            "Epoch 9/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.7626 - accuracy: 0.4836 - val_loss: 1.8446 - val_accuracy: 0.4151\n",
            "Epoch 10/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.6847 - accuracy: 0.5410 - val_loss: 1.7618 - val_accuracy: 0.4151\n",
            "Epoch 11/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.6220 - accuracy: 0.4918 - val_loss: 1.7043 - val_accuracy: 0.4528\n",
            "Epoch 12/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.4850 - accuracy: 0.5574 - val_loss: 1.6136 - val_accuracy: 0.4717\n",
            "Epoch 13/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 1.3746 - accuracy: 0.5656 - val_loss: 1.5640 - val_accuracy: 0.4528\n",
            "Epoch 14/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.2873 - accuracy: 0.6311 - val_loss: 1.4545 - val_accuracy: 0.5660\n",
            "Epoch 15/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.1996 - accuracy: 0.6721 - val_loss: 1.4035 - val_accuracy: 0.5472\n",
            "Epoch 16/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.0191 - accuracy: 0.7541 - val_loss: 1.3065 - val_accuracy: 0.5660\n",
            "Epoch 17/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.9403 - accuracy: 0.7131 - val_loss: 1.2991 - val_accuracy: 0.5094\n",
            "Epoch 18/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.9107 - accuracy: 0.7131 - val_loss: 1.2897 - val_accuracy: 0.6038\n",
            "Epoch 19/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.7719 - accuracy: 0.7377 - val_loss: 1.1811 - val_accuracy: 0.5660\n",
            "Epoch 20/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.6863 - accuracy: 0.8033 - val_loss: 1.1426 - val_accuracy: 0.5660\n",
            "Epoch 21/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.6080 - accuracy: 0.8115 - val_loss: 1.1140 - val_accuracy: 0.6038\n",
            "Epoch 22/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.5605 - accuracy: 0.8197 - val_loss: 1.0628 - val_accuracy: 0.6226\n",
            "Epoch 23/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.5151 - accuracy: 0.8525 - val_loss: 1.0700 - val_accuracy: 0.7170\n",
            "Epoch 24/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4946 - accuracy: 0.8607 - val_loss: 1.0051 - val_accuracy: 0.6604\n",
            "Epoch 25/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3540 - accuracy: 0.9426 - val_loss: 1.0207 - val_accuracy: 0.7170\n",
            "Epoch 26/500\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3875 - accuracy: 0.9016 - val_loss: 0.9773 - val_accuracy: 0.6604\n",
            "Epoch 27/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3367 - accuracy: 0.9344 - val_loss: 0.9715 - val_accuracy: 0.6604\n",
            "Epoch 28/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3627 - accuracy: 0.9262 - val_loss: 0.9515 - val_accuracy: 0.6604\n",
            "Epoch 29/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3246 - accuracy: 0.9098 - val_loss: 0.9697 - val_accuracy: 0.6415\n",
            "Epoch 30/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.2808 - accuracy: 0.9426 - val_loss: 0.9821 - val_accuracy: 0.6415\n",
            "Epoch 31/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.2070 - accuracy: 0.9836 - val_loss: 1.0267 - val_accuracy: 0.6038\n",
            "Epoch 32/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2021 - accuracy: 0.9672 - val_loss: 0.9882 - val_accuracy: 0.6226\n",
            "Epoch 33/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2137 - accuracy: 0.9508 - val_loss: 0.9898 - val_accuracy: 0.6415\n",
            "Epoch 34/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1689 - accuracy: 0.9672 - val_loss: 1.0113 - val_accuracy: 0.6604\n",
            "Epoch 35/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1591 - accuracy: 0.9590 - val_loss: 1.0066 - val_accuracy: 0.6415\n",
            "Epoch 36/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1886 - accuracy: 0.9426 - val_loss: 0.9638 - val_accuracy: 0.6415\n",
            "Epoch 37/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2001 - accuracy: 0.9590 - val_loss: 0.9454 - val_accuracy: 0.6792\n",
            "Epoch 38/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1661 - accuracy: 0.9426 - val_loss: 0.9871 - val_accuracy: 0.6792\n",
            "Epoch 39/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0985 - accuracy: 0.9836 - val_loss: 1.0209 - val_accuracy: 0.6604\n",
            "Epoch 40/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1276 - accuracy: 0.9672 - val_loss: 1.0500 - val_accuracy: 0.6226\n",
            "Epoch 41/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0780 - accuracy: 0.9918 - val_loss: 1.0045 - val_accuracy: 0.6604\n",
            "Epoch 42/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1000 - accuracy: 0.9672 - val_loss: 1.1619 - val_accuracy: 0.6226\n",
            "Epoch 43/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1149 - accuracy: 0.9672 - val_loss: 1.0277 - val_accuracy: 0.6415\n",
            "Epoch 44/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0842 - accuracy: 0.9672 - val_loss: 1.0854 - val_accuracy: 0.6038\n",
            "Epoch 45/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1174 - accuracy: 0.9754 - val_loss: 1.1253 - val_accuracy: 0.6604\n",
            "Epoch 46/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0891 - accuracy: 0.9836 - val_loss: 1.0432 - val_accuracy: 0.6604\n",
            "Epoch 47/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1076 - accuracy: 0.9672 - val_loss: 1.0496 - val_accuracy: 0.6415\n",
            "Epoch 48/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0962 - accuracy: 0.9754 - val_loss: 1.1079 - val_accuracy: 0.6226\n",
            "Epoch 49/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0752 - accuracy: 0.9754 - val_loss: 1.0517 - val_accuracy: 0.6604\n",
            "Epoch 50/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0376 - accuracy: 0.9918 - val_loss: 1.0437 - val_accuracy: 0.6415\n",
            "Epoch 51/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0468 - accuracy: 0.9918 - val_loss: 1.1017 - val_accuracy: 0.6226\n",
            "Epoch 52/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0491 - accuracy: 0.9918 - val_loss: 1.1164 - val_accuracy: 0.6792\n",
            "Epoch 53/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1379 - accuracy: 0.9590 - val_loss: 1.0792 - val_accuracy: 0.7170\n",
            "Epoch 54/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 1.1492 - val_accuracy: 0.6604\n",
            "Epoch 55/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0634 - accuracy: 0.9836 - val_loss: 1.1321 - val_accuracy: 0.7170\n",
            "Epoch 56/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0720 - accuracy: 0.9836 - val_loss: 1.1052 - val_accuracy: 0.6415\n",
            "Epoch 57/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0505 - accuracy: 0.9754 - val_loss: 1.0907 - val_accuracy: 0.6415\n",
            "Epoch 58/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0354 - accuracy: 0.9918 - val_loss: 1.2912 - val_accuracy: 0.6415\n",
            "Epoch 59/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0381 - accuracy: 0.9918 - val_loss: 1.2155 - val_accuracy: 0.6792\n",
            "Epoch 60/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0415 - accuracy: 0.9918 - val_loss: 1.1970 - val_accuracy: 0.7170\n",
            "Epoch 61/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0371 - accuracy: 0.9836 - val_loss: 1.1455 - val_accuracy: 0.6415\n",
            "Epoch 62/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0472 - accuracy: 0.9836 - val_loss: 1.1973 - val_accuracy: 0.6981\n",
            "Epoch 63/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0627 - accuracy: 0.9836 - val_loss: 1.1723 - val_accuracy: 0.7170\n",
            "Epoch 64/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0387 - accuracy: 0.9918 - val_loss: 1.2407 - val_accuracy: 0.7170\n",
            "Epoch 65/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 1.2243 - val_accuracy: 0.6415\n",
            "Epoch 66/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0357 - accuracy: 0.9836 - val_loss: 1.2396 - val_accuracy: 0.6604\n",
            "Epoch 67/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0467 - accuracy: 0.9918 - val_loss: 1.2378 - val_accuracy: 0.7170\n",
            "Epoch 68/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0504 - accuracy: 0.9918 - val_loss: 1.1549 - val_accuracy: 0.7170\n",
            "Epoch 69/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0223 - accuracy: 0.9918 - val_loss: 1.1506 - val_accuracy: 0.6604\n",
            "Epoch 70/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0474 - accuracy: 0.9918 - val_loss: 1.2361 - val_accuracy: 0.6226\n",
            "Epoch 71/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0383 - accuracy: 0.9918 - val_loss: 1.2167 - val_accuracy: 0.6415\n",
            "Epoch 72/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0366 - accuracy: 0.9918 - val_loss: 1.1904 - val_accuracy: 0.6981\n",
            "Epoch 73/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0536 - accuracy: 0.9836 - val_loss: 1.3029 - val_accuracy: 0.6415\n",
            "Epoch 74/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0154 - accuracy: 0.9918 - val_loss: 1.3232 - val_accuracy: 0.6981\n",
            "Epoch 75/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0354 - accuracy: 0.9836 - val_loss: 1.3189 - val_accuracy: 0.7170\n",
            "Epoch 76/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0467 - accuracy: 0.9836 - val_loss: 1.2284 - val_accuracy: 0.6415\n",
            "Epoch 77/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0552 - accuracy: 0.9836 - val_loss: 1.2707 - val_accuracy: 0.6604\n",
            "Epoch 78/500\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0201 - accuracy: 0.9918 - val_loss: 1.2504 - val_accuracy: 0.6415\n",
            "Epoch 79/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0207 - accuracy: 0.9918 - val_loss: 1.2729 - val_accuracy: 0.6415\n",
            "Epoch 80/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0315 - accuracy: 0.9918 - val_loss: 1.2722 - val_accuracy: 0.6604\n",
            "Epoch 81/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0280 - accuracy: 0.9836 - val_loss: 1.3293 - val_accuracy: 0.6792\n",
            "Epoch 82/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0331 - accuracy: 0.9836 - val_loss: 1.2977 - val_accuracy: 0.6604\n",
            "Epoch 83/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0335 - accuracy: 0.9836 - val_loss: 1.2237 - val_accuracy: 0.6792\n",
            "Epoch 84/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 0.9918 - val_loss: 1.2437 - val_accuracy: 0.6792\n",
            "Epoch 85/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 1.3827 - val_accuracy: 0.6604\n",
            "Epoch 86/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0156 - accuracy: 0.9918 - val_loss: 1.4227 - val_accuracy: 0.6792\n",
            "Epoch 87/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0233 - accuracy: 0.9836 - val_loss: 1.3160 - val_accuracy: 0.6792\n",
            "Epoch 88/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0372 - accuracy: 0.9836 - val_loss: 1.2611 - val_accuracy: 0.6415\n",
            "Epoch 89/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0414 - accuracy: 0.9836 - val_loss: 1.2487 - val_accuracy: 0.6981\n",
            "Epoch 90/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0480 - accuracy: 0.9836 - val_loss: 1.2601 - val_accuracy: 0.7170\n",
            "Epoch 91/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0214 - accuracy: 0.9918 - val_loss: 1.2749 - val_accuracy: 0.6604\n",
            "Epoch 92/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0187 - accuracy: 0.9836 - val_loss: 1.2981 - val_accuracy: 0.6604\n",
            "Epoch 93/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0297 - accuracy: 0.9836 - val_loss: 1.3541 - val_accuracy: 0.6981\n",
            "Epoch 94/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0212 - accuracy: 0.9836 - val_loss: 1.3580 - val_accuracy: 0.6981\n",
            "Epoch 95/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0253 - accuracy: 0.9836 - val_loss: 1.3835 - val_accuracy: 0.7170\n",
            "Epoch 96/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0214 - accuracy: 0.9836 - val_loss: 1.4728 - val_accuracy: 0.6981\n",
            "Epoch 97/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0249 - accuracy: 0.9836 - val_loss: 1.3405 - val_accuracy: 0.6415\n",
            "Epoch 98/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0180 - accuracy: 0.9918 - val_loss: 1.5320 - val_accuracy: 0.6981\n",
            "Epoch 99/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0362 - accuracy: 0.9836 - val_loss: 1.4121 - val_accuracy: 0.7170\n",
            "Epoch 100/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0320 - accuracy: 0.9836 - val_loss: 1.3573 - val_accuracy: 0.7170\n",
            "Epoch 101/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0341 - accuracy: 0.9836 - val_loss: 1.4647 - val_accuracy: 0.7170\n",
            "Epoch 102/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0196 - accuracy: 0.9836 - val_loss: 1.3997 - val_accuracy: 0.7170\n",
            "Epoch 103/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 1.3953 - val_accuracy: 0.7170\n",
            "Epoch 104/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0105 - accuracy: 0.9918 - val_loss: 1.3624 - val_accuracy: 0.6981\n",
            "Epoch 105/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.3851 - val_accuracy: 0.6415\n",
            "Epoch 106/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0159 - accuracy: 0.9836 - val_loss: 1.3562 - val_accuracy: 0.6981\n",
            "Epoch 107/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0161 - accuracy: 0.9918 - val_loss: 1.4977 - val_accuracy: 0.7170\n",
            "Epoch 108/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0227 - accuracy: 0.9918 - val_loss: 1.7753 - val_accuracy: 0.6792\n",
            "Epoch 109/500\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0283 - accuracy: 0.9918 - val_loss: 1.6405 - val_accuracy: 0.7170\n",
            "Epoch 110/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0215 - accuracy: 0.9918 - val_loss: 1.5806 - val_accuracy: 0.6792\n",
            "Epoch 111/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0151 - accuracy: 0.9918 - val_loss: 1.5506 - val_accuracy: 0.6792\n",
            "Epoch 112/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0203 - accuracy: 0.9918 - val_loss: 1.5350 - val_accuracy: 0.6792\n",
            "Epoch 113/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0291 - accuracy: 0.9836 - val_loss: 1.6795 - val_accuracy: 0.6415\n",
            "Epoch 114/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0108 - accuracy: 0.9918 - val_loss: 1.6836 - val_accuracy: 0.6226\n",
            "Epoch 115/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0251 - accuracy: 0.9836 - val_loss: 1.7260 - val_accuracy: 0.6415\n",
            "Epoch 116/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 1.6727 - val_accuracy: 0.6415\n",
            "Epoch 117/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0272 - accuracy: 0.9918 - val_loss: 1.7692 - val_accuracy: 0.6415\n",
            "Epoch 118/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.6006 - val_accuracy: 0.6415\n",
            "Epoch 119/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 1.6176 - val_accuracy: 0.6981\n",
            "Epoch 120/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0097 - accuracy: 0.9918 - val_loss: 1.5430 - val_accuracy: 0.6792\n",
            "Epoch 121/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0210 - accuracy: 0.9918 - val_loss: 1.5749 - val_accuracy: 0.7170\n",
            "Epoch 122/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.5938 - val_accuracy: 0.6981\n",
            "Epoch 123/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0312 - accuracy: 0.9836 - val_loss: 1.6305 - val_accuracy: 0.6981\n",
            "Epoch 124/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0208 - accuracy: 0.9836 - val_loss: 1.6800 - val_accuracy: 0.6981\n",
            "Epoch 125/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0242 - accuracy: 0.9836 - val_loss: 1.6367 - val_accuracy: 0.6415\n",
            "Epoch 126/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0183 - accuracy: 0.9918 - val_loss: 1.8411 - val_accuracy: 0.6415\n",
            "Epoch 127/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0292 - accuracy: 0.9836 - val_loss: 1.6241 - val_accuracy: 0.6415\n",
            "Epoch 128/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.6381 - val_accuracy: 0.6604\n",
            "Epoch 129/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0220 - accuracy: 0.9918 - val_loss: 1.6517 - val_accuracy: 0.6981\n",
            "Epoch 130/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0172 - accuracy: 0.9918 - val_loss: 1.6478 - val_accuracy: 0.6604\n",
            "Epoch 131/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.5692 - val_accuracy: 0.6981\n",
            "Epoch 132/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.6903 - val_accuracy: 0.7170\n",
            "Epoch 133/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0232 - accuracy: 0.9918 - val_loss: 1.6251 - val_accuracy: 0.7170\n",
            "Epoch 134/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0098 - accuracy: 0.9918 - val_loss: 1.6843 - val_accuracy: 0.6981\n",
            "Epoch 135/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0305 - accuracy: 0.9836 - val_loss: 1.6492 - val_accuracy: 0.6415\n",
            "Epoch 136/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0128 - accuracy: 0.9918 - val_loss: 1.6517 - val_accuracy: 0.6415\n",
            "Epoch 137/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 1.6036 - val_accuracy: 0.6415\n",
            "Epoch 138/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0350 - accuracy: 0.9836 - val_loss: 1.6767 - val_accuracy: 0.6415\n",
            "Epoch 139/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 1.7601 - val_accuracy: 0.6415\n",
            "Epoch 140/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.6930 - val_accuracy: 0.6415\n",
            "Epoch 141/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0243 - accuracy: 0.9836 - val_loss: 1.6192 - val_accuracy: 0.6226\n",
            "Epoch 142/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0242 - accuracy: 0.9918 - val_loss: 1.8197 - val_accuracy: 0.6415\n",
            "Epoch 143/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0221 - accuracy: 0.9918 - val_loss: 1.6577 - val_accuracy: 0.6604\n",
            "Epoch 144/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0146 - accuracy: 0.9918 - val_loss: 1.7366 - val_accuracy: 0.6604\n",
            "Epoch 145/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0236 - accuracy: 0.9836 - val_loss: 1.6291 - val_accuracy: 0.6792\n",
            "Epoch 146/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0254 - accuracy: 0.9836 - val_loss: 1.6064 - val_accuracy: 0.6792\n",
            "Epoch 147/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0159 - accuracy: 0.9918 - val_loss: 1.4847 - val_accuracy: 0.6981\n",
            "Epoch 148/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 1.5442 - val_accuracy: 0.6415\n",
            "Epoch 149/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 1.5876 - val_accuracy: 0.6604\n",
            "Epoch 150/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0204 - accuracy: 0.9918 - val_loss: 1.6669 - val_accuracy: 0.6604\n",
            "Epoch 151/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 1.5628 - val_accuracy: 0.6981\n",
            "Epoch 152/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0204 - accuracy: 0.9836 - val_loss: 1.6214 - val_accuracy: 0.6415\n",
            "Epoch 153/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 0.9918 - val_loss: 1.6836 - val_accuracy: 0.6415\n",
            "Epoch 154/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0159 - accuracy: 0.9918 - val_loss: 1.6867 - val_accuracy: 0.6226\n",
            "Epoch 155/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0158 - accuracy: 0.9836 - val_loss: 1.7088 - val_accuracy: 0.6415\n",
            "Epoch 156/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0201 - accuracy: 0.9836 - val_loss: 1.7026 - val_accuracy: 0.6604\n",
            "Epoch 157/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0168 - accuracy: 0.9836 - val_loss: 1.6510 - val_accuracy: 0.6604\n",
            "Epoch 158/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0245 - accuracy: 0.9836 - val_loss: 1.7020 - val_accuracy: 0.6604\n",
            "Epoch 159/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0203 - accuracy: 0.9836 - val_loss: 1.7165 - val_accuracy: 0.6604\n",
            "Epoch 160/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 1.6401 - val_accuracy: 0.7170\n",
            "Epoch 161/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0277 - accuracy: 0.9754 - val_loss: 1.7101 - val_accuracy: 0.7170\n",
            "Epoch 162/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 1.7919 - val_accuracy: 0.6792\n",
            "Epoch 163/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0266 - accuracy: 0.9836 - val_loss: 1.7476 - val_accuracy: 0.6981\n",
            "Epoch 164/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0141 - accuracy: 0.9918 - val_loss: 1.7170 - val_accuracy: 0.6981\n",
            "Epoch 165/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 1.6614 - val_accuracy: 0.6792\n",
            "Epoch 166/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0139 - accuracy: 0.9918 - val_loss: 1.6844 - val_accuracy: 0.6981\n",
            "Epoch 167/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0156 - accuracy: 0.9836 - val_loss: 1.6586 - val_accuracy: 0.6981\n",
            "Epoch 168/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 1.7008 - val_accuracy: 0.5849\n",
            "Epoch 169/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 1.7910 - val_accuracy: 0.6792\n",
            "Epoch 170/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0135 - accuracy: 0.9836 - val_loss: 1.7648 - val_accuracy: 0.6981\n",
            "Epoch 171/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0174 - accuracy: 0.9918 - val_loss: 1.7436 - val_accuracy: 0.6981\n",
            "Epoch 172/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0114 - accuracy: 0.9918 - val_loss: 1.7964 - val_accuracy: 0.6792\n",
            "Epoch 173/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0251 - accuracy: 0.9836 - val_loss: 1.7932 - val_accuracy: 0.6981\n",
            "Epoch 174/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0140 - accuracy: 0.9918 - val_loss: 1.8173 - val_accuracy: 0.6792\n",
            "Epoch 175/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 1.8058 - val_accuracy: 0.6792\n",
            "Epoch 176/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0130 - accuracy: 0.9918 - val_loss: 1.8464 - val_accuracy: 0.6792\n",
            "Epoch 177/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.9140 - val_accuracy: 0.6792\n",
            "Epoch 178/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 1.9278 - val_accuracy: 0.6792\n",
            "Epoch 179/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0180 - accuracy: 0.9836 - val_loss: 1.9267 - val_accuracy: 0.6226\n",
            "Epoch 180/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0184 - accuracy: 0.9918 - val_loss: 1.9413 - val_accuracy: 0.6415\n",
            "Epoch 181/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0113 - accuracy: 0.9918 - val_loss: 1.9634 - val_accuracy: 0.6415\n",
            "Epoch 182/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0199 - accuracy: 0.9918 - val_loss: 1.9057 - val_accuracy: 0.6226\n",
            "Epoch 183/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0264 - accuracy: 0.9836 - val_loss: 2.0293 - val_accuracy: 0.6604\n",
            "Epoch 184/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0143 - accuracy: 0.9918 - val_loss: 2.0424 - val_accuracy: 0.6981\n",
            "Epoch 185/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0140 - accuracy: 0.9918 - val_loss: 2.0479 - val_accuracy: 0.6604\n",
            "Epoch 186/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0151 - accuracy: 0.9918 - val_loss: 1.9069 - val_accuracy: 0.6792\n",
            "Epoch 187/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0176 - accuracy: 0.9836 - val_loss: 1.8555 - val_accuracy: 0.6981\n",
            "Epoch 188/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0138 - accuracy: 0.9918 - val_loss: 1.9425 - val_accuracy: 0.6981\n",
            "Epoch 189/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0148 - accuracy: 0.9918 - val_loss: 1.9017 - val_accuracy: 0.6981\n",
            "Epoch 190/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9918 - val_loss: 1.9444 - val_accuracy: 0.6415\n",
            "Epoch 191/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0114 - accuracy: 0.9918 - val_loss: 1.9580 - val_accuracy: 0.6226\n",
            "Epoch 192/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0153 - accuracy: 0.9918 - val_loss: 2.0038 - val_accuracy: 0.6226\n",
            "Epoch 193/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0144 - accuracy: 0.9836 - val_loss: 2.0338 - val_accuracy: 0.6226\n",
            "Epoch 194/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 0.9836 - val_loss: 2.0200 - val_accuracy: 0.6226\n",
            "Epoch 195/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.0544 - val_accuracy: 0.6415\n",
            "Epoch 196/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0149 - accuracy: 0.9836 - val_loss: 2.0239 - val_accuracy: 0.6415\n",
            "Epoch 197/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0102 - accuracy: 0.9918 - val_loss: 2.0644 - val_accuracy: 0.6415\n",
            "Epoch 198/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0154 - accuracy: 0.9836 - val_loss: 2.0015 - val_accuracy: 0.6226\n",
            "Epoch 199/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0142 - accuracy: 0.9918 - val_loss: 1.9779 - val_accuracy: 0.6415\n",
            "Epoch 200/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0106 - accuracy: 0.9918 - val_loss: 1.9735 - val_accuracy: 0.6792\n",
            "Epoch 201/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 1.9466 - val_accuracy: 0.6792\n",
            "Epoch 202/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 1.8714 - val_accuracy: 0.6792\n",
            "Epoch 203/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0132 - accuracy: 0.9918 - val_loss: 2.1086 - val_accuracy: 0.6226\n",
            "Epoch 204/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0179 - accuracy: 0.9918 - val_loss: 2.4182 - val_accuracy: 0.6604\n",
            "Epoch 205/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0107 - accuracy: 0.9918 - val_loss: 2.1527 - val_accuracy: 0.6604\n",
            "Epoch 206/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0203 - accuracy: 0.9836 - val_loss: 2.2335 - val_accuracy: 0.6226\n",
            "Epoch 207/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.0795 - val_accuracy: 0.6226\n",
            "Epoch 208/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0113 - accuracy: 0.9918 - val_loss: 2.2933 - val_accuracy: 0.6415\n",
            "Epoch 209/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0276 - accuracy: 0.9836 - val_loss: 2.3827 - val_accuracy: 0.6415\n",
            "Epoch 210/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 2.3289 - val_accuracy: 0.6415\n",
            "Epoch 211/500\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 2.3314 - val_accuracy: 0.6415\n",
            "Epoch 212/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0166 - accuracy: 0.9918 - val_loss: 2.3010 - val_accuracy: 0.6415\n",
            "Epoch 213/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0170 - accuracy: 0.9918 - val_loss: 2.1777 - val_accuracy: 0.6415\n",
            "Epoch 214/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 0.9918 - val_loss: 2.0322 - val_accuracy: 0.6981\n",
            "Epoch 215/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 2.0383 - val_accuracy: 0.6981\n",
            "Epoch 216/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.0910 - val_accuracy: 0.6981\n",
            "Epoch 217/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0133 - accuracy: 0.9918 - val_loss: 2.3872 - val_accuracy: 0.6604\n",
            "Epoch 218/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0186 - accuracy: 0.9836 - val_loss: 2.2368 - val_accuracy: 0.6604\n",
            "Epoch 219/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 2.1068 - val_accuracy: 0.6604\n",
            "Epoch 220/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0179 - accuracy: 0.9836 - val_loss: 2.0767 - val_accuracy: 0.6415\n",
            "Epoch 221/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 2.1428 - val_accuracy: 0.6604\n",
            "Epoch 222/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0127 - accuracy: 0.9836 - val_loss: 2.1844 - val_accuracy: 0.6604\n",
            "Epoch 223/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0157 - accuracy: 0.9918 - val_loss: 2.1154 - val_accuracy: 0.6604\n",
            "Epoch 224/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 2.2583 - val_accuracy: 0.6226\n",
            "Epoch 225/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0201 - accuracy: 0.9836 - val_loss: 2.0788 - val_accuracy: 0.6226\n",
            "Epoch 226/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0184 - accuracy: 0.9836 - val_loss: 2.3638 - val_accuracy: 0.6226\n",
            "Epoch 227/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0160 - accuracy: 0.9836 - val_loss: 2.2775 - val_accuracy: 0.6226\n",
            "Epoch 228/500\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0166 - accuracy: 0.9836 - val_loss: 2.3502 - val_accuracy: 0.6226\n",
            "Epoch 229/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 2.2813 - val_accuracy: 0.6981\n",
            "Epoch 230/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0155 - accuracy: 0.9918 - val_loss: 2.3399 - val_accuracy: 0.6226\n",
            "Epoch 231/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 2.3020 - val_accuracy: 0.6226\n",
            "Epoch 232/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0144 - accuracy: 0.9918 - val_loss: 2.4963 - val_accuracy: 0.6226\n",
            "Epoch 233/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0118 - accuracy: 0.9918 - val_loss: 2.3268 - val_accuracy: 0.6226\n",
            "Epoch 234/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0099 - accuracy: 0.9918 - val_loss: 2.3739 - val_accuracy: 0.5849\n",
            "Epoch 235/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0184 - accuracy: 0.9836 - val_loss: 2.3245 - val_accuracy: 0.6226\n",
            "Epoch 236/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0142 - accuracy: 0.9918 - val_loss: 2.2274 - val_accuracy: 0.6226\n",
            "Epoch 237/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 2.2663 - val_accuracy: 0.6226\n",
            "Epoch 238/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 0.9836 - val_loss: 2.2255 - val_accuracy: 0.6226\n",
            "Epoch 239/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0137 - accuracy: 0.9918 - val_loss: 2.2125 - val_accuracy: 0.6792\n",
            "Epoch 240/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0110 - accuracy: 0.9918 - val_loss: 2.3117 - val_accuracy: 0.6792\n",
            "Epoch 241/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0137 - accuracy: 0.9918 - val_loss: 2.4557 - val_accuracy: 0.6226\n",
            "Epoch 242/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 2.7180 - val_accuracy: 0.6226\n",
            "Epoch 243/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0114 - accuracy: 0.9918 - val_loss: 2.3798 - val_accuracy: 0.6415\n",
            "Epoch 244/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 2.4046 - val_accuracy: 0.6415\n",
            "Epoch 245/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.3637 - val_accuracy: 0.6226\n",
            "Epoch 246/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0123 - accuracy: 0.9836 - val_loss: 2.4431 - val_accuracy: 0.6415\n",
            "Epoch 247/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0263 - accuracy: 0.9918 - val_loss: 2.2824 - val_accuracy: 0.6226\n",
            "Epoch 248/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 2.2913 - val_accuracy: 0.6226\n",
            "Epoch 249/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 2.3309 - val_accuracy: 0.6226\n",
            "Epoch 250/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.3942 - val_accuracy: 0.6226\n",
            "Epoch 251/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0177 - accuracy: 0.9918 - val_loss: 2.3590 - val_accuracy: 0.6226\n",
            "Epoch 252/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0198 - accuracy: 0.9836 - val_loss: 2.2758 - val_accuracy: 0.6226\n",
            "Epoch 253/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0106 - accuracy: 0.9918 - val_loss: 2.3074 - val_accuracy: 0.6226\n",
            "Epoch 254/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0162 - accuracy: 0.9836 - val_loss: 2.5016 - val_accuracy: 0.6226\n",
            "Epoch 255/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 2.4497 - val_accuracy: 0.6226\n",
            "Epoch 256/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0155 - accuracy: 0.9918 - val_loss: 2.4781 - val_accuracy: 0.6226\n",
            "Epoch 257/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0093 - accuracy: 0.9918 - val_loss: 2.5172 - val_accuracy: 0.6226\n",
            "Epoch 258/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0123 - accuracy: 0.9918 - val_loss: 2.5777 - val_accuracy: 0.6226\n",
            "Epoch 259/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0252 - accuracy: 0.9836 - val_loss: 2.7659 - val_accuracy: 0.6415\n",
            "Epoch 260/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 2.5171 - val_accuracy: 0.6038\n",
            "Epoch 261/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0139 - accuracy: 0.9918 - val_loss: 2.4112 - val_accuracy: 0.6604\n",
            "Epoch 262/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0198 - accuracy: 0.9918 - val_loss: 2.9891 - val_accuracy: 0.6038\n",
            "Epoch 263/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0129 - accuracy: 0.9836 - val_loss: 2.8443 - val_accuracy: 0.6038\n",
            "Epoch 264/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0090 - accuracy: 0.9918 - val_loss: 2.6444 - val_accuracy: 0.6038\n",
            "Epoch 265/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 2.5803 - val_accuracy: 0.5849\n",
            "Epoch 266/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.9918 - val_loss: 2.6305 - val_accuracy: 0.5849\n",
            "Epoch 267/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0140 - accuracy: 0.9836 - val_loss: 2.5363 - val_accuracy: 0.5849\n",
            "Epoch 268/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0195 - accuracy: 0.9918 - val_loss: 2.7047 - val_accuracy: 0.5849\n",
            "Epoch 269/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 2.5592 - val_accuracy: 0.5849\n",
            "Epoch 270/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0100 - accuracy: 0.9918 - val_loss: 2.7258 - val_accuracy: 0.6038\n",
            "Epoch 271/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0162 - accuracy: 0.9836 - val_loss: 2.9302 - val_accuracy: 0.6038\n",
            "Epoch 272/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.6621 - val_accuracy: 0.5849\n",
            "Epoch 273/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0133 - accuracy: 0.9918 - val_loss: 3.2749 - val_accuracy: 0.6038\n",
            "Epoch 274/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 3.3537 - val_accuracy: 0.5849\n",
            "Epoch 275/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.3594 - val_accuracy: 0.6038\n",
            "Epoch 276/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0225 - accuracy: 0.9836 - val_loss: 3.1987 - val_accuracy: 0.6038\n",
            "Epoch 277/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0150 - accuracy: 0.9918 - val_loss: 2.8764 - val_accuracy: 0.5849\n",
            "Epoch 278/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.7006 - val_accuracy: 0.5849\n",
            "Epoch 279/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0207 - accuracy: 0.9836 - val_loss: 2.6537 - val_accuracy: 0.5849\n",
            "Epoch 280/500\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0100 - accuracy: 0.9918 - val_loss: 2.5150 - val_accuracy: 0.5849\n",
            "Epoch 281/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0109 - accuracy: 0.9918 - val_loss: 2.3943 - val_accuracy: 0.6981\n",
            "Epoch 282/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0167 - accuracy: 0.9836 - val_loss: 2.4221 - val_accuracy: 0.6792\n",
            "Epoch 283/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0168 - accuracy: 0.9836 - val_loss: 2.5328 - val_accuracy: 0.6415\n",
            "Epoch 284/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0136 - accuracy: 0.9918 - val_loss: 2.9323 - val_accuracy: 0.6415\n",
            "Epoch 285/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0181 - accuracy: 0.9836 - val_loss: 2.7386 - val_accuracy: 0.6226\n",
            "Epoch 286/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0097 - accuracy: 0.9918 - val_loss: 2.9990 - val_accuracy: 0.5849\n",
            "Epoch 287/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0175 - accuracy: 0.9836 - val_loss: 2.9195 - val_accuracy: 0.6226\n",
            "Epoch 288/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0167 - accuracy: 0.9836 - val_loss: 2.7431 - val_accuracy: 0.6226\n",
            "Epoch 289/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0157 - accuracy: 0.9918 - val_loss: 2.5073 - val_accuracy: 0.6226\n",
            "Epoch 290/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.4875 - val_accuracy: 0.6415\n",
            "Epoch 291/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0153 - accuracy: 0.9836 - val_loss: 2.4941 - val_accuracy: 0.6226\n",
            "Epoch 292/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0167 - accuracy: 0.9836 - val_loss: 2.5756 - val_accuracy: 0.6038\n",
            "Epoch 293/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.5108 - val_accuracy: 0.6415\n",
            "Epoch 294/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 2.5412 - val_accuracy: 0.7170\n",
            "Epoch 295/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0191 - accuracy: 0.9918 - val_loss: 2.5600 - val_accuracy: 0.6415\n",
            "Epoch 296/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0146 - accuracy: 0.9836 - val_loss: 2.7413 - val_accuracy: 0.6604\n",
            "Epoch 297/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0111 - accuracy: 0.9918 - val_loss: 2.7039 - val_accuracy: 0.6415\n",
            "Epoch 298/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0127 - accuracy: 0.9836 - val_loss: 2.8834 - val_accuracy: 0.6604\n",
            "Epoch 299/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 2.9896 - val_accuracy: 0.6604\n",
            "Epoch 300/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.9539 - val_accuracy: 0.6604\n",
            "Epoch 301/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0123 - accuracy: 0.9918 - val_loss: 3.1162 - val_accuracy: 0.6604\n",
            "Epoch 302/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 3.0457 - val_accuracy: 0.6604\n",
            "Epoch 303/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 2.9752 - val_accuracy: 0.6792\n",
            "Epoch 304/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0108 - accuracy: 0.9918 - val_loss: 3.1842 - val_accuracy: 0.6415\n",
            "Epoch 305/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0135 - accuracy: 0.9836 - val_loss: 3.1173 - val_accuracy: 0.6415\n",
            "Epoch 306/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 2.8839 - val_accuracy: 0.6415\n",
            "Epoch 307/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 2.9296 - val_accuracy: 0.6415\n",
            "Epoch 308/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0114 - accuracy: 0.9918 - val_loss: 2.9687 - val_accuracy: 0.6415\n",
            "Epoch 309/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0150 - accuracy: 0.9918 - val_loss: 2.8756 - val_accuracy: 0.6415\n",
            "Epoch 310/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 2.9224 - val_accuracy: 0.6415\n",
            "Epoch 311/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0173 - accuracy: 0.9836 - val_loss: 3.0246 - val_accuracy: 0.6604\n",
            "Epoch 312/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0168 - accuracy: 0.9836 - val_loss: 3.0275 - val_accuracy: 0.6604\n",
            "Epoch 313/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0130 - accuracy: 0.9918 - val_loss: 3.1825 - val_accuracy: 0.6415\n",
            "Epoch 314/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0101 - accuracy: 0.9918 - val_loss: 3.2054 - val_accuracy: 0.6415\n",
            "Epoch 315/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 3.4442 - val_accuracy: 0.6415\n",
            "Epoch 316/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.2422 - val_accuracy: 0.6415\n",
            "Epoch 317/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 3.2668 - val_accuracy: 0.6415\n",
            "Epoch 318/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0145 - accuracy: 0.9836 - val_loss: 2.8936 - val_accuracy: 0.6415\n",
            "Epoch 319/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0133 - accuracy: 0.9836 - val_loss: 3.0847 - val_accuracy: 0.6415\n",
            "Epoch 320/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.1575 - val_accuracy: 0.6226\n",
            "Epoch 321/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0133 - accuracy: 0.9918 - val_loss: 2.9203 - val_accuracy: 0.6226\n",
            "Epoch 322/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 2.8407 - val_accuracy: 0.6226\n",
            "Epoch 323/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0152 - accuracy: 0.9836 - val_loss: 3.0135 - val_accuracy: 0.6226\n",
            "Epoch 324/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0109 - accuracy: 0.9918 - val_loss: 3.0904 - val_accuracy: 0.6415\n",
            "Epoch 325/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0140 - accuracy: 0.9836 - val_loss: 2.9424 - val_accuracy: 0.6604\n",
            "Epoch 326/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0111 - accuracy: 0.9918 - val_loss: 2.9725 - val_accuracy: 0.6415\n",
            "Epoch 327/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 2.9569 - val_accuracy: 0.6415\n",
            "Epoch 328/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 3.0080 - val_accuracy: 0.6415\n",
            "Epoch 329/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0181 - accuracy: 0.9836 - val_loss: 3.1255 - val_accuracy: 0.6226\n",
            "Epoch 330/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0118 - accuracy: 0.9836 - val_loss: 3.0121 - val_accuracy: 0.6226\n",
            "Epoch 331/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0158 - accuracy: 0.9918 - val_loss: 3.0901 - val_accuracy: 0.6226\n",
            "Epoch 332/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 3.1159 - val_accuracy: 0.5849\n",
            "Epoch 333/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 3.2233 - val_accuracy: 0.5849\n",
            "Epoch 334/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0127 - accuracy: 0.9836 - val_loss: 3.1729 - val_accuracy: 0.5849\n",
            "Epoch 335/500\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 3.1879 - val_accuracy: 0.5849\n",
            "Epoch 336/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 3.5806 - val_accuracy: 0.6415\n",
            "Epoch 337/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0133 - accuracy: 0.9836 - val_loss: 3.4247 - val_accuracy: 0.6415\n",
            "Epoch 338/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0145 - accuracy: 0.9836 - val_loss: 3.3211 - val_accuracy: 0.6604\n",
            "Epoch 339/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0168 - accuracy: 0.9836 - val_loss: 2.9295 - val_accuracy: 0.6415\n",
            "Epoch 340/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0182 - accuracy: 0.9918 - val_loss: 2.9072 - val_accuracy: 0.6415\n",
            "Epoch 341/500\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 2.9319 - val_accuracy: 0.6415\n",
            "Epoch 342/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 2.9675 - val_accuracy: 0.6415\n",
            "Epoch 343/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0131 - accuracy: 0.9918 - val_loss: 3.0598 - val_accuracy: 0.6415\n",
            "Epoch 344/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0112 - accuracy: 0.9918 - val_loss: 3.1935 - val_accuracy: 0.6415\n",
            "Epoch 345/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0123 - accuracy: 0.9918 - val_loss: 3.2828 - val_accuracy: 0.6415\n",
            "Epoch 346/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0130 - accuracy: 0.9918 - val_loss: 3.3659 - val_accuracy: 0.6415\n",
            "Epoch 347/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 3.3335 - val_accuracy: 0.6415\n",
            "Epoch 348/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 0.9836 - val_loss: 3.2917 - val_accuracy: 0.6604\n",
            "Epoch 349/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0219 - accuracy: 0.9836 - val_loss: 3.6102 - val_accuracy: 0.6604\n",
            "Epoch 350/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0129 - accuracy: 0.9836 - val_loss: 3.6667 - val_accuracy: 0.6604\n",
            "Epoch 351/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 3.5702 - val_accuracy: 0.6604\n",
            "Epoch 352/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0112 - accuracy: 0.9918 - val_loss: 3.5217 - val_accuracy: 0.6604\n",
            "Epoch 353/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0139 - accuracy: 0.9836 - val_loss: 3.5439 - val_accuracy: 0.6604\n",
            "Epoch 354/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0142 - accuracy: 0.9836 - val_loss: 3.6510 - val_accuracy: 0.6792\n",
            "Epoch 355/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 3.5736 - val_accuracy: 0.6604\n",
            "Epoch 356/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0399 - accuracy: 0.9918 - val_loss: 2.9969 - val_accuracy: 0.6226\n",
            "Epoch 357/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0135 - accuracy: 0.9836 - val_loss: 3.1454 - val_accuracy: 0.5849\n",
            "Epoch 358/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0103 - accuracy: 0.9918 - val_loss: 3.0988 - val_accuracy: 0.5849\n",
            "Epoch 359/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.9918 - val_loss: 3.0940 - val_accuracy: 0.6038\n",
            "Epoch 360/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0144 - accuracy: 0.9836 - val_loss: 3.0446 - val_accuracy: 0.6226\n",
            "Epoch 361/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0242 - accuracy: 0.9836 - val_loss: 2.7344 - val_accuracy: 0.6226\n",
            "Epoch 362/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0120 - accuracy: 0.9918 - val_loss: 2.7542 - val_accuracy: 0.6226\n",
            "Epoch 363/500\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.0114 - accuracy: 0.9918 - val_loss: 2.7756 - val_accuracy: 0.6415\n",
            "Epoch 364/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 2.7977 - val_accuracy: 0.6415\n",
            "Epoch 365/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.7740 - val_accuracy: 0.6415\n",
            "Epoch 366/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 2.7788 - val_accuracy: 0.6415\n",
            "Epoch 367/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 2.8238 - val_accuracy: 0.6604\n",
            "Epoch 368/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0163 - accuracy: 0.9918 - val_loss: 2.8768 - val_accuracy: 0.6415\n",
            "Epoch 369/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 0.9836 - val_loss: 2.9851 - val_accuracy: 0.6226\n",
            "Epoch 370/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0127 - accuracy: 0.9918 - val_loss: 2.9936 - val_accuracy: 0.6226\n",
            "Epoch 371/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.2507 - val_accuracy: 0.6415\n",
            "Epoch 372/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 3.2359 - val_accuracy: 0.6415\n",
            "Epoch 373/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0117 - accuracy: 0.9836 - val_loss: 3.2708 - val_accuracy: 0.6604\n",
            "Epoch 374/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 3.3034 - val_accuracy: 0.6604\n",
            "Epoch 375/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 3.4149 - val_accuracy: 0.6415\n",
            "Epoch 376/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0120 - accuracy: 0.9918 - val_loss: 3.3336 - val_accuracy: 0.6415\n",
            "Epoch 377/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.9836 - val_loss: 3.3813 - val_accuracy: 0.6415\n",
            "Epoch 378/500\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.0147 - accuracy: 0.9836 - val_loss: 3.5608 - val_accuracy: 0.6415\n",
            "Epoch 379/500\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0145 - accuracy: 0.9918 - val_loss: 3.7068 - val_accuracy: 0.6604\n",
            "Epoch 380/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0111 - accuracy: 0.9918 - val_loss: 3.6181 - val_accuracy: 0.6604\n",
            "Epoch 381/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0151 - accuracy: 0.9836 - val_loss: 3.6331 - val_accuracy: 0.6604\n",
            "Epoch 382/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0161 - accuracy: 0.9836 - val_loss: 3.3978 - val_accuracy: 0.6604\n",
            "Epoch 383/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 3.4463 - val_accuracy: 0.6604\n",
            "Epoch 384/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0132 - accuracy: 0.9836 - val_loss: 3.5424 - val_accuracy: 0.6604\n",
            "Epoch 385/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 3.5331 - val_accuracy: 0.6604\n",
            "Epoch 386/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 3.3852 - val_accuracy: 0.6415\n",
            "Epoch 387/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0132 - accuracy: 0.9836 - val_loss: 3.4973 - val_accuracy: 0.6415\n",
            "Epoch 388/500\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0130 - accuracy: 0.9918 - val_loss: 3.6218 - val_accuracy: 0.6604\n",
            "Epoch 389/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0568 - accuracy: 0.9754 - val_loss: 3.6808 - val_accuracy: 0.6604\n",
            "Epoch 390/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 3.5932 - val_accuracy: 0.6604\n",
            "Epoch 391/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0145 - accuracy: 0.9836 - val_loss: 3.4410 - val_accuracy: 0.6604\n",
            "Epoch 392/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 3.5587 - val_accuracy: 0.6604\n",
            "Epoch 393/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0148 - accuracy: 0.9918 - val_loss: 3.1883 - val_accuracy: 0.5849\n",
            "Epoch 394/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.2074 - val_accuracy: 0.5849\n",
            "Epoch 395/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0158 - accuracy: 0.9836 - val_loss: 3.1955 - val_accuracy: 0.5849\n",
            "Epoch 396/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 3.2006 - val_accuracy: 0.6226\n",
            "Epoch 397/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0134 - accuracy: 0.9836 - val_loss: 3.2358 - val_accuracy: 0.6415\n",
            "Epoch 398/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 3.1583 - val_accuracy: 0.6226\n",
            "Epoch 399/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0128 - accuracy: 0.9918 - val_loss: 3.1427 - val_accuracy: 0.6226\n",
            "Epoch 400/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 3.2516 - val_accuracy: 0.6415\n",
            "Epoch 401/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 0.9918 - val_loss: 3.2867 - val_accuracy: 0.6415\n",
            "Epoch 402/500\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0137 - accuracy: 0.9836 - val_loss: 3.3403 - val_accuracy: 0.6415\n",
            "Epoch 403/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0112 - accuracy: 0.9918 - val_loss: 3.4868 - val_accuracy: 0.6415\n",
            "Epoch 404/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0106 - accuracy: 0.9918 - val_loss: 3.6079 - val_accuracy: 0.6415\n",
            "Epoch 405/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 3.7746 - val_accuracy: 0.6415\n",
            "Epoch 406/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0199 - accuracy: 0.9836 - val_loss: 3.6101 - val_accuracy: 0.6415\n",
            "Epoch 407/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0166 - accuracy: 0.9918 - val_loss: 3.3970 - val_accuracy: 0.6604\n",
            "Epoch 408/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0123 - accuracy: 0.9918 - val_loss: 3.3801 - val_accuracy: 0.6604\n",
            "Epoch 409/500\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.0138 - accuracy: 0.9918 - val_loss: 3.3988 - val_accuracy: 0.6604\n",
            "Epoch 410/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0109 - accuracy: 0.9918 - val_loss: 3.3186 - val_accuracy: 0.6415\n",
            "Epoch 411/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 3.3655 - val_accuracy: 0.6415\n",
            "Epoch 412/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0137 - accuracy: 0.9836 - val_loss: 3.5242 - val_accuracy: 0.6415\n",
            "Epoch 413/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0110 - accuracy: 0.9918 - val_loss: 3.6007 - val_accuracy: 0.6415\n",
            "Epoch 414/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.3576 - val_accuracy: 0.6415\n",
            "Epoch 415/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0104 - accuracy: 0.9918 - val_loss: 3.3769 - val_accuracy: 0.6415\n",
            "Epoch 416/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0112 - accuracy: 0.9918 - val_loss: 3.3547 - val_accuracy: 0.6415\n",
            "Epoch 417/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0154 - accuracy: 0.9836 - val_loss: 3.3090 - val_accuracy: 0.6415\n",
            "Epoch 418/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0118 - accuracy: 0.9918 - val_loss: 3.1928 - val_accuracy: 0.6415\n",
            "Epoch 419/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0110 - accuracy: 0.9918 - val_loss: 3.3070 - val_accuracy: 0.6415\n",
            "Epoch 420/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0131 - accuracy: 0.9836 - val_loss: 3.7200 - val_accuracy: 0.6415\n",
            "Epoch 421/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 3.6681 - val_accuracy: 0.6604\n",
            "Epoch 422/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.4360 - val_accuracy: 0.6604\n",
            "Epoch 423/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 3.1616 - val_accuracy: 0.6604\n",
            "Epoch 424/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0132 - accuracy: 0.9918 - val_loss: 3.1598 - val_accuracy: 0.6415\n",
            "Epoch 425/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0180 - accuracy: 0.9918 - val_loss: 3.3737 - val_accuracy: 0.6415\n",
            "Epoch 426/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0201 - accuracy: 0.9836 - val_loss: 3.2686 - val_accuracy: 0.6415\n",
            "Epoch 427/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0136 - accuracy: 0.9918 - val_loss: 3.3421 - val_accuracy: 0.6415\n",
            "Epoch 428/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0131 - accuracy: 0.9918 - val_loss: 3.2632 - val_accuracy: 0.6415\n",
            "Epoch 429/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0146 - accuracy: 0.9918 - val_loss: 3.2057 - val_accuracy: 0.6415\n",
            "Epoch 430/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0109 - accuracy: 0.9918 - val_loss: 3.2785 - val_accuracy: 0.6415\n",
            "Epoch 431/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0110 - accuracy: 0.9918 - val_loss: 3.3804 - val_accuracy: 0.6415\n",
            "Epoch 432/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0123 - accuracy: 0.9836 - val_loss: 3.4543 - val_accuracy: 0.6415\n",
            "Epoch 433/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0127 - accuracy: 0.9918 - val_loss: 3.3238 - val_accuracy: 0.6415\n",
            "Epoch 434/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 3.4468 - val_accuracy: 0.6415\n",
            "Epoch 435/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0117 - accuracy: 0.9836 - val_loss: 3.5500 - val_accuracy: 0.6415\n",
            "Epoch 436/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0142 - accuracy: 0.9836 - val_loss: 3.5727 - val_accuracy: 0.6415\n",
            "Epoch 437/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0112 - accuracy: 0.9918 - val_loss: 3.6533 - val_accuracy: 0.6415\n",
            "Epoch 438/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 3.5244 - val_accuracy: 0.6415\n",
            "Epoch 439/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0283 - accuracy: 0.9754 - val_loss: 3.0456 - val_accuracy: 0.6981\n",
            "Epoch 440/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0135 - accuracy: 0.9918 - val_loss: 3.0483 - val_accuracy: 0.6981\n",
            "Epoch 441/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0163 - accuracy: 0.9836 - val_loss: 3.0689 - val_accuracy: 0.6981\n",
            "Epoch 442/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0133 - accuracy: 0.9836 - val_loss: 3.0434 - val_accuracy: 0.6981\n",
            "Epoch 443/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0117 - accuracy: 0.9836 - val_loss: 3.0519 - val_accuracy: 0.6981\n",
            "Epoch 444/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0141 - accuracy: 0.9918 - val_loss: 3.0000 - val_accuracy: 0.6604\n",
            "Epoch 445/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 3.0063 - val_accuracy: 0.6981\n",
            "Epoch 446/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 2.8705 - val_accuracy: 0.6604\n",
            "Epoch 447/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0153 - accuracy: 0.9836 - val_loss: 2.9191 - val_accuracy: 0.6604\n",
            "Epoch 448/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0115 - accuracy: 0.9918 - val_loss: 2.9098 - val_accuracy: 0.6415\n",
            "Epoch 449/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0128 - accuracy: 0.9836 - val_loss: 2.8935 - val_accuracy: 0.6604\n",
            "Epoch 450/500\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 3.0567 - val_accuracy: 0.6604\n",
            "Epoch 451/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0098 - accuracy: 0.9918 - val_loss: 3.0330 - val_accuracy: 0.6604\n",
            "Epoch 452/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 2.9820 - val_accuracy: 0.6792\n",
            "Epoch 453/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0130 - accuracy: 0.9836 - val_loss: 3.1344 - val_accuracy: 0.6604\n",
            "Epoch 454/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0138 - accuracy: 0.9836 - val_loss: 3.2937 - val_accuracy: 0.6604\n",
            "Epoch 455/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0177 - accuracy: 0.9918 - val_loss: 3.3240 - val_accuracy: 0.6604\n",
            "Epoch 456/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0123 - accuracy: 0.9918 - val_loss: 3.3669 - val_accuracy: 0.6604\n",
            "Epoch 457/500\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0120 - accuracy: 0.9918 - val_loss: 3.3739 - val_accuracy: 0.6604\n",
            "Epoch 458/500\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0155 - accuracy: 0.9918 - val_loss: 3.4920 - val_accuracy: 0.6604\n",
            "Epoch 459/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0143 - accuracy: 0.9836 - val_loss: 3.7684 - val_accuracy: 0.6604\n",
            "Epoch 460/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 3.7144 - val_accuracy: 0.6604\n",
            "Epoch 461/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 3.7792 - val_accuracy: 0.6604\n",
            "Epoch 462/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0123 - accuracy: 0.9836 - val_loss: 3.6967 - val_accuracy: 0.6604\n",
            "Epoch 463/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0113 - accuracy: 0.9918 - val_loss: 3.6488 - val_accuracy: 0.6604\n",
            "Epoch 464/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 3.1726 - val_accuracy: 0.6792\n",
            "Epoch 465/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0170 - accuracy: 0.9836 - val_loss: 3.5630 - val_accuracy: 0.6604\n",
            "Epoch 466/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 3.4954 - val_accuracy: 0.6604\n",
            "Epoch 467/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0149 - accuracy: 0.9918 - val_loss: 3.5300 - val_accuracy: 0.6604\n",
            "Epoch 468/500\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0132 - accuracy: 0.9836 - val_loss: 3.3959 - val_accuracy: 0.6604\n",
            "Epoch 469/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 3.3681 - val_accuracy: 0.6604\n",
            "Epoch 470/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0120 - accuracy: 0.9918 - val_loss: 3.4038 - val_accuracy: 0.6604\n",
            "Epoch 471/500\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0183 - accuracy: 0.9836 - val_loss: 4.0458 - val_accuracy: 0.6604\n",
            "Epoch 472/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 3.9662 - val_accuracy: 0.6415\n",
            "Epoch 473/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0146 - accuracy: 0.9836 - val_loss: 3.8104 - val_accuracy: 0.6604\n",
            "Epoch 474/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 3.8680 - val_accuracy: 0.6604\n",
            "Epoch 475/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0114 - accuracy: 0.9918 - val_loss: 3.8090 - val_accuracy: 0.6604\n",
            "Epoch 476/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0128 - accuracy: 0.9918 - val_loss: 3.8740 - val_accuracy: 0.6604\n",
            "Epoch 477/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0113 - accuracy: 0.9918 - val_loss: 3.8348 - val_accuracy: 0.6604\n",
            "Epoch 478/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 3.8618 - val_accuracy: 0.6604\n",
            "Epoch 479/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0132 - accuracy: 0.9918 - val_loss: 3.8009 - val_accuracy: 0.6604\n",
            "Epoch 480/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0118 - accuracy: 0.9918 - val_loss: 3.8464 - val_accuracy: 0.6604\n",
            "Epoch 481/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0164 - accuracy: 0.9836 - val_loss: 3.9835 - val_accuracy: 0.6604\n",
            "Epoch 482/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 3.9697 - val_accuracy: 0.6604\n",
            "Epoch 483/500\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 3.8268 - val_accuracy: 0.6604\n",
            "Epoch 484/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0109 - accuracy: 0.9918 - val_loss: 3.9286 - val_accuracy: 0.6604\n",
            "Epoch 485/500\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 4.0454 - val_accuracy: 0.6604\n",
            "Epoch 486/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 4.0476 - val_accuracy: 0.6604\n",
            "Epoch 487/500\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0142 - accuracy: 0.9836 - val_loss: 3.9016 - val_accuracy: 0.6604\n",
            "Epoch 488/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0156 - accuracy: 0.9836 - val_loss: 3.4992 - val_accuracy: 0.6604\n",
            "Epoch 489/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.5975 - val_accuracy: 0.6604\n",
            "Epoch 490/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0144 - accuracy: 0.9836 - val_loss: 3.6799 - val_accuracy: 0.6604\n",
            "Epoch 491/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0322 - accuracy: 0.9836 - val_loss: 3.4875 - val_accuracy: 0.6604\n",
            "Epoch 492/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0154 - accuracy: 0.9836 - val_loss: 3.4825 - val_accuracy: 0.6604\n",
            "Epoch 493/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0406 - accuracy: 0.9836 - val_loss: 3.4861 - val_accuracy: 0.6604\n",
            "Epoch 494/500\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 3.4556 - val_accuracy: 0.6604\n",
            "Epoch 495/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 3.4632 - val_accuracy: 0.6604\n",
            "Epoch 496/500\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0141 - accuracy: 0.9836 - val_loss: 3.6395 - val_accuracy: 0.6604\n",
            "Epoch 497/500\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0116 - accuracy: 0.9918 - val_loss: 3.6241 - val_accuracy: 0.6604\n",
            "Epoch 498/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0109 - accuracy: 0.9918 - val_loss: 3.5788 - val_accuracy: 0.6604\n",
            "Epoch 499/500\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.5798 - val_accuracy: 0.6604\n",
            "Epoch 500/500\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0138 - accuracy: 0.9836 - val_loss: 3.5348 - val_accuracy: 0.6604\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f95364c7b10>"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj9bWUtM-FEB"
      },
      "source": [
        "* Neural Network for Stemmed data with adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_lSKBcv0PE1",
        "outputId": "883155c7-0532-4ea7-ab76-bacf69f2cafc"
      },
      "source": [
        "NN_s1 = Sequential()\n",
        "NN_s1.add(Dense(200,input_shape=(158,),activation='relu'))\n",
        "NN_s1.add(Dense(150,activation='relu'))\n",
        "NN_s1.add(Dense(100,activation='relu'))\n",
        "NN_s1.add(Dense(50,activation='relu'))\n",
        "NN_s1.add(Dense(25,activation='relu'))\n",
        "NN_s1.add(Dense(8,activation='softmax'))\n",
        "NN_s1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "NN_s1.fit(X_train_S_tf1,y_train_tc,epochs=100,validation_data=(X_test_S_tf1,y_test_tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 73ms/step - loss: 2.0809 - accuracy: 0.0984 - val_loss: 2.0728 - val_accuracy: 0.1887\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0588 - accuracy: 0.2951 - val_loss: 2.0665 - val_accuracy: 0.1321\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0382 - accuracy: 0.3361 - val_loss: 2.0538 - val_accuracy: 0.1321\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0071 - accuracy: 0.3525 - val_loss: 2.0371 - val_accuracy: 0.1887\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.9694 - accuracy: 0.3852 - val_loss: 2.0122 - val_accuracy: 0.2264\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.9079 - accuracy: 0.4590 - val_loss: 1.9755 - val_accuracy: 0.2264\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 1.8289 - accuracy: 0.4426 - val_loss: 1.9278 - val_accuracy: 0.2453\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.7199 - accuracy: 0.5000 - val_loss: 1.8716 - val_accuracy: 0.2642\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.5752 - accuracy: 0.5492 - val_loss: 1.8002 - val_accuracy: 0.2830\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.4059 - accuracy: 0.6066 - val_loss: 1.7190 - val_accuracy: 0.3396\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.2067 - accuracy: 0.7213 - val_loss: 1.6157 - val_accuracy: 0.4151\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.9867 - accuracy: 0.7787 - val_loss: 1.5124 - val_accuracy: 0.5094\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.7795 - accuracy: 0.8443 - val_loss: 1.3959 - val_accuracy: 0.5660\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5731 - accuracy: 0.8852 - val_loss: 1.2995 - val_accuracy: 0.5094\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3942 - accuracy: 0.8934 - val_loss: 1.2261 - val_accuracy: 0.5660\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.2721 - accuracy: 0.9098 - val_loss: 1.1739 - val_accuracy: 0.5849\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1813 - accuracy: 0.9672 - val_loss: 1.1328 - val_accuracy: 0.6981\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1242 - accuracy: 0.9754 - val_loss: 1.1209 - val_accuracy: 0.6981\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0849 - accuracy: 0.9918 - val_loss: 1.1167 - val_accuracy: 0.6792\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0538 - accuracy: 0.9918 - val_loss: 1.1420 - val_accuracy: 0.6792\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0482 - accuracy: 0.9918 - val_loss: 1.1541 - val_accuracy: 0.6604\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0270 - accuracy: 0.9918 - val_loss: 1.1581 - val_accuracy: 0.6604\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0299 - accuracy: 0.9918 - val_loss: 1.1703 - val_accuracy: 0.6981\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0206 - accuracy: 0.9918 - val_loss: 1.1911 - val_accuracy: 0.6792\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0188 - accuracy: 0.9836 - val_loss: 1.1961 - val_accuracy: 0.6981\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0204 - accuracy: 0.9918 - val_loss: 1.1977 - val_accuracy: 0.6981\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.0241 - accuracy: 0.9918 - val_loss: 1.1850 - val_accuracy: 0.6792\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9918 - val_loss: 1.2129 - val_accuracy: 0.6792\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0253 - accuracy: 0.9918 - val_loss: 1.2719 - val_accuracy: 0.6415\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0223 - accuracy: 0.9918 - val_loss: 1.1968 - val_accuracy: 0.6792\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 1.2053 - val_accuracy: 0.6604\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.2505 - val_accuracy: 0.6415\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0159 - accuracy: 0.9918 - val_loss: 1.2880 - val_accuracy: 0.6415\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0245 - accuracy: 0.9918 - val_loss: 1.2400 - val_accuracy: 0.6792\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 1.2050 - val_accuracy: 0.6792\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0145 - accuracy: 0.9918 - val_loss: 1.2073 - val_accuracy: 0.6604\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 1.2181 - val_accuracy: 0.6415\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0129 - accuracy: 0.9918 - val_loss: 1.2626 - val_accuracy: 0.6792\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0167 - accuracy: 0.9918 - val_loss: 1.2608 - val_accuracy: 0.6792\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0176 - accuracy: 0.9918 - val_loss: 1.2436 - val_accuracy: 0.6981\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0138 - accuracy: 0.9836 - val_loss: 1.2276 - val_accuracy: 0.6792\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0176 - accuracy: 0.9918 - val_loss: 1.2249 - val_accuracy: 0.6792\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0169 - accuracy: 0.9918 - val_loss: 1.2507 - val_accuracy: 0.6981\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0140 - accuracy: 0.9918 - val_loss: 1.2460 - val_accuracy: 0.6981\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0125 - accuracy: 0.9918 - val_loss: 1.2340 - val_accuracy: 0.6792\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0156 - accuracy: 0.9918 - val_loss: 1.2377 - val_accuracy: 0.6792\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0135 - accuracy: 0.9918 - val_loss: 1.2726 - val_accuracy: 0.6792\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0154 - accuracy: 0.9918 - val_loss: 1.2535 - val_accuracy: 0.6981\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0139 - accuracy: 0.9918 - val_loss: 1.2353 - val_accuracy: 0.6981\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0128 - accuracy: 0.9918 - val_loss: 1.2265 - val_accuracy: 0.6792\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0163 - accuracy: 0.9918 - val_loss: 1.2408 - val_accuracy: 0.6981\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0139 - accuracy: 0.9836 - val_loss: 1.2600 - val_accuracy: 0.6981\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0140 - accuracy: 0.9918 - val_loss: 1.2640 - val_accuracy: 0.6981\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0145 - accuracy: 0.9918 - val_loss: 1.2434 - val_accuracy: 0.6792\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0148 - accuracy: 0.9836 - val_loss: 1.2465 - val_accuracy: 0.6792\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0168 - accuracy: 0.9918 - val_loss: 1.2482 - val_accuracy: 0.6792\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0130 - accuracy: 0.9918 - val_loss: 1.3121 - val_accuracy: 0.6415\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0262 - accuracy: 0.9918 - val_loss: 1.3358 - val_accuracy: 0.6415\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0113 - accuracy: 0.9918 - val_loss: 1.2528 - val_accuracy: 0.6415\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0380 - accuracy: 0.9918 - val_loss: 1.3538 - val_accuracy: 0.6226\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0324 - accuracy: 0.9918 - val_loss: 1.2541 - val_accuracy: 0.6981\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0173 - accuracy: 0.9918 - val_loss: 1.3155 - val_accuracy: 0.6792\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0181 - accuracy: 0.9918 - val_loss: 1.2531 - val_accuracy: 0.6792\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0160 - accuracy: 0.9918 - val_loss: 1.2580 - val_accuracy: 0.6792\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0190 - accuracy: 0.9836 - val_loss: 1.3000 - val_accuracy: 0.6792\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0150 - accuracy: 0.9918 - val_loss: 1.2915 - val_accuracy: 0.6604\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0127 - accuracy: 0.9918 - val_loss: 1.2662 - val_accuracy: 0.6792\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0132 - accuracy: 0.9918 - val_loss: 1.2672 - val_accuracy: 0.6792\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0151 - accuracy: 0.9918 - val_loss: 1.2794 - val_accuracy: 0.6792\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0128 - accuracy: 0.9836 - val_loss: 1.2930 - val_accuracy: 0.6792\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 1.2943 - val_accuracy: 0.6792\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0185 - accuracy: 0.9836 - val_loss: 1.2723 - val_accuracy: 0.6792\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 1.3015 - val_accuracy: 0.6792\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0160 - accuracy: 0.9918 - val_loss: 1.2887 - val_accuracy: 0.6792\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9918 - val_loss: 1.2920 - val_accuracy: 0.6792\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0196 - accuracy: 0.9836 - val_loss: 1.2685 - val_accuracy: 0.6604\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 0.9918 - val_loss: 1.2690 - val_accuracy: 0.6792\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0112 - accuracy: 0.9918 - val_loss: 1.3258 - val_accuracy: 0.6792\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0261 - accuracy: 0.9918 - val_loss: 1.3697 - val_accuracy: 0.6792\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0136 - accuracy: 0.9918 - val_loss: 1.2721 - val_accuracy: 0.6981\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0268 - accuracy: 0.9918 - val_loss: 1.3330 - val_accuracy: 0.6604\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0244 - accuracy: 0.9918 - val_loss: 1.2927 - val_accuracy: 0.6604\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0247 - accuracy: 0.9836 - val_loss: 1.3746 - val_accuracy: 0.6604\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0154 - accuracy: 0.9918 - val_loss: 1.3298 - val_accuracy: 0.6792\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.9836 - val_loss: 1.3027 - val_accuracy: 0.6604\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0156 - accuracy: 0.9918 - val_loss: 1.3017 - val_accuracy: 0.6792\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.9918 - val_loss: 1.3056 - val_accuracy: 0.6792\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 0.9836 - val_loss: 1.3197 - val_accuracy: 0.6604\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0140 - accuracy: 0.9918 - val_loss: 1.3094 - val_accuracy: 0.6792\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 1.2912 - val_accuracy: 0.6604\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0173 - accuracy: 0.9918 - val_loss: 1.2906 - val_accuracy: 0.6604\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 0.9918 - val_loss: 1.2938 - val_accuracy: 0.6792\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0141 - accuracy: 0.9918 - val_loss: 1.3119 - val_accuracy: 0.6604\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0136 - accuracy: 0.9918 - val_loss: 1.2931 - val_accuracy: 0.6604\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 1.2756 - val_accuracy: 0.6792\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0127 - accuracy: 0.9918 - val_loss: 1.2725 - val_accuracy: 0.6604\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0152 - accuracy: 0.9836 - val_loss: 1.2847 - val_accuracy: 0.6792\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0144 - accuracy: 0.9836 - val_loss: 1.2776 - val_accuracy: 0.6981\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0128 - accuracy: 0.9918 - val_loss: 1.2830 - val_accuracy: 0.6981\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0126 - accuracy: 0.9918 - val_loss: 1.2948 - val_accuracy: 0.6792\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa5656ea90>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSQnvzNs-Mvt"
      },
      "source": [
        "* Neural Network with Stemmed data using SGD optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBR-ohEE752R",
        "outputId": "b0543ce8-5363-40e9-80e3-b1012859b14e"
      },
      "source": [
        "NN_s2 = Sequential()\n",
        "NN_s2.add(Dense(200,input_shape=(158,),activation='relu'))\n",
        "NN_s2.add(Dense(150,activation='relu'))\n",
        "NN_s2.add(Dense(100,activation='relu'))\n",
        "NN_s2.add(Dense(50,activation='relu'))\n",
        "NN_s2.add(Dense(25,activation='relu'))\n",
        "NN_s2.add(Dense(8,activation='softmax'))\n",
        "NN_s2.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "NN_s2.fit(X_train_S_tf1,y_train_tc,epochs=100,validation_data=(X_test_S_tf1,y_test_tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 71ms/step - loss: 2.0818 - accuracy: 0.1066 - val_loss: 2.0814 - val_accuracy: 0.1132\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0813 - accuracy: 0.1066 - val_loss: 2.0812 - val_accuracy: 0.1132\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0805 - accuracy: 0.1066 - val_loss: 2.0809 - val_accuracy: 0.1509\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0799 - accuracy: 0.1230 - val_loss: 2.0807 - val_accuracy: 0.1698\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0792 - accuracy: 0.1311 - val_loss: 2.0804 - val_accuracy: 0.1698\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0787 - accuracy: 0.1311 - val_loss: 2.0802 - val_accuracy: 0.1698\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0780 - accuracy: 0.1721 - val_loss: 2.0800 - val_accuracy: 0.1132\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0774 - accuracy: 0.1639 - val_loss: 2.0798 - val_accuracy: 0.1509\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0771 - accuracy: 0.1885 - val_loss: 2.0795 - val_accuracy: 0.1698\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0764 - accuracy: 0.1803 - val_loss: 2.0793 - val_accuracy: 0.1509\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0760 - accuracy: 0.1967 - val_loss: 2.0791 - val_accuracy: 0.1698\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0754 - accuracy: 0.2049 - val_loss: 2.0789 - val_accuracy: 0.1698\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0749 - accuracy: 0.2049 - val_loss: 2.0788 - val_accuracy: 0.1698\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0746 - accuracy: 0.2049 - val_loss: 2.0785 - val_accuracy: 0.1698\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0743 - accuracy: 0.2049 - val_loss: 2.0784 - val_accuracy: 0.1698\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0738 - accuracy: 0.2049 - val_loss: 2.0783 - val_accuracy: 0.1698\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0733 - accuracy: 0.2131 - val_loss: 2.0781 - val_accuracy: 0.1698\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0729 - accuracy: 0.2131 - val_loss: 2.0779 - val_accuracy: 0.1698\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0726 - accuracy: 0.2131 - val_loss: 2.0778 - val_accuracy: 0.1698\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0721 - accuracy: 0.2131 - val_loss: 2.0776 - val_accuracy: 0.1698\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0717 - accuracy: 0.2131 - val_loss: 2.0774 - val_accuracy: 0.1698\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0714 - accuracy: 0.2131 - val_loss: 2.0772 - val_accuracy: 0.1698\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0711 - accuracy: 0.2131 - val_loss: 2.0771 - val_accuracy: 0.1698\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0706 - accuracy: 0.2049 - val_loss: 2.0769 - val_accuracy: 0.1698\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0702 - accuracy: 0.2049 - val_loss: 2.0768 - val_accuracy: 0.1698\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0700 - accuracy: 0.2049 - val_loss: 2.0766 - val_accuracy: 0.1698\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0696 - accuracy: 0.1885 - val_loss: 2.0764 - val_accuracy: 0.1698\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0693 - accuracy: 0.1967 - val_loss: 2.0763 - val_accuracy: 0.1698\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0689 - accuracy: 0.1885 - val_loss: 2.0762 - val_accuracy: 0.1698\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0686 - accuracy: 0.1885 - val_loss: 2.0761 - val_accuracy: 0.1698\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0681 - accuracy: 0.1885 - val_loss: 2.0760 - val_accuracy: 0.1698\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0677 - accuracy: 0.1803 - val_loss: 2.0758 - val_accuracy: 0.1698\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0674 - accuracy: 0.1803 - val_loss: 2.0757 - val_accuracy: 0.1698\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0670 - accuracy: 0.1803 - val_loss: 2.0755 - val_accuracy: 0.1698\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 2.0666 - accuracy: 0.1967 - val_loss: 2.0754 - val_accuracy: 0.1698\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0663 - accuracy: 0.1803 - val_loss: 2.0752 - val_accuracy: 0.1698\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0661 - accuracy: 0.1803 - val_loss: 2.0751 - val_accuracy: 0.1698\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0657 - accuracy: 0.1803 - val_loss: 2.0749 - val_accuracy: 0.1698\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0653 - accuracy: 0.1803 - val_loss: 2.0748 - val_accuracy: 0.1698\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0649 - accuracy: 0.1803 - val_loss: 2.0746 - val_accuracy: 0.1698\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0646 - accuracy: 0.1803 - val_loss: 2.0745 - val_accuracy: 0.1698\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0643 - accuracy: 0.1803 - val_loss: 2.0745 - val_accuracy: 0.1698\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0638 - accuracy: 0.1803 - val_loss: 2.0743 - val_accuracy: 0.1698\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0635 - accuracy: 0.1803 - val_loss: 2.0742 - val_accuracy: 0.1698\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0631 - accuracy: 0.1803 - val_loss: 2.0740 - val_accuracy: 0.1698\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0628 - accuracy: 0.1803 - val_loss: 2.0739 - val_accuracy: 0.1698\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0624 - accuracy: 0.1803 - val_loss: 2.0738 - val_accuracy: 0.1698\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0621 - accuracy: 0.1721 - val_loss: 2.0738 - val_accuracy: 0.1698\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0617 - accuracy: 0.1803 - val_loss: 2.0737 - val_accuracy: 0.1698\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0617 - accuracy: 0.1803 - val_loss: 2.0736 - val_accuracy: 0.1698\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0612 - accuracy: 0.1803 - val_loss: 2.0735 - val_accuracy: 0.1698\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0607 - accuracy: 0.1885 - val_loss: 2.0735 - val_accuracy: 0.1698\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0604 - accuracy: 0.1803 - val_loss: 2.0734 - val_accuracy: 0.1698\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0602 - accuracy: 0.1721 - val_loss: 2.0733 - val_accuracy: 0.1698\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0600 - accuracy: 0.1803 - val_loss: 2.0732 - val_accuracy: 0.1698\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0593 - accuracy: 0.1803 - val_loss: 2.0731 - val_accuracy: 0.1698\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0591 - accuracy: 0.1803 - val_loss: 2.0731 - val_accuracy: 0.1698\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0588 - accuracy: 0.1803 - val_loss: 2.0730 - val_accuracy: 0.1698\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0586 - accuracy: 0.1803 - val_loss: 2.0729 - val_accuracy: 0.1698\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0582 - accuracy: 0.1803 - val_loss: 2.0728 - val_accuracy: 0.1698\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0580 - accuracy: 0.1803 - val_loss: 2.0728 - val_accuracy: 0.1698\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0576 - accuracy: 0.1803 - val_loss: 2.0727 - val_accuracy: 0.1698\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0574 - accuracy: 0.1803 - val_loss: 2.0726 - val_accuracy: 0.1698\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0570 - accuracy: 0.1721 - val_loss: 2.0726 - val_accuracy: 0.1698\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0567 - accuracy: 0.1803 - val_loss: 2.0726 - val_accuracy: 0.1698\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0566 - accuracy: 0.1803 - val_loss: 2.0726 - val_accuracy: 0.1698\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 2.0562 - accuracy: 0.1803 - val_loss: 2.0725 - val_accuracy: 0.1698\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0559 - accuracy: 0.1721 - val_loss: 2.0724 - val_accuracy: 0.1698\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0557 - accuracy: 0.1803 - val_loss: 2.0723 - val_accuracy: 0.1698\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0554 - accuracy: 0.1803 - val_loss: 2.0723 - val_accuracy: 0.1698\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0552 - accuracy: 0.1803 - val_loss: 2.0722 - val_accuracy: 0.1698\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0549 - accuracy: 0.1803 - val_loss: 2.0721 - val_accuracy: 0.1698\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0545 - accuracy: 0.1721 - val_loss: 2.0720 - val_accuracy: 0.1698\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0544 - accuracy: 0.1803 - val_loss: 2.0720 - val_accuracy: 0.1698\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0541 - accuracy: 0.1803 - val_loss: 2.0719 - val_accuracy: 0.1698\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0536 - accuracy: 0.1803 - val_loss: 2.0718 - val_accuracy: 0.1698\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0535 - accuracy: 0.1803 - val_loss: 2.0716 - val_accuracy: 0.1698\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0531 - accuracy: 0.1803 - val_loss: 2.0715 - val_accuracy: 0.1698\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0527 - accuracy: 0.1803 - val_loss: 2.0715 - val_accuracy: 0.1698\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0526 - accuracy: 0.1721 - val_loss: 2.0713 - val_accuracy: 0.1698\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0521 - accuracy: 0.1803 - val_loss: 2.0713 - val_accuracy: 0.1698\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 2.0518 - accuracy: 0.1803 - val_loss: 2.0712 - val_accuracy: 0.1698\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0516 - accuracy: 0.1721 - val_loss: 2.0712 - val_accuracy: 0.1698\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0515 - accuracy: 0.1721 - val_loss: 2.0711 - val_accuracy: 0.1698\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0509 - accuracy: 0.1803 - val_loss: 2.0710 - val_accuracy: 0.1698\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0508 - accuracy: 0.1721 - val_loss: 2.0710 - val_accuracy: 0.1698\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0504 - accuracy: 0.1803 - val_loss: 2.0709 - val_accuracy: 0.1698\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0501 - accuracy: 0.1803 - val_loss: 2.0709 - val_accuracy: 0.1698\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0497 - accuracy: 0.1721 - val_loss: 2.0709 - val_accuracy: 0.1698\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 2.0495 - accuracy: 0.1803 - val_loss: 2.0708 - val_accuracy: 0.1698\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 2.0492 - accuracy: 0.1721 - val_loss: 2.0707 - val_accuracy: 0.1698\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0490 - accuracy: 0.1803 - val_loss: 2.0706 - val_accuracy: 0.1698\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0486 - accuracy: 0.1721 - val_loss: 2.0706 - val_accuracy: 0.1698\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0484 - accuracy: 0.1721 - val_loss: 2.0705 - val_accuracy: 0.1698\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0479 - accuracy: 0.1721 - val_loss: 2.0704 - val_accuracy: 0.1698\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0479 - accuracy: 0.1803 - val_loss: 2.0703 - val_accuracy: 0.1698\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 2.0473 - accuracy: 0.1721 - val_loss: 2.0703 - val_accuracy: 0.1698\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0471 - accuracy: 0.1803 - val_loss: 2.0701 - val_accuracy: 0.1698\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0467 - accuracy: 0.1721 - val_loss: 2.0700 - val_accuracy: 0.1698\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 2.0465 - accuracy: 0.1803 - val_loss: 2.0699 - val_accuracy: 0.1698\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa59014890>"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85z57nSN-V7G"
      },
      "source": [
        "* Neural Network with Stemmed data and RMS prop optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKShlSQR8os7",
        "outputId": "c408d1d2-6cd3-45e5-ee68-67219822d23c"
      },
      "source": [
        "NN_s3 = Sequential()\n",
        "NN_s3.add(Dense(200,input_shape=(158,),activation='relu'))\n",
        "NN_s3.add(Dense(150,activation='relu'))\n",
        "NN_s3.add(Dense(100,activation='relu'))\n",
        "NN_s3.add(Dense(50,activation='relu'))\n",
        "NN_s3.add(Dense(25,activation='relu'))\n",
        "NN_s3.add(Dense(8,activation='softmax'))\n",
        "NN_s3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "NN_s3.fit(X_train_S_tf1,y_train_tc,epochs=100,validation_data=(X_test_S_tf1,y_test_tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 76ms/step - loss: 2.0757 - accuracy: 0.1311 - val_loss: 2.0618 - val_accuracy: 0.2642\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 2.0238 - accuracy: 0.3361 - val_loss: 2.0404 - val_accuracy: 0.1698\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.9493 - accuracy: 0.2869 - val_loss: 2.0046 - val_accuracy: 0.1698\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.8499 - accuracy: 0.3361 - val_loss: 1.9234 - val_accuracy: 0.3208\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.7124 - accuracy: 0.4836 - val_loss: 1.8528 - val_accuracy: 0.2830\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.5431 - accuracy: 0.5328 - val_loss: 1.7429 - val_accuracy: 0.4528\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.3771 - accuracy: 0.6230 - val_loss: 1.7553 - val_accuracy: 0.2830\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.2094 - accuracy: 0.6475 - val_loss: 1.6023 - val_accuracy: 0.4340\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.0314 - accuracy: 0.7213 - val_loss: 1.5228 - val_accuracy: 0.4906\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.8599 - accuracy: 0.8115 - val_loss: 1.3856 - val_accuracy: 0.5283\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.7134 - accuracy: 0.8525 - val_loss: 1.3229 - val_accuracy: 0.4340\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.6046 - accuracy: 0.8443 - val_loss: 1.2625 - val_accuracy: 0.4151\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.4559 - accuracy: 0.8770 - val_loss: 1.1696 - val_accuracy: 0.5849\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3793 - accuracy: 0.9344 - val_loss: 1.1614 - val_accuracy: 0.6038\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.2958 - accuracy: 0.9508 - val_loss: 1.0952 - val_accuracy: 0.6038\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2204 - accuracy: 0.9672 - val_loss: 1.0747 - val_accuracy: 0.6038\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1873 - accuracy: 0.9836 - val_loss: 1.0724 - val_accuracy: 0.6226\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1241 - accuracy: 0.9918 - val_loss: 1.0205 - val_accuracy: 0.6226\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0921 - accuracy: 0.9918 - val_loss: 1.0539 - val_accuracy: 0.6415\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0715 - accuracy: 0.9918 - val_loss: 1.0279 - val_accuracy: 0.6226\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0774 - accuracy: 0.9836 - val_loss: 1.2406 - val_accuracy: 0.5094\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0876 - accuracy: 0.9836 - val_loss: 1.0552 - val_accuracy: 0.6038\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0465 - accuracy: 0.9918 - val_loss: 1.3318 - val_accuracy: 0.6038\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0562 - accuracy: 0.9836 - val_loss: 1.0533 - val_accuracy: 0.6415\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0337 - accuracy: 0.9918 - val_loss: 1.1620 - val_accuracy: 0.5472\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0541 - accuracy: 0.9836 - val_loss: 1.0982 - val_accuracy: 0.6226\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0463 - accuracy: 0.9836 - val_loss: 1.0583 - val_accuracy: 0.6415\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0303 - accuracy: 0.9918 - val_loss: 1.1809 - val_accuracy: 0.6604\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0227 - accuracy: 0.9918 - val_loss: 1.0748 - val_accuracy: 0.6415\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0284 - accuracy: 0.9918 - val_loss: 1.1694 - val_accuracy: 0.6415\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0271 - accuracy: 0.9918 - val_loss: 1.2704 - val_accuracy: 0.6038\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0305 - accuracy: 0.9918 - val_loss: 1.4806 - val_accuracy: 0.6226\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0272 - accuracy: 0.9918 - val_loss: 1.1205 - val_accuracy: 0.6415\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0253 - accuracy: 0.9918 - val_loss: 1.2591 - val_accuracy: 0.6604\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0259 - accuracy: 0.9918 - val_loss: 1.2095 - val_accuracy: 0.5849\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0397 - accuracy: 0.9836 - val_loss: 1.1353 - val_accuracy: 0.6415\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 1.3722 - val_accuracy: 0.6226\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0231 - accuracy: 0.9918 - val_loss: 1.1778 - val_accuracy: 0.6038\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0241 - accuracy: 0.9918 - val_loss: 1.2268 - val_accuracy: 0.6415\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0286 - accuracy: 0.9836 - val_loss: 1.2545 - val_accuracy: 0.6415\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0217 - accuracy: 0.9918 - val_loss: 1.2420 - val_accuracy: 0.6038\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0226 - accuracy: 0.9918 - val_loss: 1.3155 - val_accuracy: 0.6415\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0215 - accuracy: 0.9918 - val_loss: 1.1867 - val_accuracy: 0.6415\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0275 - accuracy: 0.9836 - val_loss: 1.2454 - val_accuracy: 0.6038\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0190 - accuracy: 0.9918 - val_loss: 1.2606 - val_accuracy: 0.6415\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0249 - accuracy: 0.9836 - val_loss: 1.3172 - val_accuracy: 0.6415\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0205 - accuracy: 0.9918 - val_loss: 1.2638 - val_accuracy: 0.6226\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0144 - accuracy: 0.9918 - val_loss: 1.3005 - val_accuracy: 0.6415\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0254 - accuracy: 0.9836 - val_loss: 1.3815 - val_accuracy: 0.6415\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0207 - accuracy: 0.9918 - val_loss: 1.3119 - val_accuracy: 0.6415\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0237 - accuracy: 0.9836 - val_loss: 1.3611 - val_accuracy: 0.6038\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0245 - accuracy: 0.9836 - val_loss: 1.3110 - val_accuracy: 0.6415\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0217 - accuracy: 0.9836 - val_loss: 1.3266 - val_accuracy: 0.6415\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0211 - accuracy: 0.9836 - val_loss: 1.3862 - val_accuracy: 0.6226\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9918 - val_loss: 1.4842 - val_accuracy: 0.6415\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 0.9918 - val_loss: 1.3811 - val_accuracy: 0.6226\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0200 - accuracy: 0.9836 - val_loss: 1.3975 - val_accuracy: 0.6226\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0191 - accuracy: 0.9918 - val_loss: 1.5642 - val_accuracy: 0.6226\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0178 - accuracy: 0.9918 - val_loss: 1.4670 - val_accuracy: 0.6038\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0186 - accuracy: 0.9836 - val_loss: 1.4858 - val_accuracy: 0.6226\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0194 - accuracy: 0.9836 - val_loss: 1.5053 - val_accuracy: 0.6226\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0177 - accuracy: 0.9918 - val_loss: 1.6215 - val_accuracy: 0.6415\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0176 - accuracy: 0.9836 - val_loss: 1.6609 - val_accuracy: 0.6415\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0172 - accuracy: 0.9918 - val_loss: 1.6010 - val_accuracy: 0.6415\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0176 - accuracy: 0.9918 - val_loss: 1.6924 - val_accuracy: 0.6415\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0170 - accuracy: 0.9836 - val_loss: 1.7077 - val_accuracy: 0.6415\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0170 - accuracy: 0.9836 - val_loss: 1.7154 - val_accuracy: 0.6415\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0166 - accuracy: 0.9918 - val_loss: 1.7620 - val_accuracy: 0.6604\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0166 - accuracy: 0.9918 - val_loss: 1.6880 - val_accuracy: 0.6415\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0164 - accuracy: 0.9836 - val_loss: 1.7995 - val_accuracy: 0.6415\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0166 - accuracy: 0.9836 - val_loss: 1.8298 - val_accuracy: 0.6415\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0164 - accuracy: 0.9918 - val_loss: 1.7271 - val_accuracy: 0.6415\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0160 - accuracy: 0.9836 - val_loss: 1.8076 - val_accuracy: 0.6415\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0162 - accuracy: 0.9836 - val_loss: 1.8130 - val_accuracy: 0.6415\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0121 - accuracy: 0.9918 - val_loss: 1.8270 - val_accuracy: 0.6415\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0170 - accuracy: 0.9918 - val_loss: 1.8731 - val_accuracy: 0.6792\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0122 - accuracy: 0.9918 - val_loss: 1.8946 - val_accuracy: 0.6415\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0163 - accuracy: 0.9836 - val_loss: 1.9378 - val_accuracy: 0.6415\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0159 - accuracy: 0.9836 - val_loss: 1.9564 - val_accuracy: 0.6415\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0159 - accuracy: 0.9918 - val_loss: 1.8890 - val_accuracy: 0.6792\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 1.9239 - val_accuracy: 0.6415\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0165 - accuracy: 0.9918 - val_loss: 1.9574 - val_accuracy: 0.6415\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0161 - accuracy: 0.9918 - val_loss: 1.9195 - val_accuracy: 0.6792\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0154 - accuracy: 0.9918 - val_loss: 1.9674 - val_accuracy: 0.6415\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 0.9918 - val_loss: 1.9805 - val_accuracy: 0.6415\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0160 - accuracy: 0.9918 - val_loss: 2.0209 - val_accuracy: 0.6792\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0151 - accuracy: 0.9918 - val_loss: 2.0067 - val_accuracy: 0.6226\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9836 - val_loss: 2.0321 - val_accuracy: 0.6415\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0146 - accuracy: 0.9836 - val_loss: 2.0157 - val_accuracy: 0.6415\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0147 - accuracy: 0.9918 - val_loss: 2.1007 - val_accuracy: 0.6792\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0143 - accuracy: 0.9836 - val_loss: 2.0056 - val_accuracy: 0.6792\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9836 - val_loss: 1.9929 - val_accuracy: 0.6792\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0145 - accuracy: 0.9918 - val_loss: 2.0467 - val_accuracy: 0.6226\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0143 - accuracy: 0.9918 - val_loss: 2.0345 - val_accuracy: 0.6792\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 0.9918 - val_loss: 2.0341 - val_accuracy: 0.6415\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0143 - accuracy: 0.9836 - val_loss: 2.0520 - val_accuracy: 0.6415\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0143 - accuracy: 0.9918 - val_loss: 2.0902 - val_accuracy: 0.6792\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0143 - accuracy: 0.9918 - val_loss: 2.0731 - val_accuracy: 0.6226\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0144 - accuracy: 0.9918 - val_loss: 2.1163 - val_accuracy: 0.6792\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.0139 - accuracy: 0.9836 - val_loss: 2.1058 - val_accuracy: 0.6792\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa58eb6850>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmMha1WS-lA8"
      },
      "source": [
        "* Neural network with stemmed data give same accuracy for RMSprop and Adam but we will choose only Adam as it is considered the best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPR3CPUd2VF5"
      },
      "source": [
        "## **Testing the simple chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL3w5IfINZUR"
      },
      "source": [
        "with open('/content/drive/My Drive/Data Science/NLP/Project1/model_nb_S.pkl', 'rb') as file:\n",
        "  model = pickle.load(file)\n",
        "\n",
        "def preprocess(x):\n",
        "  x = \"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",x.lower()) if i not in string.punctuation])\n",
        "  x = tf.transform([PS.stem(i) for i in x.split(\" \")])\n",
        "  x = model.predict(x[0])\n",
        "  x = le.inverse_transform([x])\n",
        "  print(x)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ_eDstoi_Fu"
      },
      "source": [
        "def preprocess(x):\n",
        "  x = \"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",x.lower()) if i not in string.punctuation])\n",
        "  x = tfs.transform([PS.stem(i) for i in x.split(\" \")])\n",
        "  x = model.predict(x[0])\n",
        "  x = le.inverse_transform([x])\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAgS9VP7ldo7",
        "outputId": "da3fd910-4d96-416a-c760-127d4093793a"
      },
      "source": [
        "preprocess(\"Hello \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Link: Machine Learning wiki ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfT9dyuCqPw3"
      },
      "source": [
        "* Even though the chatbot's accuracy is good still we see that the classes it predicts is completely imbalanced, F1 score is not good for all the classes.\n",
        "\n",
        "* This leads to not having a certain response pop up eventhough your question is directly related to that.\n",
        "\n",
        "* We further clean up the data and make them little better to handle all the requests\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkHHm3IDmEWo"
      },
      "source": [
        "df3 = pd.read_csv(\"/content/drive/My Drive/Data Science/NLP/Project1/Bot data.csv\",encoding = \"ISO-8859-1\")\n",
        "\n",
        "#Updated sheet with new comments"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEI7Az7RKi6Y",
        "outputId": "94a54c49-1f8f-46ef-f250-4f1abd749d77"
      },
      "source": [
        "le = LabelEncoder()\n",
        "df3['Target'] = le.fit_transform(df3['Responses'])\n",
        "df3['Target'].value_counts()\n",
        "\n",
        "#This is slightly balanced dataset with better inputs"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    41\n",
              "3    40\n",
              "7    37\n",
              "2    36\n",
              "5    35\n",
              "6    33\n",
              "0    33\n",
              "1    31\n",
              "Name: Target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg02LJae8ejn"
      },
      "source": [
        "# pd.DataFrame(df3.groupby('Responses')['Target'].mean())\n",
        "data_dict = {0:'Hello! how can i help you ?',1:'I am your virtual learning assistant',2:'I hope I was able to assist you, Good Bye',\n",
        "             3:'Link: Machine Learning wiki',4:'Link: Neural Nets wiki',5:'Link: Olympus wiki',6:'Please use respectful words',\n",
        "             7:'Transferring the request to your PM'}\n",
        "\n",
        "def data_dict():\n",
        "  return {0:'Hello! how can i help you ?',1:'I am your virtual learning assistant',2:'I hope I was able to assist you, Good Bye',\n",
        "             3:'Link: Machine Learning wiki',4:'Link: Neural Nets wiki',5:'Link: Olympus wiki',6:'Please use respectful words',\n",
        "             7:'Transferring the request to your PM'}\n",
        "\n",
        "\n",
        "\n",
        "#Collecting data in the form of dictionary to inverse transform in the end             "
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upw7lTbJmuDR"
      },
      "source": [
        "cleaned_text1 = [[i for i in nltk.word_tokenize(\"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",t.lower()) if i not in string.punctuation]))] for t in df3['Pattern']]\n",
        "# cleaned_text1 = [[i for i in nltk.word_tokenize(\"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",t.lower()) if i not in string.punctuation])) if i not in stopwords.words('english')] for t in df3['Pattern']]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzCSfPz8n08g"
      },
      "source": [
        "PS = PorterStemmer()\n",
        "WNL = WordNetLemmatizer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cleaned_text1, df3['Target'], test_size=0.3, random_state=11)\n",
        "\n",
        "X_train_L = []\n",
        "for i in X_train:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(WNL.lemmatize(j,pos='v'))\n",
        "  X_train_L.append(m)\n",
        "\n",
        "\n",
        "X_test_L = []\n",
        "for i in X_test:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(WNL.lemmatize(j,pos='v'))\n",
        "  X_test_L.append(m)\n",
        "\n",
        "#Using Lemmatization\n",
        "\n",
        "X_train_S = []\n",
        "for i in X_train:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(PS.stem(j))\n",
        "  X_train_S.append(m)\n",
        "\n",
        "\n",
        "X_test_S = []\n",
        "for i in X_test:\n",
        "  m = []\n",
        "  for j in i:\n",
        "    m.append(PS.stem(j))\n",
        "  X_test_S.append(m)\n",
        "\n",
        "#Using Stemmer\n",
        "\n",
        "X_train_S = [\" \".join(i) for i in X_train_S]\n",
        "X_test_S = [\" \".join(i) for i in X_test_S]\n",
        "\n",
        "\n",
        "tfs = TfidfVectorizer()\n",
        "X_train_S_tf = tfs.fit_transform(X_train_S)\n",
        "X_test_S_tf = tfs.transform(X_test_S)\n",
        "\n",
        "\n",
        "X_train_L = [\" \".join(i) for i in X_train_L]\n",
        "X_test_L = [\" \".join(i) for i in X_test_L]\n",
        "tfL = TfidfVectorizer()\n",
        "X_train_L_tf = tfL.fit_transform(X_train_L)\n",
        "X_test_L_tf = tfL.transform(X_test_L)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpy4RuMhg9z3"
      },
      "source": [
        "# We use Pipe operator as this is the best model with 71% accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnGfkce6KaAI"
      },
      "source": [
        "model_nb = MultinomialNB(alpha=0.2)\n",
        "pipe = Pipeline([(\"Lemmatizingvectorizer\",tfL),\n",
        "                 (\"Model\",model_nb)])\n",
        "#using LEmma data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X7Jr2eAgZy-",
        "outputId": "cce38b4f-2e9c-42dd-af8d-8e58440a4fe4"
      },
      "source": [
        "pipe.fit(X_train_L,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('Lemmatizingvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('Model',\n",
              "                 MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM55ZoLDgzKR"
      },
      "source": [
        "pred_L_tf = pipe.predict(X_test_L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtCNftYSLMIT",
        "outputId": "534abf75-f813-4a70-f3d3-d8431a3de7e9"
      },
      "source": [
        "print(classification_report(y_test,pred_L_tf))\n",
        "\n",
        "#Even though the accuracy is bad, we can see that the F1 score is almost balanced on all classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.36      0.50        14\n",
            "           1       1.00      0.91      0.95        11\n",
            "           2       1.00      0.60      0.75        10\n",
            "           3       0.44      1.00      0.61        11\n",
            "           4       1.00      0.64      0.78        14\n",
            "           5       0.77      1.00      0.87        10\n",
            "           6       0.67      0.40      0.50         5\n",
            "           7       0.57      0.73      0.64        11\n",
            "\n",
            "    accuracy                           0.71        86\n",
            "   macro avg       0.79      0.70      0.70        86\n",
            "weighted avg       0.80      0.71      0.71        86\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM7O3-FxLnYS"
      },
      "source": [
        "model_nb_S = MultinomialNB(alpha=0.5)\n",
        "model_nb_S.fit(X_train_S_tf,y_train)\n",
        "pred_S_tf = model_nb_S.predict(X_test_S_tf)\n",
        "\n",
        "#using Stem data - Better accuracy"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaYaHLxXLxeI",
        "outputId": "31b33d97-72c0-4e98-c805-25331d5a90e7"
      },
      "source": [
        "print(classification_report(y_test,pred_S_tf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.36      0.53        14\n",
            "           1       0.90      0.82      0.86        11\n",
            "           2       1.00      0.60      0.75        10\n",
            "           3       0.42      1.00      0.59        11\n",
            "           4       1.00      0.57      0.73        14\n",
            "           5       0.77      1.00      0.87        10\n",
            "           6       0.50      0.40      0.44         5\n",
            "           7       0.57      0.73      0.64        11\n",
            "\n",
            "    accuracy                           0.69        86\n",
            "   macro avg       0.77      0.68      0.68        86\n",
            "weighted avg       0.80      0.69      0.69        86\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR2tJe4_Lzll"
      },
      "source": [
        "DTC = DecisionTreeClassifier(max_depth=33)\n",
        "DTC.fit(X_train_L_tf,y_train)\n",
        "pred_L_tf_DTC = model_nb.predict(X_test_L_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSOVHWIgMW5y",
        "outputId": "11d3e771-de83-47e1-e1bc-952d6ef98401"
      },
      "source": [
        "print(classification_report(y_test,pred_L_tf_DTC))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.36      0.50        14\n",
            "           1       1.00      0.91      0.95        11\n",
            "           2       1.00      0.60      0.75        10\n",
            "           3       0.44      1.00      0.61        11\n",
            "           4       1.00      0.64      0.78        14\n",
            "           5       0.77      1.00      0.87        10\n",
            "           6       0.67      0.40      0.50         5\n",
            "           7       0.57      0.73      0.64        11\n",
            "\n",
            "    accuracy                           0.71        86\n",
            "   macro avg       0.79      0.70      0.70        86\n",
            "weighted avg       0.80      0.71      0.71        86\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip0qkpoXNBx9"
      },
      "source": [
        "RFC1 = RandomForestClassifier()\n",
        "param_grid = {\"n_estimators\":[50,100,150,200,250,300,350,400,450,500],\"criterion\":['gini','entropy']}\n",
        "GSCV1 = GridSearchCV(estimator=RFC1,param_grid=param_grid)\n",
        "GSCV1.fit(X_train_S_tf,y_train)\n",
        "pred_S_tf_RFC = GSCV1.predict(X_test_S_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggNlDztxNdwp",
        "outputId": "a8cab25a-9abf-4a6d-bfbf-48f24d95dd53"
      },
      "source": [
        "print(classification_report(y_test,pred_S_tf_RFC))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.43      0.48        14\n",
            "           1       0.89      0.73      0.80        11\n",
            "           2       1.00      0.20      0.33        10\n",
            "           3       0.50      0.55      0.52        11\n",
            "           4       0.48      0.79      0.59        14\n",
            "           5       0.83      1.00      0.91        10\n",
            "           6       0.40      0.40      0.40         5\n",
            "           7       0.67      0.73      0.70        11\n",
            "\n",
            "    accuracy                           0.62        86\n",
            "   macro avg       0.66      0.60      0.59        86\n",
            "weighted avg       0.67      0.62      0.60        86\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3AWLzQarPIG"
      },
      "source": [
        "## We pickup the pipelined Naive bayes model that takes in lemmatized vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEdBwuGbQVUB"
      },
      "source": [
        "pk_du = pickle.dump(pipe,open('/content/drive/My Drive/Data Science/NLP/Project1/pipe.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ8PVwevV7b1"
      },
      "source": [
        "with open('/content/drive/My Drive/Data Science/NLP/Project1/pipe.pkl', 'rb') as file:\n",
        "  model = pickle.load(file)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMhXcgq0kkpq",
        "outputId": "10118887-1309-4139-b257-bfab2e90086c"
      },
      "source": [
        "model"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('Lemmatizingvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('Model',\n",
              "                 MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpxcKAkCTBig"
      },
      "source": [
        "# def preprocess(x):\n",
        "#   x = \"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",x.lower()) if i not in string.punctuation])\n",
        "#   x = [WNL.lemmatize(i) for i in x.split(\" \")]\n",
        "#   x = [\" \".join(x)]\n",
        "#   pred = model.predict(x)\n",
        "#   x = le.inverse_transform([pred])\n",
        "#   return x[0],pred\n",
        "\n",
        "\n",
        "le = LabelEncoder()"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvty6QgGrdQ7"
      },
      "source": [
        "## The Great Learning Chat bot!!\n",
        "\n",
        "* This bot not only takes End or Quit as input to quit the conversation, it also takes any response like **\"thanks for the help\"**  and  **\"Transferring the request to your PM\"**\n",
        "to end the conversation\n",
        "\n",
        "* The model's accuracy is 71% and it gives a comparitively good prediction than the previous dataset. The more we add data the better we can expect responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVRXPkYbmZ8W"
      },
      "source": [
        "def preprocess(x):\n",
        "  WNL = WordNetLemmatizer()\n",
        "  data = data_dict()\n",
        "  pred = model.predict([\" \".join([WNL.lemmatize(i) for i in \"\".join([i for i in re.sub(\"<.*>|[\\s]{2,}| [.0-9]+| [.0-9]{2,10}|[^a-zA-Z]\",\" \",x.lower()) if i not in string.punctuation]).split(\" \")])])\n",
        "  x = data[pred[0]]\n",
        "  return x,pred"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L831Yrf5dLT1"
      },
      "source": [
        "def bot():\n",
        "  print(\"To start, say 'Hello' and to end say 'Quit' or 'End' \\n\")\n",
        "  i = input(\"You: \").lower()\n",
        "  if (i== 'end' or i=='quit'):\n",
        "    print(\"\\nThanks for connecting\")\n",
        "  else:\n",
        "    x,y = preprocess(i)\n",
        "    if y == 2 or y == 7:\n",
        "      print(\"\\n\"+\"Bot: \"+x+\"\\n\")\n",
        "    else:\n",
        "      print(\"\\n\"+ \"Bot: \" +x+\"\\n\")\n",
        "      bot()"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1zp-oJjhCZj",
        "outputId": "e293eb36-9747-4002-fdf6-797d9ad307a6"
      },
      "source": [
        "bot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To start, say 'Hello' and to end say 'Quit' or 'End' \n",
            "\n",
            "You: Hello there\n",
            "\n",
            "Bot: Hello! how can i help you ?\n",
            "\n",
            "To start, say 'Hello' and to end say 'Quit' or 'End' \n",
            "\n",
            "You: What is the course fee?\n",
            "\n",
            "Bot: Link: Olympus wiki\n",
            "\n",
            "To start, say 'Hello' and to end say 'Quit' or 'End' \n",
            "\n",
            "You: stupid bitch\n",
            "\n",
            "Bot: Please use respectful words\n",
            "\n",
            "To start, say 'Hello' and to end say 'Quit' or 'End' \n",
            "\n",
            "You: Can i talk to a human??\n",
            "\n",
            "Bot: Transferring the request to your PM\n",
            "\n"
          ]
        }
      ]
    }
  ]
}